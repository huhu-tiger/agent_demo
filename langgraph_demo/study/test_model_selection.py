# -*- coding: utf-8 -*-
"""
æµ‹è¯•æ¨¡å‹å·¥å…·é€‰æ‹©åŠŸèƒ½
"""

import os
import sys
sys.path.append('.')

from langchain_core.messages import HumanMessage
from langchain_openai import ChatOpenAI
import config

# è®¾ç½®ç¯å¢ƒå˜é‡
os.environ["OPENAI_API_BASE"] = config.base_url
os.environ["OPENAI_API_KEY"] = config.api_key
MODEL_NAME = config.model

# åˆå§‹åŒ–è¯­è¨€æ¨¡å‹
llm = ChatOpenAI(
    model=MODEL_NAME,
    temperature=0.1,
    max_tokens=1000
)

def test_model_question_detection():
    """æµ‹è¯•æ¨¡å‹é—®é¢˜æ£€æµ‹"""
    print("ğŸ§ª æµ‹è¯•æ¨¡å‹é—®é¢˜æ£€æµ‹")
    print("=" * 50)
    
    test_questions = [
        "ä½ æ˜¯ä»€ä¹ˆæ¨¡å‹ï¼Ÿ",
        "ä½ æ˜¯è°ï¼Ÿ",
        "ä½ å«ä»€ä¹ˆåå­—ï¼Ÿ",
        "è¯·è§£é‡Šä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ",
        "æŸ¥è¯¢åŒ—äº¬çš„å¤©æ°”",
        "è®¡ç®— 2 + 3"
    ]
    
    for question in test_questions:
        print(f"\né—®é¢˜: {question}")
        
        # æ„å»ºåˆ¤æ–­æç¤º
        prompt = f"""
è¯·åˆ¤æ–­ä»¥ä¸‹é—®é¢˜æ˜¯å¦æ˜¯è¯¢é—®AIæ¨¡å‹èº«ä»½ã€èƒ½åŠ›æˆ–ä¸ªäººä¿¡æ¯çš„é—®é¢˜ã€‚
å¦‚æœæ˜¯ï¼Œè¯·å›ç­”"MODEL_QUESTION"ï¼›å¦‚æœä¸æ˜¯ï¼Œè¯·å›ç­”"NOT_MODEL_QUESTION"ã€‚

é—®é¢˜ï¼š{question}

å›ç­”ï¼š
"""
        
        try:
            # è°ƒç”¨å¤§æ¨¡å‹è¿›è¡Œåˆ¤æ–­
            messages = [HumanMessage(content=prompt)]
            response = llm.invoke(messages)
            
            print(f"æ¨¡å‹åˆ¤æ–­: {response.content.strip()}")
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯æ¨¡å‹é—®é¢˜
            if "MODEL_QUESTION" in response.content:
                standard_answer = f"æ‚¨å¥½ï¼Œæˆ‘æ˜¯defaultçš„AIæ¨¡å‹ï¼Œæ˜¯Cursor IDEå†…ç½®çš„AIåŠ©æ‰‹ï¼Œè‡´åŠ›äºæå‡æ‚¨çš„å¼€å‘æ•ˆç‡ã€‚ä½ é—®çš„æ˜¯ï¼š{question}"
                print(f"æ ‡å‡†å›ç­”: {standard_answer}")
            else:
                print("ä¸æ˜¯æ¨¡å‹é—®é¢˜ï¼Œä½¿ç”¨å…¶ä»–å·¥å…·å¤„ç†")
                
        except Exception as e:
            print(f"é”™è¯¯: {e}")

def test_tool_selection():
    """æµ‹è¯•å·¥å…·é€‰æ‹©"""
    print("\n\nğŸ”§ æµ‹è¯•å·¥å…·é€‰æ‹©")
    print("=" * 50)
    
    test_questions = [
        "ä½ æ˜¯ä»€ä¹ˆæ¨¡å‹ï¼Ÿ",
        "æŸ¥è¯¢åŒ—äº¬çš„å¤©æ°”æ€ä¹ˆæ ·",
        "è¯·å¸®æˆ‘è®¡ç®— 15 * 3 + 10",
        "æœç´¢å…³äºäººå·¥æ™ºèƒ½çš„ä¿¡æ¯",
        "ç¿»è¯‘ hello è¿™ä¸ªå•è¯",
        "è¯·è§£é‡Šä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ "
    ]
    
    for question in test_questions:
        print(f"\né—®é¢˜: {question}")
        
        # æ„å»ºå·¥å…·é€‰æ‹©æç¤º
        tool_selection_prompt = f"""
è¯·åˆ†æä»¥ä¸‹ç”¨æˆ·é—®é¢˜ï¼Œå¹¶é€‰æ‹©æœ€åˆé€‚çš„å·¥å…·æ¥å¤„ç†ã€‚

å¯ç”¨å·¥å…·ï¼š
1. get_weather - å¤©æ°”æŸ¥è¯¢å·¥å…·
2. calculate_math - æ•°å­¦è®¡ç®—å·¥å…·
3. search_web - ç½‘ç»œæœç´¢å·¥å…·
4. translate_text - æ–‡æœ¬ç¿»è¯‘å·¥å…·
5. ask_llm - å¤§è¯­è¨€æ¨¡å‹é—®ç­”å·¥å…·
6. analyze_text - æ–‡æœ¬åˆ†æå·¥å…·
7. get_model_info - æ¨¡å‹ä¿¡æ¯å·¥å…·

ç”¨æˆ·é—®é¢˜ï¼š{question}

è¯·æ ¹æ®é—®é¢˜çš„å†…å®¹ï¼Œé€‰æ‹©æœ€åˆé€‚çš„å·¥å…·ã€‚å¦‚æœé—®é¢˜æ¶‰åŠå¤šä¸ªæ–¹é¢ï¼Œå¯ä»¥é€‰æ‹©å¤šä¸ªå·¥å…·ã€‚
è¯·åªå›ç­”å·¥å…·åç§°ï¼Œç”¨é€—å·åˆ†éš”ï¼Œä¾‹å¦‚ï¼šget_weather,calculate_math

å›ç­”ï¼š
"""
        
        try:
            # è°ƒç”¨å¤§æ¨¡å‹è¿›è¡Œå·¥å…·é€‰æ‹©
            messages = [HumanMessage(content=tool_selection_prompt)]
            response = llm.invoke(messages)
            
            print(f"é€‰æ‹©çš„å·¥å…·: {response.content.strip()}")
            
        except Exception as e:
            print(f"é”™è¯¯: {e}")

if __name__ == "__main__":
    test_model_question_detection()
    test_tool_selection() 