#!/usr/bin/env python3
"""
LangGraph InMemoryStore ç¤ºä¾‹
åŒ…å«è¯­ä¹‰æœç´¢ã€æ·»åŠ å’Œåˆ é™¤æ¶ˆæ¯çš„åŠŸèƒ½
æ”¯æŒè‡ªå®šä¹‰æ¨¡å‹é…ç½®
"""

import os
import uuid
from typing import Optional, List, Dict, Any
from datetime import datetime
import traceback

from langchain.embeddings import init_embeddings
from langchain.chat_models import init_chat_model
from langchain_core.messages import HumanMessage, AIMessage, RemoveMessage
from langgraph.store.memory import InMemoryStore
from langgraph.store.base import BaseStore
from langgraph.graph import START, MessagesState, StateGraph
from langgraph.checkpoint.memory import InMemorySaver

import config
from config import ModelConfig
# è·å–æ—¥å¿—å™¨
logger = config.logger



class LangGraphMemoryDemo:
    """LangGraph InMemoryStore æ¼”ç¤ºç±»"""
    
    def __init__(
        self, 
        model_config: Optional[ModelConfig] = None
    ):
        """
        åˆå§‹åŒ–æ¼”ç¤ºç±»
        
        Args:
            model_config: æ¨¡å‹é…ç½®å¯¹è±¡
        """
        self.limit_history = 3
        # è®¾ç½®é»˜è®¤æ¨¡å‹é…ç½®
        if model_config is None:
            model_config = ModelConfig()
        
        self.model_config = model_config
        
        # åˆå§‹åŒ–åµŒå…¥æ¨¡å‹
        embedding_config = model_config.get_embedding_config()
        if embedding_config:
            # ä½¿ç”¨è‡ªå®šä¹‰é…ç½®åˆå§‹åŒ–åµŒå…¥æ¨¡å‹
            if model_config.embedding_provider == "openai":
                self.embeddings = init_embeddings(
                    f"openai:{model_config.embedding_model}",
                    **embedding_config
                )
            else:
                # æ”¯æŒå…¶ä»–æä¾›å•†
                self.embeddings = init_embeddings(
                    f"{model_config.embedding_provider}:{model_config.embedding_model}",
                    **embedding_config
                )
        else:
            # ä½¿ç”¨é»˜è®¤é…ç½®
            self.embeddings = init_embeddings(f"{model_config.embedding_provider}:{model_config.embedding_model}")
        
        # åˆå§‹åŒ–èŠå¤©æ¨¡å‹
        chat_config = model_config.get_chat_model_config()
        if chat_config:
            # ä½¿ç”¨è‡ªå®šä¹‰é…ç½®åˆå§‹åŒ–èŠå¤©æ¨¡å‹
            if model_config.model_provider == "openai":
                self.llm = init_chat_model(
                    f"openai:{model_config.model_name}",
                    **chat_config
                )
            else:
                # æ”¯æŒå…¶ä»–æä¾›å•†
                self.llm = init_chat_model(
                    f"{model_config.model_provider}:{model_config.model_name}",
                    **chat_config
                )
        else:
            # ä½¿ç”¨é»˜è®¤é…ç½®
            self.llm = init_chat_model(f"{model_config.model_provider}:{model_config.model_name}")
        
        # è·å–åµŒå…¥æ¨¡å‹çš„ç»´åº¦
        embedding_dims = model_config.get_embedding_dimensions()
        
        # åˆ›å»ºå¸¦æœ‰è¯­ä¹‰æœç´¢çš„InMemoryStore
        self.store = InMemoryStore(
            index={
                "embed": self.embeddings,
                "dims": embedding_dims,
                "fields": ["text", "content", "summary"]  # æŒ‡å®šè¦åµŒå…¥çš„å­—æ®µ
            }
        )
        
        # åˆ›å»ºæ£€æŸ¥ç‚¹ä¿å­˜å™¨
        self.checkpointer = InMemorySaver()
        
        # æ„å»ºå›¾
        self.graph = self._build_graph()
        
        # ç”¨æˆ·é…ç½®
        self.config = {
            "configurable": {
                "thread_id": "demo_thread",
                "user_id": "demo_user",
            }
        }
    
    def get_model_info(self) -> Dict[str, str]:
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        return {
            "chat_model": f"{self.model_config.model_provider}:{self.model_config.model_name}",
            "embedding_model": f"{self.model_config.embedding_provider}:{self.model_config.embedding_model}",
            "chat_base_url": self.model_config.base_url or "é»˜è®¤",
            "embedding_base_url": self.model_config.embedding_base_url or "é»˜è®¤",
            "embedding_dimensions": str(self.model_config.get_embedding_dimensions())
        }
    
    def _build_graph(self):
        """æ„å»ºLangGraphå·¥ä½œæµ"""
        
        def chat_with_memory(state, *, store: BaseStore):
            """å¸¦æœ‰è®°å¿†åŠŸèƒ½çš„èŠå¤©èŠ‚ç‚¹"""
            logger.info(f"graph chat_with_memory: {state}")
            user_id = "demo_user"
            namespace = (user_id, "memories")
            
            # åŸºäºç”¨æˆ·æœ€åä¸€æ¡æ¶ˆæ¯è¿›è¡Œè¯­ä¹‰æœç´¢
            last_message = state["messages"][-1].content
            logger.info(f"å¤„ç†ç”¨æˆ·æ¶ˆæ¯: {last_message}")
            
            items = store.search(
                namespace, 
                query=last_message, 
                limit=self.limit_history
            )
            
            # æ„å»ºè®°å¿†ä¸Šä¸‹æ–‡
            memories = []
            for item in items:
                memory_text = item.value.get("text", item.value.get("content", ""))
                if memory_text:
                    # æ·»åŠ ç›¸ä¼¼åº¦ä¿¡æ¯
                    similarity_score = item.score
                    memories.append(f"{memory_text}")
                    logger.info(f"æ‰¾åˆ°è®°å¿†: {memory_text} (ç›¸ä¼¼åº¦: {similarity_score:.3f})")
            
            memory_context = ""
            if memories:
                memory_context = f"\n## ç”¨æˆ·è®°å¿†:\n" + "\n".join(memories)
            logger.info(f"memory_context: {memory_context}")
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦å­˜å‚¨æ–°è®°å¿†
            if "è®°ä½" in last_message or "remember" in last_message.lower():
                # æå–è¦è®°ä½çš„å†…å®¹
                memory_content = last_message.replace("è®°ä½", "").replace("remember", "").strip()
                if memory_content:
                    memory_id = str(uuid.uuid4())
                    store.put(
                        namespace,
                        memory_id,
                        {
                            "text": memory_content,
                            "timestamp": datetime.now().isoformat(),
                            "type": "user_memory"
                        }
                    )
                    memory_context += f"\nå·²è®°ä½: {memory_content}"
                    logger.info(f"å­˜å‚¨æ–°è®°å¿†: {memory_content}")
            
            # æ„å»ºç³»ç»Ÿæç¤º
            system_prompt = f"""ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„AIåŠ©æ‰‹ã€‚{memory_context}

è¯·ç”¨ä¸­æ–‡å›å¤ç”¨æˆ·çš„é—®é¢˜ã€‚å¦‚æœç”¨æˆ·è¦æ±‚è®°ä½ä»€ä¹ˆï¼Œè¯·ç¡®è®¤å·²ç»è®°ä½ã€‚
å¦‚æœç”¨æˆ·è¯¢é—®å…³äºä»–ä»¬è‡ªå·±çš„ä¿¡æ¯ï¼Œè¯·åŸºäºè®°å¿†ä¸­çš„ä¿¡æ¯å›ç­”ã€‚"""
            

            
            try:
                # è°ƒç”¨LLM
                logger.info("å¼€å§‹è°ƒç”¨LLM...")
                response = self.llm.invoke(
                    [
                        {"role": "system", "content": system_prompt},
                        *state["messages"]
                    ]
                )
                logger.info(f"ç³»ç»Ÿæç¤ºè¯: {system_prompt}")
                logger.info(f"ç”¨æˆ·è¾“å…¥: {state['messages']}")
                logger.info(f"LLMè°ƒç”¨æˆåŠŸï¼Œå“åº”ç±»å‹: {type(response)}")
                logger.info(f"LLMå“åº”å†…å®¹: {response.content if hasattr(response, 'content') else response}")
                
                return {"messages": [response]}
                
            except Exception as e:
                logger.error(f"LLMè°ƒç”¨å¤±è´¥: {e}")
                logger.error(f"LLMé”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
                # è¿”å›é”™è¯¯æ¶ˆæ¯
                error_response = AIMessage(content=f"æŠ±æ­‰ï¼Œæ¨¡å‹è°ƒç”¨å¤±è´¥: {str(e)}")
                return {"messages": [error_response]}
        
        def delete_old_messages(state):
            """åˆ é™¤æ—§æ¶ˆæ¯çš„èŠ‚ç‚¹"""
            messages = state["messages"]
            if len(messages) > self.limit_history + 1:  # ä¿ç•™æœ€è¿‘4æ¡æ¶ˆæ¯
                # åˆ é™¤æœ€æ—©çš„æ¶ˆæ¯
                logger.info(f"åˆ é™¤æ—§æ¶ˆæ¯: {messages[:-(self.limit_history + 1)]}")
                return {"messages": [RemoveMessage(id=m.id) for m in messages[:-(self.limit_history + 1)]]}
            return {}
        
        # æ„å»ºå›¾
        builder = StateGraph(MessagesState)
        builder.add_node("chat", chat_with_memory)
        builder.add_node("cleanup", delete_old_messages)
        builder.add_edge(START, "chat")
        builder.add_edge("chat", "cleanup")
        
        return builder.compile(
            checkpointer=self.checkpointer,
            store=self.store
        )
    
    def add_memory(self, content: str, memory_type: str = "general") -> str:
        """
        æ·»åŠ è®°å¿†åˆ°å­˜å‚¨
        
        Args:
            content: è®°å¿†å†…å®¹
            memory_type: è®°å¿†ç±»å‹
            
        Returns:
            è®°å¿†ID
        """
        memory_id = str(uuid.uuid4())
        namespace = ("demo_user", "memories")
        
        self.store.put(
            namespace,
            memory_id,
            {
                "text": content,
                "timestamp": datetime.now().isoformat(),
                "type": memory_type
            }
        )
        
        print(f"âœ… å·²æ·»åŠ è®°å¿†: {content}")
        return memory_id
    
    def search_memories(self, query: str, limit: int = 5) -> List[Dict[str, Any]]:
        """
        æœç´¢è®°å¿†
        
        Args:
            query: æœç´¢æŸ¥è¯¢
            limit: è¿”å›ç»“æœæ•°é‡é™åˆ¶
            
        Returns:
            æœç´¢ç»“æœåˆ—è¡¨
        """
        namespace = ("demo_user", "memories")
        results = self.store.search(namespace, query=query, limit=limit)
        
        memories = []
        for item in results:
            memories.append({
                "id": item.key,
                "content": item.value.get("text", ""),
                "type": item.value.get("type", ""),
                "timestamp": item.value.get("timestamp", ""),
                "score": item.score
            })
        
        return memories
    
    def delete_memory(self, memory_id: str) -> bool:
        """
        åˆ é™¤æŒ‡å®šè®°å¿†
        
        Args:
            memory_id: è¦åˆ é™¤çš„è®°å¿†ID
            
        Returns:
            æ˜¯å¦åˆ é™¤æˆåŠŸ
        """
        namespace = ("demo_user", "memories")
        
        # æ£€æŸ¥è®°å¿†æ˜¯å¦å­˜åœ¨
        existing = self.store.get(namespace, memory_id)
        if existing is None:
            print(f"âŒ è®°å¿† {memory_id} ä¸å­˜åœ¨")
            return False
        
        # åˆ é™¤è®°å¿†ï¼ˆé€šè¿‡å­˜å‚¨Noneæ¥åˆ é™¤ï¼‰
        self.store.put(namespace, memory_id, None)
        print(f"âœ… å·²åˆ é™¤è®°å¿†: {memory_id}")
        return True
    
    def list_all_memories(self) -> List[Dict[str, Any]]:
        """
        åˆ—å‡ºæ‰€æœ‰è®°å¿†
        
        Returns:
            æ‰€æœ‰è®°å¿†çš„åˆ—è¡¨
        """
        namespace = ("demo_user", "memories")
        # ä½¿ç”¨ç©ºæŸ¥è¯¢æ¥è·å–æ‰€æœ‰è®°å¿†
        results = self.store.search(namespace, query="", limit=100)
        memories = []
        for item in results:
            memories.append({
                "id": item.key,
                "content": item.value.get("text", ""),
                "type": item.value.get("type", ""),
                "timestamp": item.value.get("timestamp", ""),
            })
        
        return memories
    
    def chat(self, message: str) -> str:
        """
        ä¸AIåŠ©æ‰‹èŠå¤©
        
        Args:
            message: ç”¨æˆ·æ¶ˆæ¯
            
        Returns:
            AIå›å¤
        """
        logger.info(f"å¼€å§‹èŠå¤©ï¼Œç”¨æˆ·æ¶ˆæ¯: {message}")
        response_content = ""
        event_count = 0
        
        try:
            # for event in self.graph.stream(
            #     {"messages": [HumanMessage(content=message)]},
            #     self.config,
            #     # stream_mode="messages"
            #     stream_mode="values"
            # ):
            result = self.graph.invoke(
                {"messages": [HumanMessage(content=message)]},
                self.config,
            )
            logger.info(f"llm result: {result}")
            response_content = result["messages"][-1].content
                
            return response_content
            
        except Exception as e:
            logger.error(f"èŠå¤©è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
            logger.error(f"é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
            return f"æŠ±æ­‰ï¼ŒèŠå¤©è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}"
    
    def clear_all_memories(self):
        """æ¸…ç©ºæ‰€æœ‰è®°å¿†"""
        namespace = ("demo_user", "memories")
        memories = self.list_all_memories()
        
        for memory in memories:
            self.store.put(namespace, memory["id"], None)
        
        print(f"âœ… å·²æ¸…ç©ºæ‰€æœ‰è®°å¿† ({len(memories)} æ¡)")


def main():
    """ä¸»å‡½æ•° - æ¼”ç¤ºInMemoryStoreåŠŸèƒ½"""
    
    print("ğŸš€ LangGraph InMemoryStore è¯­ä¹‰æœç´¢æ¼”ç¤º")
    print("=" * 50)
    
    
    print("\nğŸ”§ æ¨¡å‹é…ç½®æ¼”ç¤º...")
    
    # æ¼”ç¤º1: ä½¿ç”¨é»˜è®¤é…ç½®
    print("\n1. ä½¿ç”¨é»˜è®¤æ¨¡å‹é…ç½®:")

    from config import custom_config

    demo_default = LangGraphMemoryDemo(model_config=custom_config)
    model_info = demo_default.get_model_info()
    print(f"   èŠå¤©æ¨¡å‹: {model_info['chat_model']}")
    print(f"   åµŒå…¥æ¨¡å‹: {model_info['embedding_model']}")
    print(f"   èŠå¤©APIåœ°å€: {model_info['chat_base_url']}")
    print(f"   åµŒå…¥APIåœ°å€: {model_info['embedding_base_url']}")
    print(f"   åµŒå…¥ç»´åº¦: {model_info['embedding_dimensions']}")
    
    # ä½¿ç”¨é»˜è®¤é…ç½®è¿›è¡ŒåŠŸèƒ½æ¼”ç¤º
    demo = demo_default
    
    print("\nğŸ“ 4. æ·»åŠ ä¸€äº›ç¤ºä¾‹è®°å¿†...")
    demo.add_memory("æˆ‘å–œæ¬¢åƒæŠ«è¨", "preference")
    demo.add_memory("æˆ‘æ˜¯ä¸€ä¸ªç¨‹åºå‘˜", "personal")
    demo.add_memory("æˆ‘ä½åœ¨åŒ—äº¬", "personal")
    demo.add_memory("æˆ‘å–œæ¬¢Pythonç¼–ç¨‹è¯­è¨€", "preference")
    demo.add_memory("æˆ‘æœ‰ä¸€åªå«å°ç™½çš„çŒ«", "personal")
    
    print("\nğŸ” 5. è¯­ä¹‰æœç´¢æ¼”ç¤º...")
    
    # æœç´¢é£Ÿç‰©åå¥½
    print("\næœç´¢é£Ÿç‰©åå¥½:")
    food_memories = demo.search_memories("é£Ÿç‰© å–œæ¬¢", limit=3)
    for i, memory in enumerate(food_memories, 1):
        print(f"  {i}. {memory['content']}")
        print(f"     ç›¸ä¼¼åº¦: {memory['score']:.4f}")
        print(f"     ç±»å‹: {memory['type']}")
        print(f"     æ—¶é—´: {memory['timestamp']}")
    
    # æœç´¢ä¸ªäººä¿¡æ¯
    print("\næœç´¢ä¸ªäººä¿¡æ¯:")
    personal_memories = demo.search_memories("ä¸ªäººä¿¡æ¯ èŒä¸š", limit=3)
    for i, memory in enumerate(personal_memories, 1):
        print(f"  {i}. {memory['content']}")
        print(f"     ç›¸ä¼¼åº¦: {memory['score']:.4f}")
        print(f"     ç±»å‹: {memory['type']}")
        print(f"     æ—¶é—´: {memory['timestamp']}")
    
    # æµ‹è¯•ä¸åŒç›¸ä¼¼åº¦é˜ˆå€¼
    print("\nç›¸ä¼¼åº¦é˜ˆå€¼æµ‹è¯•:")
    test_query = "ç¼–ç¨‹"
    all_results = demo.search_memories(test_query, limit=10)
    
    thresholds = [0.5]
    for threshold in thresholds:
        filtered_results = [m for m in all_results if m['score'] >= threshold]
        print(f"  é˜ˆå€¼ {threshold:.1f}: {len(filtered_results)} æ¡ç»“æœ")
        for memory in filtered_results[:2]:  # åªæ˜¾ç¤ºå‰2æ¡
            print(f"    - {memory['content']} (ç›¸ä¼¼åº¦: {memory['score']:.4f})")
    
    print("\nğŸ’¬ 6. ========================èŠå¤©æ¼”ç¤º===========================")
    
    # æµ‹è¯•è®°å¿†åŠŸèƒ½
    print("\nç”¨æˆ·: è®°ä½æˆ‘å–œæ¬¢åƒç«é”…")
    response = demo.chat("è®°ä½æˆ‘å–œæ¬¢åƒç«é”…")
    logger.info(f"AI: {response}")
    
    print("\nç”¨æˆ·: æˆ‘å–œæ¬¢åƒä»€ä¹ˆï¼Ÿ")
    response = demo.chat("æˆ‘å–œæ¬¢åƒä»€ä¹ˆï¼Ÿ")
    logger.info(f"AI: {response}")
    
    print("\nç”¨æˆ·: æˆ‘çš„èŒä¸šæ˜¯ä»€ä¹ˆï¼Ÿ")
    response = demo.chat("æˆ‘çš„èŒä¸šæ˜¯ä»€ä¹ˆï¼Ÿ")
    logger.info(f"AI: {response}")
    
    print("\nğŸ“‹ 7. åˆ—å‡ºæ‰€æœ‰è®°å¿†...")
    all_memories = demo.list_all_memories()
    for i, memory in enumerate(all_memories, 1):
        print(f"  {i}. {memory['content']}")
        print(f"     ç±»å‹: {memory['type']}")
        print(f"     æ—¶é—´: {memory['timestamp']}")
        print(f"     ç›¸ä¼¼åº¦: {memory['score']}")  # ç©ºæœç´¢é¡¹ï¼Œç›¸ä¼¼åº¦ä¸ºNone
        print()
    
    print("\nğŸ—‘ï¸ 8. åˆ é™¤è®°å¿†æ¼”ç¤º...")
    if all_memories:
        # åˆ é™¤ç¬¬ä¸€æ¡è®°å¿†
        first_memory = all_memories[0]
        demo.delete_memory(first_memory["id"])
        
        print("\nåˆ é™¤åçš„è®°å¿†åˆ—è¡¨:")
        remaining_memories = demo.list_all_memories()
        for i, memory in enumerate(remaining_memories, 1):
            print(f"  {i}. {memory['content']}")
            print(f"     ç±»å‹: {memory['type']}")
            print(f"     æ—¶é—´: {memory['timestamp']}")
            print(f"     ç›¸ä¼¼åº¦: {memory['score']}")  # ç©ºæœç´¢é¡¹ï¼Œç›¸ä¼¼åº¦ä¸ºNone
            print()
    
    print("\nğŸ§¹ 9. æ¸…ç©ºæ‰€æœ‰è®°å¿†...")
    demo.clear_all_memories()
    
    print("\nâœ… æ¼”ç¤ºå®Œæˆï¼")


if __name__ == "__main__":
    main()
