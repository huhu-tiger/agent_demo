# -*- coding: utf-8 -*-
"""
LangGraph ËäÇÁÇπÂêçÁß∞Ëá™ÂÆö‰πâÁ§∫‰æã
Â≠¶‰π†Ë¶ÅÁÇπÔºöËäÇÁÇπÂêçÁß∞Ë∑üË∏™„ÄÅËá™ÂÆö‰πâÊòæÁ§∫ÂêçÁß∞„ÄÅÂä®ÊÄÅÂêçÁß∞ÈÖçÁΩÆ
"""

import os
import sys
import time
import json
from typing import TypedDict, Annotated
import operator

# Ê∑ªÂä†ÂΩìÂâçËÑöÊú¨ÊâÄÂú®ÁõÆÂΩïÂà∞Ë∑ØÂæÑÔºåÁ°Æ‰øùËÉΩÂØºÂÖ•Êú¨ÁõÆÂΩï‰∏ãÁöÑ config.py
CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))
if CURRENT_DIR not in sys.path:
    sys.path.append(CURRENT_DIR)

from langgraph.graph import StateGraph, START, END
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage
import config
# Ëé∑ÂèñÊó•ÂøóÂô®
logger = config.logger
# ËÆæÁΩÆÁéØÂ¢ÉÂèòÈáè
os.environ["OPENAI_API_BASE"] = config.base_url
os.environ["OPENAI_API_KEY"] = config.api_key
MODEL_NAME = config.model

# ÂàùÂßãÂåñËØ≠Ë®ÄÊ®°Âûã
llm = ChatOpenAI(
    model=MODEL_NAME,
    temperature=0.1,
    max_tokens=500
)

# ÂÆö‰πâÁä∂ÊÄÅÁªìÊûÑ
class NodeTrackingState(TypedDict):
    """ËäÇÁÇπË∑üË∏™Áä∂ÊÄÅÂÆö‰πâ"""
    user_input: str                    # Áî®Êà∑ËæìÂÖ•
    current_node: str                  # ÂΩìÂâçËäÇÁÇπÂêçÁß∞
    node_display_name: str             # ËäÇÁÇπÊòæÁ§∫ÂêçÁß∞
    node_description: str              # ËäÇÁÇπÊèèËø∞
    execution_log: Annotated[list, operator.add]  # ÊâßË°åÊó•Âøó
    step_count: int                    # Ê≠•È™§ËÆ°Êï∞

# ËäÇÁÇπÈÖçÁΩÆÂ≠óÂÖ∏
NODE_CONFIGS = {
    "data_processor": {
        "display_name": "üìä Êï∞ÊçÆÈ¢ÑÂ§ÑÁêÜÂºïÊìé",
        "description": "Êô∫ËÉΩÂ§ÑÁêÜÂíåÂàÜÊûêËæìÂÖ•Êï∞ÊçÆ",
        "emoji": "üìä"
    },
    "ai_analyzer": {
        "display_name": "üß† AIÊô∫ËÉΩÂàÜÊûêÂô®",
        "description": "‰ΩøÁî®Ê∑±Â∫¶Â≠¶‰π†ÊäÄÊúØÂàÜÊûêÁî®Êà∑ÊÑèÂõæ",
        "emoji": "üß†"
    },
    "recommendation_engine": {
        "display_name": "üéØ Êô∫ËÉΩÊé®ËçêÂºïÊìé",
        "description": "Âü∫‰∫éAIÁÆóÊ≥ïÁîüÊàê‰∏™ÊÄßÂåñÊé®Ëçê",
        "emoji": "üéØ"
    },
    "response_generator": {
        "display_name": "‚ú® ÂìçÂ∫îÁîüÊàêÂô®",
        "description": "Êï¥ÂêàÊâÄÊúâÁªìÊûúÁîüÊàêÊúÄÁªàÂìçÂ∫î",
        "emoji": "‚ú®"
    }
}

def get_node_config(node_name: str) -> dict:
    """Ëé∑ÂèñËäÇÁÇπÈÖçÁΩÆ"""
    return NODE_CONFIGS.get(node_name, {
        "display_name": f"üîß {node_name}",
        "description": "ÈªòËÆ§ËäÇÁÇπÊèèËø∞",
        "emoji": "üîß"
    })

def create_node_log(node_name: str, action: str, step_count: int, **kwargs) -> dict:
    """ÂàõÂª∫ËäÇÁÇπÊó•Âøó"""
    config = get_node_config(node_name)
    return {
        "step": step_count,
        "node": node_name,
        "display_name": config["display_name"],
        "description": config["description"],
        "emoji": config["emoji"],
        "action": action,
        "timestamp": time.strftime("%H:%M:%S"),
        **kwargs
    }

# ÂÆö‰πâËäÇÁÇπÂáΩÊï∞
def data_processor_node(state: NodeTrackingState) -> NodeTrackingState:
    """
    Êï∞ÊçÆÈ¢ÑÂ§ÑÁêÜËäÇÁÇπ
    Â≠¶‰π†Ë¶ÅÁÇπÔºöËäÇÁÇπÂêçÁß∞ÈÖçÁΩÆ„ÄÅÊó•ÂøóËÆ∞ÂΩï
    """
    user_input = state["user_input"]
    step_count = state.get("step_count", 0) + 1
    node_name = "data_processor"
    
    # Ëé∑ÂèñËäÇÁÇπÈÖçÁΩÆ
    config = get_node_config(node_name)
    
    # ÂàõÂª∫ÊâßË°åÊó•Âøó
    log_entry = create_node_log(
        node_name=node_name,
        action="ÂºÄÂßãÊï∞ÊçÆÈ¢ÑÂ§ÑÁêÜ",
        step_count=step_count,
        input=user_input
    )
    
    # Ê®°ÊãüÊï∞ÊçÆÂ§ÑÁêÜ
    time.sleep(0.2)
    processed_data = f"È¢ÑÂ§ÑÁêÜÁªìÊûú: {user_input.upper()}"
    
    log_entry["result"] = processed_data
    log_entry["status"] = "completed"
    
    return {
        "current_node": node_name,
        "node_display_name": config["display_name"],
        "node_description": config["description"],
        "step_count": step_count,
        "execution_log": [log_entry]
    }

def ai_analyzer_node(state: NodeTrackingState) -> NodeTrackingState:
    """
    AIÂàÜÊûêËäÇÁÇπ
    Â≠¶‰π†Ë¶ÅÁÇπÔºöLLMË∞ÉÁî®„ÄÅËäÇÁÇπÁä∂ÊÄÅË∑üË∏™
    """
    user_input = state["user_input"]
    step_count = state.get("step_count", 0) + 1
    node_name = "ai_analyzer"
    
    # Ëé∑ÂèñËäÇÁÇπÈÖçÁΩÆ
    config = get_node_config(node_name)
    
    # ÂàõÂª∫ÊâßË°åÊó•Âøó
    log_entry = create_node_log(
        node_name=node_name,
        action="ÂºÄÂßãAIÂàÜÊûê",
        step_count=step_count,
        input=user_input
    )
    
    # ‰ΩøÁî®LLMËøõË°åÂàÜÊûê
    prompt = f"""
ËØ∑ÂàÜÊûêÁî®Êà∑ËæìÂÖ•: "{user_input}"
Êèê‰æõÁÆÄË¶ÅÁöÑÂàÜÊûêÁªìÊûú„ÄÇ
"""
    
    response = llm.invoke([HumanMessage(content=prompt)])
    analysis_result = response.content
    
    log_entry["result"] = analysis_result
    log_entry["status"] = "completed"
    
    return {
        "current_node": node_name,
        "node_display_name": config["display_name"],
        "node_description": config["description"],
        "step_count": step_count,
        "execution_log": [log_entry]
    }

def recommendation_engine_node(state: NodeTrackingState) -> NodeTrackingState:
    """
    Êé®ËçêÂºïÊìéËäÇÁÇπ
    Â≠¶‰π†Ë¶ÅÁÇπÔºöÂä®ÊÄÅËäÇÁÇπÂêçÁß∞„ÄÅÁä∂ÊÄÅ‰º†ÈÄí
    """
    user_input = state["user_input"]
    step_count = state.get("step_count", 0) + 1
    node_name = "recommendation_engine"
    
    # Ëé∑ÂèñËäÇÁÇπÈÖçÁΩÆ
    config = get_node_config(node_name)
    
    # ÂàõÂª∫ÊâßË°åÊó•Âøó
    log_entry = create_node_log(
        node_name=node_name,
        action="ÂºÄÂßãÁîüÊàêÊé®Ëçê",
        step_count=step_count,
        input=user_input
    )
    
    # Ê®°ÊãüÊé®ËçêÁîüÊàê
    time.sleep(0.1)
    recommendation = f"Âü∫‰∫é '{user_input}' ÁöÑÊé®Ëçê: Âª∫ËÆÆÊ∑±ÂÖ•Â≠¶‰π†Áõ∏ÂÖ≥ÊäÄÊúØ"
    
    log_entry["result"] = recommendation
    log_entry["status"] = "completed"
    
    return {
        "current_node": node_name,
        "node_display_name": config["display_name"],
        "node_description": config["description"],
        "step_count": step_count,
        "execution_log": [log_entry]
    }

def response_generator_node(state: NodeTrackingState) -> NodeTrackingState:
    """
    ÂìçÂ∫îÁîüÊàêÂô®ËäÇÁÇπ
    Â≠¶‰π†Ë¶ÅÁÇπÔºöÊúÄÁªàÁä∂ÊÄÅÊï¥Âêà„ÄÅËäÇÁÇπÂêçÁß∞Â±ïÁ§∫
    """
    user_input = state["user_input"]
    step_count = state.get("step_count", 0) + 1
    node_name = "response_generator"
    
    # Ëé∑ÂèñËäÇÁÇπÈÖçÁΩÆ
    config = get_node_config(node_name)
    
    # ÂàõÂª∫ÊâßË°åÊó•Âøó
    log_entry = create_node_log(
        node_name=node_name,
        action="ÂºÄÂßãÁîüÊàêÊúÄÁªàÂìçÂ∫î",
        step_count=step_count,
        input=user_input
    )
    
    # ÁîüÊàêÊúÄÁªàÂìçÂ∫î
    final_response = f"""
üéâ Â§ÑÁêÜÂÆåÊàêÔºÅ

Áî®Êà∑ËæìÂÖ•: {user_input}
Â§ÑÁêÜÊ≠•È™§: {step_count} Ê≠•
ÂΩìÂâçËäÇÁÇπ: {config["display_name"]}

ÊÑüË∞¢ÊÇ®ÁöÑ‰ΩøÁî®ÔºÅ
"""
    
    log_entry["result"] = final_response
    log_entry["status"] = "completed"
    
    return {
        "current_node": node_name,
        "node_display_name": config["display_name"],
        "node_description": config["description"],
        "step_count": step_count,
        "execution_log": [log_entry]
    }

def create_node_tracking_workflow():
    """ÂàõÂª∫ËäÇÁÇπË∑üË∏™Â∑•‰ΩúÊµÅ"""
    workflow = StateGraph(NodeTrackingState)
    
    # Ê∑ªÂä†ËäÇÁÇπ
    workflow.add_node("data_processor", data_processor_node)
    workflow.add_node("ai_analyzer", ai_analyzer_node)
    workflow.add_node("recommendation_engine", recommendation_engine_node)
    workflow.add_node("response_generator", response_generator_node)
    
    # ËÆæÁΩÆÊµÅÁ®ã
    workflow.set_entry_point("data_processor")
    workflow.add_edge("data_processor", "ai_analyzer")
    workflow.add_edge("ai_analyzer", "recommendation_engine")
    workflow.add_edge("recommendation_engine", "response_generator")
    workflow.add_edge("response_generator", END)
    
    return workflow

# ===== FastAPI Â∫îÁî®‰∏é WebSocket/SSE ÊµÅÂºèÊé•Âè£ =====
from fastapi import FastAPI, Body, Query
from fastapi.responses import StreamingResponse, JSONResponse
from fastapi.middleware.cors import CORSMiddleware
from fastapi import WebSocket, WebSocketDisconnect

app = FastAPI(title="LangGraph Streaming API", version="0.1.0")
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Âú®Â∫îÁî®ÂêØÂä®Êó∂ÂàõÂª∫‰∏ÄÊ¨°Â∑•‰ΩúÊµÅÔºåÈÅøÂÖçÈáçÂ§çÊûÑÂª∫
from langgraph.checkpoint.memory import MemorySaver
_memory_checkpointer = MemorySaver()
# ‰ª•Â∏¶Ê£ÄÊü•ÁÇπÁöÑÂΩ¢ÂºèÁºñËØë
graph_app = create_node_tracking_workflow().compile(checkpointer=_memory_checkpointer)

ALLOWED_STREAM_MODES = {"updates", "values", "messages", "debug"}

def _safe_serialize(obj):
    try:
        json.dumps(obj)
        return obj
    except TypeError:
        if isinstance(obj, dict):
            return {k: _safe_serialize(v) for k, v in obj.items()}
        if isinstance(obj, list):
            return [_safe_serialize(v) for v in obj]
        return str(obj)

def _format_sse(data: dict, event: str = "message") -> str:
    payload = {"event": event, "data": _safe_serialize(data)}
    return json.dumps(payload, ensure_ascii=False) + "\n"

def _parse_stream_modes(mode_param):
    if not mode_param:
        return ["updates"], []
    raw_list = []
    if isinstance(mode_param, str):
        raw_list = [p.strip() for p in mode_param.replace("|", ",").split(",") if p.strip()]
    elif isinstance(mode_param, (list, tuple, set)):
        for item in mode_param:
            if isinstance(item, str):
                raw_list.extend([p.strip() for p in item.replace("|", ",").split(",") if p.strip()])
    # ÂéªÈáçÂπ∂Ê†°È™å
    uniq = []
    for m in raw_list:
        if m not in uniq:
            uniq.append(m)
    valid = [m for m in uniq if m in ALLOWED_STREAM_MODES]
    invalid = [m for m in uniq if m not in ALLOWED_STREAM_MODES]
    if not valid:
        valid = ["updates"]
    return valid, invalid

def _normalize_content(content):
    """Â∞ÜÂ§çÊùÇ message content ËßÑÊï¥‰∏∫Â≠óÁ¨¶‰∏≤ÔºåÈÅøÂÖç‰∏çÂèØÂ∫èÂàóÂåñ/‰∏çÂèØÂìàÂ∏åÂØπË±°Âêë‰∏ãÊ∏∏‰º†Êí≠„ÄÇ"""
    try:
        if content is None:
            return ""
        if isinstance(content, str):
            return content
        if isinstance(content, (int, float, bool)):
            return str(content)
        # LangChain Â∏∏ËßÅÔºölist[dict|str|obj]
        if isinstance(content, list):
            parts = []
            for item in content:
                if isinstance(item, str):
                    parts.append(item)
                elif isinstance(item, dict):
                    # ‰ºòÂÖàÊèêÂèñ text Â≠óÊÆµ
                    txt = item.get("text") or item.get("content")
                    if isinstance(txt, str):
                        parts.append(txt)
                    else:
                        parts.append(json.dumps(_safe_serialize(item), ensure_ascii=False))
                else:
                    # ÂÖúÂ∫ï‰∏∫Â≠óÁ¨¶‰∏≤
                    parts.append(str(item))
            return "".join(parts)
        if isinstance(content, dict):
            # Â∞ùËØïÊèêÂèñ textÔºåÂê¶ÂàôÊï¥‰ΩìÂ∫èÂàóÂåñ
            txt = content.get("text") or content.get("content")
            if isinstance(txt, str):
                return txt
            return json.dumps(_safe_serialize(content), ensure_ascii=False)
        # ÂØπË±°ÔºöÂ∞ùËØïÂèñ .text/.value/.content Á≠â
        for attr in ("text", "value", "content"):
            if hasattr(content, attr):
                val = getattr(content, attr)
                return _normalize_content(val)
        return str(content)
    except Exception:
        return str(content)

def _extract_message_list(value):
    messages = []
    src = None
    if isinstance(value, dict) and "messages" in value:
        src = value["messages"]
    elif isinstance(value, list):
        src = value
    else:
        src = [value]
    for m in src:
        role = getattr(m, "type", None) or getattr(m, "role", None)
        content = getattr(m, "content", None)
        if isinstance(m, dict):
            role = m.get("type") or m.get("role") or role
            content = m.get("content") or content
        content = _normalize_content(content)
        messages.append({"role": role or "unknown", "content": content})
    return messages

def _iter_node_items(chunk):
    """ÂÖºÂÆπÊÄßËø≠‰ª£ÔºöÂ∞Ü chunk Áªü‰∏Ä‰∏∫ (node_name, node_value) Â∫èÂàó„ÄÇ
    ÊîØÊåÅ dict„ÄÅ(node, value) ÂÖÉÁªÑ„ÄÅ[(node,value), ...] ÂàóË°®„ÄÅÊàñÂÖ∂ÂÆÉÁ±ªÂûã„ÄÇ
    """
    if isinstance(chunk, dict):
        for k, v in chunk.items():
            yield k, v
        return
    if isinstance(chunk, tuple):
        if len(chunk) == 2:
            yield chunk[0], chunk[1]
            return
    if isinstance(chunk, list):
        for item in chunk:
            if isinstance(item, tuple) and len(item) == 2:
                yield item[0], item[1]
            elif isinstance(item, dict):
                for k, v in item.items():
                    yield k, v
            else:
                yield "unknown", item
        return
    # ÂÖ∂ÂÆÉÊú™Áü•Á±ªÂûãÔºå‰Ωú‰∏∫Âçï‰∏™ÂÄºËæìÂá∫
    yield "unknown", chunk

def _format_tuple_messages_chunk(msg_obj, meta: dict) -> dict:
    """Â∞Ü (MessageChunk/Message, meta) ËßÑËåÉÂåñ‰∏∫Áªü‰∏ÄË¥üËΩΩ„ÄÇ"""
    node_name = meta.get("langgraph_node") or "unknown"
    cfg = get_node_config(str(node_name))
    content = _normalize_content(getattr(msg_obj, "content", msg_obj))
    role = getattr(msg_obj, "type", None) or getattr(msg_obj, "role", None) or "assistant"
    payload = {
        "node": node_name,
        "display_name": cfg["display_name"],
        "description": cfg["description"],
        "step": meta.get("langgraph_step"),
        "path": meta.get("langgraph_path"),
        "triggers": meta.get("langgraph_triggers"),
        "provider": meta.get("ls_provider"),
        "model": meta.get("ls_model_name"),
        "message": {
            "role": role,
            "content": content
        }
    }
    return payload

def _node_sse_generator(user_input: str, stream_mode = "updates", thread_id: str | None = None, checkpoint: str | None = None, replay_history: bool = True):
    modes, invalid = _parse_stream_modes(stream_mode)
    if invalid:
        yield _format_sse({"warning": "invalid_modes", "ignored": invalid, "allowed": sorted(list(ALLOWED_STREAM_MODES))}, event="warning")
    inputs = {
        "user_input": user_input,
        "execution_log": [],
        "step_count": 0
    }
    # ÈÄè‰º†‰ºöËØù‰∏éÊñ≠ÁÇπÈÖçÁΩÆ
    stream_kwargs = {}
    cfg = {}
    configurable = {}
    if thread_id:
        configurable["thread_id"] = thread_id
    if checkpoint:
        # ÂêåÊ≠•‰º†ÈÄí‰∏§ÁßçÂ∏∏ËßÅÈîÆÔºå‰æø‰∫é‰∏çÂêåÁâàÊú¨ÂÖºÂÆπ
        configurable["checkpoint"] = checkpoint
        configurable["checkpoint_id"] = checkpoint
    if configurable:
        cfg["configurable"] = configurable
    if cfg:
        stream_kwargs["config"] = cfg
        
    logger.info(f"stream_kwargs: {stream_kwargs}")

    # ÊåâÈúÄÂõûÊîæÂéÜÂè≤ÔºöÂ∞ÜÂ∑≤Êúâ execution_log ‰ª•Áã¨Á´ãÁöÑ history ‰∫ã‰ª∂ËæìÂá∫

    if replay_history and stream_kwargs.get("config"):
        try:
            snapshot = graph_app.get_state(stream_kwargs["config"])
            logger.info(f"history snapshot: {snapshot}")
            if snapshot and hasattr(snapshot, "values") and isinstance(snapshot.values, dict):
                prev_logs = snapshot.values.get("execution_log")
                if isinstance(prev_logs, list):
                    for hist in prev_logs:
                        node = hist.get("node", "unknown")
                        cfg_node = get_node_config(str(node))
                        out = {
                            "step": hist.get("step"),
                            "node": node,
                            "display_name": cfg_node["display_name"],
                            "description": cfg_node["description"],
                            "timestamp": hist.get("timestamp"),
                            "action": hist.get("action"),
                            "result": (hist.get("result")[:500] + "...") if isinstance(hist.get("result"), str) and len(hist.get("result")) > 500 else hist.get("result"),
                            "thread_id": thread_id,
                            "checkpoint": checkpoint
                        }
                        yield _format_sse(out, event="history")
        except Exception as _e:
            # ÂéÜÂè≤ÂõûÊîæÂ§±Ë¥•ÂøΩÁï•
            pass

    try:
        for mode in modes:
            # ÊØè‰∏™Ê®°ÂºèÁöÑËµ∑Âßã‰∫ã‰ª∂
            yield _format_sse({"mode": mode, "status": "start", "thread_id": thread_id, "checkpoint": checkpoint}, event="mode_start")
            if mode == "updates":
                for chunk in graph_app.stream(inputs, stream_mode=mode, **stream_kwargs):
                    for key, value in chunk.items():
                        if isinstance(value, dict) and "current_node" in value:
                            node_name = value["current_node"]
                            display_name = value.get("node_display_name", node_name)
                            description = value.get("node_description", "")
                            step_count = value.get("step_count", 0)
                            payload = {
                                "step": step_count,
                                "node": node_name,
                                "display_name": display_name,
                                "description": description,
                                "thread_id": thread_id,
                                "checkpoint": checkpoint
                            }
                            if "execution_log" in value:
                                for log in value["execution_log"]:
                                    if log.get("node") == node_name:
                                        result = log.get("result")
                                        if isinstance(result, str) and len(result) > 500:
                                            result = result[:500] + "..."
                                        payload["log"] = {
                                            "timestamp": log.get("timestamp"),
                                            "action": log.get("action"),
                                            "result": result
                                        }
                                        break
                            yield _format_sse(payload, event="node")
            # elif mode == "messages":
            #     for chunk in graph_app.stream(inputs, stream_mode=mode, **stream_kwargs):
            #         logger.info(f"chunk: {chunk}")
            #         # ‰ºòÂÖàÂ§ÑÁêÜ (msg, meta) ÂÖÉÁªÑ
            #         if isinstance(chunk, tuple) and len(chunk) == 2 and isinstance(chunk[1], dict):
            #             payload = _format_tuple_messages_chunk(chunk[0], chunk[1])
            #             yield _format_sse(payload, event="node")
            #             continue
            #         # Â§ÑÁêÜÂàóË°®ÔºöÂèØËÉΩÂåÖÂê´Â§ö‰∏™ (msg, meta)
            #         if isinstance(chunk, list):
            #             for item in chunk:
            #                 if isinstance(item, tuple) and len(item) == 2 and isinstance(item[1], dict):
            #                     payload = _format_tuple_messages_chunk(item[0], item[1])
            #                     yield _format_sse(payload, event="node")
            #                 else:
            #                     # ÂõûÈÄÄÂà∞ÈÄöÁî®Ëß£Êûê
            #                     for node_name, node_value in _iter_node_items(item):
            #                         derived_name = node_name if isinstance(node_name, str) else (
            #                             node_value.get("langgraph_node") if isinstance(node_value, dict) else "unknown"
            #                         )
            #                         cfg = get_node_config(str(derived_name))
            #                         messages = _extract_message_list(node_value)
            #                         logger.info(f"messages: {messages}")
            #                         logger.info(f"cfg: {cfg}")
            #                         payload = {
            #                             "node": derived_name,
            #                             "display_name": cfg["display_name"],
            #                             "description": cfg["description"],
            #                             "messages": messages,
            #                             "thread_id": thread_id,
            #                             "checkpoint": checkpoint
            #                         }
            #                         yield _format_sse(payload, event="node")
            #             continue
            #         # ÂÖ∂ÂÆÉÊÉÖÂÜµÔºö‰ΩøÁî®ÈÄöÁî®Ëß£Êûê
            #         for node_name, node_value in _iter_node_items(chunk):
            #             derived_name = node_name if isinstance(node_name, str) else (
            #                 node_value.get("langgraph_node") if isinstance(node_value, dict) else "unknown"
            #             )
            #             cfg = get_node_config(str(derived_name))
            #             messages = _extract_message_list(node_value)
            #             logger.info(f"messages: {messages}")
            #             logger.info(f"cfg: {cfg}")
            #             payload = {
            #                 "node": derived_name,
            #                 "display_name": cfg["display_name"],
            #                 "description": cfg["description"],
            #                 "messages": messages,
            #                 "thread_id": thread_id,
            #                 "checkpoint": checkpoint
            #             }
            #             yield _format_sse(payload, event="node")
            else:
                for chunk in graph_app.stream(inputs, stream_mode=mode, **stream_kwargs):
                    yield _format_sse({"chunk": chunk, "thread_id": thread_id, "checkpoint": checkpoint}, event=mode)
            # ÊØè‰∏™Ê®°ÂºèÁöÑÁªìÊùü‰∫ã‰ª∂
            yield _format_sse({"mode": mode, "status": "completed", "thread_id": thread_id, "checkpoint": checkpoint}, event="mode_end")
        yield _format_sse({"status": "completed", "thread_id": thread_id, "checkpoint": checkpoint}, event="end")
    except Exception as e:
        yield _format_sse({"status": "error", "message": str(e)}, event="error")

@app.get("/health")
def health():
    return {"status": "ok"}

@app.post("/stream")
def stream_post(body: dict = Body(...)):
    user_input = body.get("user_input") or ""
    stream_mode = body.get("stream_mode") or "updates"
    thread_id = body.get("thread_id")
    checkpoint = body.get("checkpoint")
    replay_history = bool(body.get("replay_history", False))
    if not user_input:
        def _err():
            yield _format_sse({"status": "error", "message": "missing user_input"}, event="error")
        return StreamingResponse(_err(), media_type="text/event-stream")
    return StreamingResponse(_node_sse_generator(user_input, stream_mode=stream_mode, thread_id=thread_id, checkpoint=checkpoint, replay_history=replay_history), media_type="text/event-stream")

@app.get("/stream")
def stream_get(
    user_input: str = Query(..., description="Áî®Êà∑ËæìÂÖ•"),
    stream_mode: str = Query("updates", description="ÊµÅÊ®°ÂºèÔºöÂèØÁî® updates,values,messages,debugÔºõÊîØÊåÅÁî®ÈÄóÂè∑/Á´ñÁ∫øÁªÑÂêà"),
    thread_id: str | None = Query(None, description="‰ºöËØùÁ∫øÁ®ãIDÔºåÁî®‰∫éÊåÅ‰πÖÂåñ‰∏éÊñ≠ÁÇπÁª≠‰Ωú"),
    checkpoint: str | None = Query(None, description="Êñ≠ÁÇπID/ÂêçÁß∞ÔºåÁî®‰∫é‰ªéÊåáÂÆöÊñ≠ÁÇπÊÅ¢Â§ç"),
    replay_history: bool = Query(False, description="ÊòØÂê¶Âú®ÂºÄÂßãÊó∂ÂõûÊîæÂéÜÂè≤ execution_log")
):
    return StreamingResponse(_node_sse_generator(user_input, stream_mode=stream_mode, thread_id=thread_id, checkpoint=checkpoint, replay_history=replay_history), media_type="text/event-stream")

@app.get("/history")
def history(
    thread_id: str = Query(..., description="‰ºöËØùÁ∫øÁ®ãID"),
    checkpoint: str | None = Query(None, description="ÂèØÈÄâÁöÑÊñ≠ÁÇπID/ÂêçÁß∞")
):
    def _history_sse_generator():
        # ËØªÂèñÁä∂ÊÄÅÂø´ÁÖß
        config = {"configurable": {"thread_id": thread_id}}
        if checkpoint:
            config["configurable"]["checkpoint_id"] = checkpoint
            config["configurable"]["checkpoint"] = checkpoint
        logger.info(f"history config: {config}")
        try:
            snapshot = graph_app.get_state(config=config)
            logger.info(f"history snapshot: {snapshot}")
            # ÂºÄÂßã‰∫ã‰ª∂
            yield _format_sse({"status": "start", "thread_id": thread_id, "checkpoint": checkpoint}, event="history_start")
            store = None
            if snapshot is not None:
                if hasattr(snapshot, "values"):
                    store = snapshot.values
                elif hasattr(snapshot, "value"):
                    store = snapshot.value
            prev_logs = store.get("execution_log") if isinstance(store, dict) else None
            if isinstance(prev_logs, list) and prev_logs:
                for hist in prev_logs:
                    node = hist.get("node", "unknown")
                    cfg_node = get_node_config(str(node))
                    out = {
                        "step": hist.get("step"),
                        "node": node,
                        "display_name": cfg_node["display_name"],
                        "description": cfg_node["description"],
                        "timestamp": hist.get("timestamp"),
                        "action": hist.get("action"),
                        "result": (hist.get("result")[:500] + "...") if isinstance(hist.get("result"), str) and len(hist.get("result")) > 500 else hist.get("result"),
                        "thread_id": thread_id,
                        "checkpoint": checkpoint
                    }
                    yield _format_sse(out, event="history")
            else:
                yield _format_sse({"thread_id": thread_id, "checkpoint": checkpoint, "history": []}, event="history_empty")
            # ÁªìÊùü‰∫ã‰ª∂
            yield _format_sse({"status": "completed", "thread_id": thread_id, "checkpoint": checkpoint}, event="end")
        except Exception as e:
            yield _format_sse({"status": "error", "message": str(e)}, event="error")

    return StreamingResponse(_history_sse_generator(), media_type="text/event-stream")

# ===== Êñ∞Â¢ûÔºöWebSocket Á´ØÁÇπ =====
@app.websocket("/ws")
async def websocket_stream(websocket: WebSocket):
    await websocket.accept()
    try:
        while True:
            try:
                message_text = await websocket.receive_text()
            except WebSocketDisconnect:
                break
            except Exception:
                # Êú™ËÉΩËß£ÊûêÊ∂àÊÅØÔºåÂèëÈÄÅÈîôËØØÂπ∂ÁªßÁª≠
                await websocket.send_text(_format_sse({"status": "error", "message": "invalid message"}, event="error"))
                continue
            try:
                body = json.loads(message_text) if message_text else {}
            except Exception:
                body = {}
            user_input = body.get("user_input") or ""
            stream_mode = body.get("stream_mode") or "updates"
            thread_id = body.get("thread_id")
            checkpoint = body.get("checkpoint")
            replay_history = bool(body.get("replay_history", False))
            if not user_input:
                await websocket.send_text(_format_sse({"status": "error", "message": "missing user_input"}, event="error"))
                continue
            # Â∞ÜÁîüÊàêÁöÑ‰∫ã‰ª∂ÈÄöËøá WebSocket ÈÄêÊù°ÂèëÈÄÅÔºàÊñáÊú¨Â∏ßÔºâ
            for line in _node_sse_generator(
                user_input,
                stream_mode=stream_mode,
                thread_id=thread_id,
                checkpoint=checkpoint,
                replay_history=replay_history,
            ):
                await websocket.send_text(line)
    except WebSocketDisconnect:
        pass
    except Exception as e:
        try:
            await websocket.send_text(_format_sse({"status": "error", "message": str(e)}, event="error"))
        except Exception:
            pass
    finally:
        try:
            await websocket.close()
        except Exception:
            pass

def test_node_name_tracking():
    """ÊµãËØïËäÇÁÇπÂêçÁß∞Ë∑üË∏™ÂäüËÉΩ"""
    print("üöÄ ËäÇÁÇπÂêçÁß∞Ë∑üË∏™ÊµãËØï")
    print("=" * 60)
    
    # ÂàõÂª∫Â∑•‰ΩúÊµÅ
    graph = create_node_tracking_workflow()
    
    # ÊµãËØïËæìÂÖ•
    user_input = "ÊàëÊÉ≥Â≠¶‰π†PythonÁºñÁ®ã"
    
    print(f"üìù ËæìÂÖ•: {user_input}")
    print("-" * 40)
    
    # ÂáÜÂ§áËæìÂÖ•Áä∂ÊÄÅ
    inputs = {
        "user_input": user_input,
        "execution_log": [],
        "step_count": 0
    }
    
    print("üîç ËäÇÁÇπÊâßË°åË∑üË∏™:")
    print("-" * 40)
    
    try:
        # ‰ΩøÁî® updates Ê®°ÂºèË∑üË∏™ËäÇÁÇπÊâßË°å
        for chunk in graph.stream(inputs, stream_mode="updates"):
            # ÊèêÂèñËäÇÁÇπ‰ø°ÊÅØ
            for key, value in chunk.items():
                if isinstance(value, dict) and "current_node" in value:
                    node_name = value["current_node"]
                    display_name = value.get("node_display_name", node_name)
                    description = value.get("node_description", "")
                    step_count = value.get("step_count", 0)
                    
                    print(f"üìç Ê≠•È™§ {step_count}: {display_name}")
                    print(f"   üìã ÊèèËø∞: {description}")
                    print(f"   üîß ËäÇÁÇπÂêç: {node_name}")
                    
                    # ÊòæÁ§∫ËäÇÁÇπÊâßË°åÊó•Âøó
                    if "execution_log" in value:
                        for log in value["execution_log"]:
                            if log.get("node") == node_name:
                                print(f"   ‚è∞ Êó∂Èó¥: {log.get('timestamp', 'N/A')}")
                                print(f"   üéØ Âä®‰Ωú: {log.get('action', 'N/A')}")
                                if "result" in log:
                                    result = log["result"]
                                    if len(result) > 80:
                                        result = result[:80] + "..."
                                    print(f"   üìä ÁªìÊûú: {result}")
                                print()
                    
    except Exception as e:
        print(f"‚ùå ÈîôËØØ: {e}")

def show_node_configurations():
    """ÊòæÁ§∫ËäÇÁÇπÈÖçÁΩÆ"""
    print("\nüé® ËäÇÁÇπÈÖçÁΩÆÂ±ïÁ§∫")
    print("=" * 60)
    
    print("ÂèØÁî®ËäÇÁÇπÈÖçÁΩÆ:")
    for node_name, config in NODE_CONFIGS.items():
        print(f"\n{config['emoji']} {config['display_name']}")
        print(f"   ËäÇÁÇπÂêç: {node_name}")
        print(f"   ÊèèËø∞: {config['description']}")
    
    print("\nÈÖçÁΩÆÁâπÁÇπ:")
    print("‚úÖ ÊîØÊåÅ‰∏≠ÊñáÊòæÁ§∫ÂêçÁß∞")
    print("‚úÖ ÊîØÊåÅemojiË°®ÊÉÖÁ¨¶Âè∑")
    print("‚úÖ ËØ¶ÁªÜÁöÑËäÇÁÇπÊèèËø∞")
    print("‚úÖ Áªü‰∏ÄÁöÑÈÖçÁΩÆÁÆ°ÁêÜ")
    print("‚úÖ Âä®ÊÄÅËäÇÁÇπ‰ø°ÊÅØËé∑Âèñ")

def demonstrate_custom_node_names():
    """ÊºîÁ§∫Ëá™ÂÆö‰πâËäÇÁÇπÂêçÁß∞ÂäüËÉΩ"""
    print("\nüéØ Ëá™ÂÆö‰πâËäÇÁÇπÂêçÁß∞ÊºîÁ§∫")
    print("=" * 60)
    
    # ÂàõÂª∫Â∑•‰ΩúÊµÅ
    graph = create_node_tracking_workflow()
    
    # ÊµãËØïËæìÂÖ•
    test_inputs = [
        "Â¶Ç‰ΩïÂ≠¶‰π†Êú∫Âô®Â≠¶‰π†",
        "Êé®Ëçê‰∏Ä‰∫õÁºñÁ®ã‰π¶Á±ç",
        "PythonÊúâ‰ªÄ‰πà‰ºòÂäø"
    ]
    
    for i, user_input in enumerate(test_inputs, 1):
        print(f"\nüìù ÊµãËØï {i}: {user_input}")
        print("-" * 40)
        
        # ÂáÜÂ§áËæìÂÖ•Áä∂ÊÄÅ
        inputs = {
            "user_input": user_input,
            "execution_log": [],
            "step_count": 0
        }
        
        try:
            # ÊâßË°åÂ∑•‰ΩúÊµÅÂπ∂Ë∑üË∏™ËäÇÁÇπ
            node_execution_order = []
            
            for chunk in graph.stream(inputs, stream_mode="updates"):
                for key, value in chunk.items():
                    if isinstance(value, dict) and "current_node" in value:
                        node_name = value["current_node"]
                        display_name = value.get("node_display_name", node_name)
                        step_count = value.get("step_count", 0)
                        
                        if step_count not in [log["step"] for log in node_execution_order]:
                            node_execution_order.append({
                                "step": step_count,
                                "node": node_name,
                                "display_name": display_name
                            })
            
            # ÊòæÁ§∫ÊâßË°åÈ°∫Â∫è
            print("ÊâßË°åÈ°∫Â∫è:")
            for execution in node_execution_order:
                print(f"  {execution['step']}. {execution['display_name']}")
                
        except Exception as e:
            print(f"‚ùå ÈîôËØØ: {e}")

if __name__ == "__main__":
    # ÈªòËÆ§ÂêØÂä® FastAPI ÊúçÂä°Ôºà‰ΩøÁî®ÂØºÂÖ•Â≠óÁ¨¶‰∏≤‰ª•ÂêØÁî® reload/workersÔºâ
    # todo cd langgraph_demo/study 
    import uvicorn
    uvicorn.run("11_stream_fastapi_ws:app", host="0.0.0.0", port=8000, reload=True)
    
    print("\n‚úÖ ËäÇÁÇπÂêçÁß∞Ëá™ÂÆö‰πâÁ§∫‰æãÂÆåÊàêÔºÅ")
    print("\nüìö Â≠¶‰π†Ë¶ÅÁÇπÊÄªÁªì:")
    print("1. ËäÇÁÇπÂêçÁß∞ÈÖçÁΩÆ: ÊîØÊåÅ‰∏≠Êñá„ÄÅemojiÂíåÊèèËø∞")
    print("2. Áä∂ÊÄÅË∑üË∏™: ÂÆûÊó∂ÁõëÊéßÂΩìÂâçÊâßË°åÁöÑËäÇÁÇπ")
    print("3. ÊòæÁ§∫ÂêçÁß∞: Áî®Êà∑ÂèãÂ•ΩÁöÑËäÇÁÇπÂ±ïÁ§∫")
    print("4. ÈÖçÁΩÆÁÆ°ÁêÜ: Áªü‰∏ÄÁöÑËäÇÁÇπÈÖçÁΩÆÂ≠óÂÖ∏")
    print("5. Âä®ÊÄÅËé∑Âèñ: Ê†πÊçÆËäÇÁÇπÂêçËé∑ÂèñÈÖçÁΩÆ‰ø°ÊÅØ")
    print("6. ÊâßË°åÊó•Âøó: ËØ¶ÁªÜÁöÑËäÇÁÇπÊâßË°åËÆ∞ÂΩï") 