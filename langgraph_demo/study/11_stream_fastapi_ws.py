# -*- coding: utf-8 -*-
"""
LangGraph èŠ‚ç‚¹åç§°è‡ªå®šä¹‰ç¤ºä¾‹
å­¦ä¹ è¦ç‚¹ï¼šèŠ‚ç‚¹åç§°è·Ÿè¸ªã€è‡ªå®šä¹‰æ˜¾ç¤ºåç§°ã€åŠ¨æ€åç§°é…ç½®
"""

import os
import sys
import time
import json
from typing import TypedDict, Annotated
import operator

# æ·»åŠ å½“å‰è„šæœ¬æ‰€åœ¨ç›®å½•åˆ°è·¯å¾„ï¼Œç¡®ä¿èƒ½å¯¼å…¥æœ¬ç›®å½•ä¸‹çš„ config.py
CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))
if CURRENT_DIR not in sys.path:
    sys.path.append(CURRENT_DIR)

from langgraph.graph import StateGraph, START, END
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage
import config
# è·å–æ—¥å¿—å™¨
logger = config.logger
# è®¾ç½®ç¯å¢ƒå˜é‡
os.environ["OPENAI_API_BASE"] = config.base_url
os.environ["OPENAI_API_KEY"] = config.api_key
MODEL_NAME = config.model

# åˆå§‹åŒ–è¯­è¨€æ¨¡å‹
llm = ChatOpenAI(
    model=MODEL_NAME,
    temperature=0.1,
    max_tokens=500
)

# å®šä¹‰çŠ¶æ€ç»“æ„
class NodeTrackingState(TypedDict):
    """èŠ‚ç‚¹è·Ÿè¸ªçŠ¶æ€å®šä¹‰"""
    user_input: str                    # ç”¨æˆ·è¾“å…¥
    current_node: str                  # å½“å‰èŠ‚ç‚¹åç§°
    node_display_name: str             # èŠ‚ç‚¹æ˜¾ç¤ºåç§°
    node_description: str              # èŠ‚ç‚¹æè¿°
    execution_log: Annotated[list, operator.add]  # æ‰§è¡Œæ—¥å¿—
    step_count: int                    # æ­¥éª¤è®¡æ•°

# èŠ‚ç‚¹é…ç½®å­—å…¸
NODE_CONFIGS = {
    "data_processor": {
        "display_name": "ğŸ“Š æ•°æ®é¢„å¤„ç†å¼•æ“",
        "description": "æ™ºèƒ½å¤„ç†å’Œåˆ†æè¾“å…¥æ•°æ®",
        "emoji": "ğŸ“Š"
    },
    "ai_analyzer": {
        "display_name": "ğŸ§  AIæ™ºèƒ½åˆ†æå™¨",
        "description": "ä½¿ç”¨æ·±åº¦å­¦ä¹ æŠ€æœ¯åˆ†æç”¨æˆ·æ„å›¾",
        "emoji": "ğŸ§ "
    },
    "recommendation_engine": {
        "display_name": "ğŸ¯ æ™ºèƒ½æ¨èå¼•æ“",
        "description": "åŸºäºAIç®—æ³•ç”Ÿæˆä¸ªæ€§åŒ–æ¨è",
        "emoji": "ğŸ¯"
    },
    "response_generator": {
        "display_name": "âœ¨ å“åº”ç”Ÿæˆå™¨",
        "description": "æ•´åˆæ‰€æœ‰ç»“æœç”Ÿæˆæœ€ç»ˆå“åº”",
        "emoji": "âœ¨"
    }
}

def get_node_config(node_name: str) -> dict:
    """è·å–èŠ‚ç‚¹é…ç½®"""
    return NODE_CONFIGS.get(node_name, {
        "display_name": f"ğŸ”§ {node_name}",
        "description": "é»˜è®¤èŠ‚ç‚¹æè¿°",
        "emoji": "ğŸ”§"
    })

def create_node_log(node_name: str, action: str, step_count: int, **kwargs) -> dict:
    """åˆ›å»ºèŠ‚ç‚¹æ—¥å¿—"""
    config = get_node_config(node_name)
    return {
        "step": step_count,
        "node": node_name,
        "display_name": config["display_name"],
        "description": config["description"],
        "emoji": config["emoji"],
        "action": action,
        "timestamp": time.strftime("%H:%M:%S"),
        **kwargs
    }

# å®šä¹‰èŠ‚ç‚¹å‡½æ•°
def data_processor_node(state: NodeTrackingState) -> NodeTrackingState:
    """
    æ•°æ®é¢„å¤„ç†èŠ‚ç‚¹
    å­¦ä¹ è¦ç‚¹ï¼šèŠ‚ç‚¹åç§°é…ç½®ã€æ—¥å¿—è®°å½•
    """
    user_input = state["user_input"]
    step_count = state.get("step_count", 0) + 1
    node_name = "data_processor"
    
    # è·å–èŠ‚ç‚¹é…ç½®
    config = get_node_config(node_name)
    
    # åˆ›å»ºæ‰§è¡Œæ—¥å¿—
    log_entry = create_node_log(
        node_name=node_name,
        action="å¼€å§‹æ•°æ®é¢„å¤„ç†",
        step_count=step_count,
        input=user_input
    )
    
    # æ¨¡æ‹Ÿæ•°æ®å¤„ç†
    time.sleep(0.2)
    processed_data = f"é¢„å¤„ç†ç»“æœ: {user_input.upper()}"
    
    log_entry["result"] = processed_data
    log_entry["status"] = "completed"
    
    return {
        "current_node": node_name,
        "node_display_name": config["display_name"],
        "node_description": config["description"],
        "step_count": step_count,
        "execution_log": [log_entry]
    }

def ai_analyzer_node(state: NodeTrackingState) -> NodeTrackingState:
    """
    AIåˆ†æèŠ‚ç‚¹
    å­¦ä¹ è¦ç‚¹ï¼šLLMè°ƒç”¨ã€èŠ‚ç‚¹çŠ¶æ€è·Ÿè¸ª
    """
    user_input = state["user_input"]
    step_count = state.get("step_count", 0) + 1
    node_name = "ai_analyzer"
    
    # è·å–èŠ‚ç‚¹é…ç½®
    config = get_node_config(node_name)
    
    # åˆ›å»ºæ‰§è¡Œæ—¥å¿—
    log_entry = create_node_log(
        node_name=node_name,
        action="å¼€å§‹AIåˆ†æ",
        step_count=step_count,
        input=user_input
    )
    
    # ä½¿ç”¨LLMè¿›è¡Œåˆ†æ
    prompt = f"""
è¯·åˆ†æç”¨æˆ·è¾“å…¥: "{user_input}"
æä¾›ç®€è¦çš„åˆ†æç»“æœã€‚
"""
    
    response = llm.invoke([HumanMessage(content=prompt)])
    analysis_result = response.content
    
    log_entry["result"] = analysis_result
    log_entry["status"] = "completed"
    
    return {
        "current_node": node_name,
        "node_display_name": config["display_name"],
        "node_description": config["description"],
        "step_count": step_count,
        "execution_log": [log_entry]
    }

def recommendation_engine_node(state: NodeTrackingState) -> NodeTrackingState:
    """
    æ¨èå¼•æ“èŠ‚ç‚¹
    å­¦ä¹ è¦ç‚¹ï¼šåŠ¨æ€èŠ‚ç‚¹åç§°ã€çŠ¶æ€ä¼ é€’
    """
    user_input = state["user_input"]
    step_count = state.get("step_count", 0) + 1
    node_name = "recommendation_engine"
    
    # è·å–èŠ‚ç‚¹é…ç½®
    config = get_node_config(node_name)
    
    # åˆ›å»ºæ‰§è¡Œæ—¥å¿—
    log_entry = create_node_log(
        node_name=node_name,
        action="å¼€å§‹ç”Ÿæˆæ¨è",
        step_count=step_count,
        input=user_input
    )
    
    # æ¨¡æ‹Ÿæ¨èç”Ÿæˆ
    time.sleep(0.1)
    recommendation = f"åŸºäº '{user_input}' çš„æ¨è: å»ºè®®æ·±å…¥å­¦ä¹ ç›¸å…³æŠ€æœ¯"
    
    log_entry["result"] = recommendation
    log_entry["status"] = "completed"
    
    return {
        "current_node": node_name,
        "node_display_name": config["display_name"],
        "node_description": config["description"],
        "step_count": step_count,
        "execution_log": [log_entry]
    }

def response_generator_node(state: NodeTrackingState) -> NodeTrackingState:
    """
    å“åº”ç”Ÿæˆå™¨èŠ‚ç‚¹
    å­¦ä¹ è¦ç‚¹ï¼šæœ€ç»ˆçŠ¶æ€æ•´åˆã€èŠ‚ç‚¹åç§°å±•ç¤º
    """
    user_input = state["user_input"]
    step_count = state.get("step_count", 0) + 1
    node_name = "response_generator"
    
    # è·å–èŠ‚ç‚¹é…ç½®
    config = get_node_config(node_name)
    
    # åˆ›å»ºæ‰§è¡Œæ—¥å¿—
    log_entry = create_node_log(
        node_name=node_name,
        action="å¼€å§‹ç”Ÿæˆæœ€ç»ˆå“åº”",
        step_count=step_count,
        input=user_input
    )
    
    # ç”Ÿæˆæœ€ç»ˆå“åº”
    final_response = f"""
ğŸ‰ å¤„ç†å®Œæˆï¼

ç”¨æˆ·è¾“å…¥: {user_input}
å¤„ç†æ­¥éª¤: {step_count} æ­¥
å½“å‰èŠ‚ç‚¹: {config["display_name"]}

æ„Ÿè°¢æ‚¨çš„ä½¿ç”¨ï¼
"""
    
    log_entry["result"] = final_response
    log_entry["status"] = "completed"
    
    return {
        "current_node": node_name,
        "node_display_name": config["display_name"],
        "node_description": config["description"],
        "step_count": step_count,
        "execution_log": [log_entry]
    }

def create_node_tracking_workflow():
    """åˆ›å»ºèŠ‚ç‚¹è·Ÿè¸ªå·¥ä½œæµ"""
    workflow = StateGraph(NodeTrackingState)
    
    # æ·»åŠ èŠ‚ç‚¹
    workflow.add_node("data_processor", data_processor_node)
    workflow.add_node("ai_analyzer", ai_analyzer_node)
    workflow.add_node("recommendation_engine", recommendation_engine_node)
    workflow.add_node("response_generator", response_generator_node)
    
    # è®¾ç½®æµç¨‹
    workflow.set_entry_point("data_processor")
    workflow.add_edge("data_processor", "ai_analyzer")
    workflow.add_edge("ai_analyzer", "recommendation_engine")
    workflow.add_edge("recommendation_engine", "response_generator")
    workflow.add_edge("response_generator", END)
    
    return workflow

# ===== FastAPI åº”ç”¨ä¸ WebSocket/SSE æµå¼æ¥å£ =====
from fastapi import FastAPI, Body, Query
from fastapi.responses import StreamingResponse, JSONResponse
from fastapi.middleware.cors import CORSMiddleware
from fastapi import WebSocket, WebSocketDisconnect

app = FastAPI(title="LangGraph Streaming API", version="0.1.0")
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# åœ¨åº”ç”¨å¯åŠ¨æ—¶åˆ›å»ºä¸€æ¬¡å·¥ä½œæµï¼Œé¿å…é‡å¤æ„å»º
from langgraph.checkpoint.memory import MemorySaver
_memory_checkpointer = MemorySaver()
# ä»¥å¸¦æ£€æŸ¥ç‚¹çš„å½¢å¼ç¼–è¯‘
graph_app = create_node_tracking_workflow().compile(checkpointer=_memory_checkpointer)

ALLOWED_STREAM_MODES = {"updates", "values", "messages", "debug"}

def _safe_serialize(obj):
    try:
        json.dumps(obj)
        return obj
    except TypeError:
        if isinstance(obj, dict):
            return {k: _safe_serialize(v) for k, v in obj.items()}
        if isinstance(obj, list):
            return [_safe_serialize(v) for v in obj]
        return str(obj)

def _format_sse(data: dict, event: str = "message") -> str:
    payload = {"event": event, "data": _safe_serialize(data)}
    return json.dumps(payload, ensure_ascii=False) + "\n"

def _parse_stream_modes(mode_param):
    if not mode_param:
        return ["updates"], []
    raw_list = []
    if isinstance(mode_param, str):
        raw_list = [p.strip() for p in mode_param.replace("|", ",").split(",") if p.strip()]
    elif isinstance(mode_param, (list, tuple, set)):
        for item in mode_param:
            if isinstance(item, str):
                raw_list.extend([p.strip() for p in item.replace("|", ",").split(",") if p.strip()])
    # å»é‡å¹¶æ ¡éªŒ
    uniq = []
    for m in raw_list:
        if m not in uniq:
            uniq.append(m)
    valid = [m for m in uniq if m in ALLOWED_STREAM_MODES]
    invalid = [m for m in uniq if m not in ALLOWED_STREAM_MODES]
    if not valid:
        valid = ["updates"]
    return valid, invalid

def _normalize_content(content):
    """å°†å¤æ‚ message content è§„æ•´ä¸ºå­—ç¬¦ä¸²ï¼Œé¿å…ä¸å¯åºåˆ—åŒ–/ä¸å¯å“ˆå¸Œå¯¹è±¡å‘ä¸‹æ¸¸ä¼ æ’­ã€‚"""
    try:
        if content is None:
            return ""
        if isinstance(content, str):
            return content
        if isinstance(content, (int, float, bool)):
            return str(content)
        # LangChain å¸¸è§ï¼šlist[dict|str|obj]
        if isinstance(content, list):
            parts = []
            for item in content:
                if isinstance(item, str):
                    parts.append(item)
                elif isinstance(item, dict):
                    # ä¼˜å…ˆæå– text å­—æ®µ
                    txt = item.get("text") or item.get("content")
                    if isinstance(txt, str):
                        parts.append(txt)
                    else:
                        parts.append(json.dumps(_safe_serialize(item), ensure_ascii=False))
                else:
                    # å…œåº•ä¸ºå­—ç¬¦ä¸²
                    parts.append(str(item))
            return "".join(parts)
        if isinstance(content, dict):
            # å°è¯•æå– textï¼Œå¦åˆ™æ•´ä½“åºåˆ—åŒ–
            txt = content.get("text") or content.get("content")
            if isinstance(txt, str):
                return txt
            return json.dumps(_safe_serialize(content), ensure_ascii=False)
        # å¯¹è±¡ï¼šå°è¯•å– .text/.value/.content ç­‰
        for attr in ("text", "value", "content"):
            if hasattr(content, attr):
                val = getattr(content, attr)
                return _normalize_content(val)
        return str(content)
    except Exception:
        return str(content)

def _extract_message_list(value):
    messages = []
    src = None
    if isinstance(value, dict) and "messages" in value:
        src = value["messages"]
    elif isinstance(value, list):
        src = value
    else:
        src = [value]
    for m in src:
        role = getattr(m, "type", None) or getattr(m, "role", None)
        content = getattr(m, "content", None)
        if isinstance(m, dict):
            role = m.get("type") or m.get("role") or role
            content = m.get("content") or content
        content = _normalize_content(content)
        messages.append({"role": role or "unknown", "content": content})
    return messages

def _iter_node_items(chunk):
    """å…¼å®¹æ€§è¿­ä»£ï¼šå°† chunk ç»Ÿä¸€ä¸º (node_name, node_value) åºåˆ—ã€‚
    æ”¯æŒ dictã€(node, value) å…ƒç»„ã€[(node,value), ...] åˆ—è¡¨ã€æˆ–å…¶å®ƒç±»å‹ã€‚
    """
    if isinstance(chunk, dict):
        for k, v in chunk.items():
            yield k, v
        return
    if isinstance(chunk, tuple):
        if len(chunk) == 2:
            yield chunk[0], chunk[1]
            return
    if isinstance(chunk, list):
        for item in chunk:
            if isinstance(item, tuple) and len(item) == 2:
                yield item[0], item[1]
            elif isinstance(item, dict):
                for k, v in item.items():
                    yield k, v
            else:
                yield "unknown", item
        return
    # å…¶å®ƒæœªçŸ¥ç±»å‹ï¼Œä½œä¸ºå•ä¸ªå€¼è¾“å‡º
    yield "unknown", chunk

def _format_tuple_messages_chunk(msg_obj, meta: dict) -> dict:
    """å°† (MessageChunk/Message, meta) è§„èŒƒåŒ–ä¸ºç»Ÿä¸€è´Ÿè½½ã€‚"""
    node_name = meta.get("langgraph_node") or "unknown"
    cfg = get_node_config(str(node_name))
    content = _normalize_content(getattr(msg_obj, "content", msg_obj))
    role = getattr(msg_obj, "type", None) or getattr(msg_obj, "role", None) or "assistant"
    payload = {
        "node": node_name,
        "display_name": cfg["display_name"],
        "description": cfg["description"],
        "step": meta.get("langgraph_step"),
        "path": meta.get("langgraph_path"),
        "triggers": meta.get("langgraph_triggers"),
        "provider": meta.get("ls_provider"),
        "model": meta.get("ls_model_name"),
        "message": {
            "role": role,
            "content": content
        }
    }
    return payload

def _node_sse_generator(user_input: str, stream_mode = "updates", thread_id: str | None = None, checkpoint: str | None = None, replay_history: bool = True):
    modes, invalid = _parse_stream_modes(stream_mode)
    if invalid:
        yield _format_sse({"warning": "invalid_modes", "ignored": invalid, "allowed": sorted(list(ALLOWED_STREAM_MODES))}, event="warning")
    inputs = {
        "user_input": user_input,
        "execution_log": [],
        "step_count": 0
    }
    # é€ä¼ ä¼šè¯ä¸æ–­ç‚¹é…ç½®
    stream_kwargs = {}
    cfg = {}
    configurable = {}
    if thread_id:
        configurable["thread_id"] = thread_id
    if checkpoint:
        # åŒæ­¥ä¼ é€’ä¸¤ç§å¸¸è§é”®ï¼Œä¾¿äºä¸åŒç‰ˆæœ¬å…¼å®¹
        configurable["checkpoint"] = checkpoint
        configurable["checkpoint_id"] = checkpoint
    if configurable:
        cfg["configurable"] = configurable
    if cfg:
        stream_kwargs["config"] = cfg
        
    logger.info(f"stream_kwargs: {stream_kwargs}")

    # æŒ‰éœ€å›æ”¾å†å²ï¼šå°†å·²æœ‰ execution_log ä»¥ç‹¬ç«‹çš„ history äº‹ä»¶è¾“å‡º

    if replay_history and stream_kwargs.get("config"):
        try:
            snapshot = graph_app.get_state(stream_kwargs["config"])
            logger.info(f"history snapshot: {snapshot}")
            if snapshot and hasattr(snapshot, "values") and isinstance(snapshot.values, dict):
                prev_logs = snapshot.values.get("execution_log")
                if isinstance(prev_logs, list):
                    for hist in prev_logs:
                        node = hist.get("node", "unknown")
                        cfg_node = get_node_config(str(node))
                        out = {
                            "step": hist.get("step"),
                            "node": node,
                            "display_name": cfg_node["display_name"],
                            "description": cfg_node["description"],
                            "timestamp": hist.get("timestamp"),
                            "action": hist.get("action"),
                            "result": (hist.get("result")[:500] + "...") if isinstance(hist.get("result"), str) and len(hist.get("result")) > 500 else hist.get("result"),
                            "thread_id": thread_id,
                            "checkpoint": checkpoint
                        }
                        yield _format_sse(out, event="history")
        except Exception as _e:
            # å†å²å›æ”¾å¤±è´¥å¿½ç•¥
            pass

    try:
        for mode in modes:
            # æ¯ä¸ªæ¨¡å¼çš„èµ·å§‹äº‹ä»¶
            yield _format_sse({"mode": mode, "status": "start", "thread_id": thread_id, "checkpoint": checkpoint}, event="mode_start")
            if mode == "updates":
                for chunk in graph_app.stream(inputs, stream_mode=mode, **stream_kwargs):
                    for key, value in chunk.items():
                        if isinstance(value, dict) and "current_node" in value:
                            node_name = value["current_node"]
                            display_name = value.get("node_display_name", node_name)
                            description = value.get("node_description", "")
                            step_count = value.get("step_count", 0)
                            payload = {
                                "step": step_count,
                                "node": node_name,
                                "display_name": display_name,
                                "description": description,
                                "thread_id": thread_id,
                                "checkpoint": checkpoint
                            }
                            if "execution_log" in value:
                                for log in value["execution_log"]:
                                    if log.get("node") == node_name:
                                        result = log.get("result")
                                        if isinstance(result, str) and len(result) > 500:
                                            result = result[:500] + "..."
                                        payload["log"] = {
                                            "timestamp": log.get("timestamp"),
                                            "action": log.get("action"),
                                            "result": result
                                        }
                                        break
                            yield _format_sse(payload, event="node")
            # elif mode == "messages":
            #     for chunk in graph_app.stream(inputs, stream_mode=mode, **stream_kwargs):
            #         logger.info(f"chunk: {chunk}")
            #         # ä¼˜å…ˆå¤„ç† (msg, meta) å…ƒç»„
            #         if isinstance(chunk, tuple) and len(chunk) == 2 and isinstance(chunk[1], dict):
            #             payload = _format_tuple_messages_chunk(chunk[0], chunk[1])
            #             yield _format_sse(payload, event="node")
            #             continue
            #         # å¤„ç†åˆ—è¡¨ï¼šå¯èƒ½åŒ…å«å¤šä¸ª (msg, meta)
            #         if isinstance(chunk, list):
            #             for item in chunk:
            #                 if isinstance(item, tuple) and len(item) == 2 and isinstance(item[1], dict):
            #                     payload = _format_tuple_messages_chunk(item[0], item[1])
            #                     yield _format_sse(payload, event="node")
            #                 else:
            #                     # å›é€€åˆ°é€šç”¨è§£æ
            #                     for node_name, node_value in _iter_node_items(item):
            #                         derived_name = node_name if isinstance(node_name, str) else (
            #                             node_value.get("langgraph_node") if isinstance(node_value, dict) else "unknown"
            #                         )
            #                         cfg = get_node_config(str(derived_name))
            #                         messages = _extract_message_list(node_value)
            #                         logger.info(f"messages: {messages}")
            #                         logger.info(f"cfg: {cfg}")
            #                         payload = {
            #                             "node": derived_name,
            #                             "display_name": cfg["display_name"],
            #                             "description": cfg["description"],
            #                             "messages": messages,
            #                             "thread_id": thread_id,
            #                             "checkpoint": checkpoint
            #                         }
            #                         yield _format_sse(payload, event="node")
            #             continue
            #         # å…¶å®ƒæƒ…å†µï¼šä½¿ç”¨é€šç”¨è§£æ
            #         for node_name, node_value in _iter_node_items(chunk):
            #             derived_name = node_name if isinstance(node_name, str) else (
            #                 node_value.get("langgraph_node") if isinstance(node_value, dict) else "unknown"
            #             )
            #             cfg = get_node_config(str(derived_name))
            #             messages = _extract_message_list(node_value)
            #             logger.info(f"messages: {messages}")
            #             logger.info(f"cfg: {cfg}")
            #             payload = {
            #                 "node": derived_name,
            #                 "display_name": cfg["display_name"],
            #                 "description": cfg["description"],
            #                 "messages": messages,
            #                 "thread_id": thread_id,
            #                 "checkpoint": checkpoint
            #             }
            #             yield _format_sse(payload, event="node")
            else:
                for chunk in graph_app.stream(inputs, stream_mode=mode, **stream_kwargs):
                    yield _format_sse({"chunk": chunk, "thread_id": thread_id, "checkpoint": checkpoint}, event=mode)
            # æ¯ä¸ªæ¨¡å¼çš„ç»“æŸäº‹ä»¶
            yield _format_sse({"mode": mode, "status": "completed", "thread_id": thread_id, "checkpoint": checkpoint}, event="mode_end")
        yield _format_sse({"status": "completed", "thread_id": thread_id, "checkpoint": checkpoint}, event="end")
    except Exception as e:
        yield _format_sse({"status": "error", "message": str(e)}, event="error")

@app.get("/health")
def health():
    return {"status": "ok"}

@app.post("/stream")
def stream_post(body: dict = Body(...)):
    user_input = body.get("user_input") or ""
    stream_mode = body.get("stream_mode") or "updates"
    thread_id = body.get("thread_id")
    checkpoint = body.get("checkpoint")
    replay_history = bool(body.get("replay_history", False))
    if not user_input:
        def _err():
            yield _format_sse({"status": "error", "message": "missing user_input"}, event="error")
        return StreamingResponse(_err(), media_type="text/event-stream")
    return StreamingResponse(_node_sse_generator(user_input, stream_mode=stream_mode, thread_id=thread_id, checkpoint=checkpoint, replay_history=replay_history), media_type="text/event-stream")

@app.get("/stream")
def stream_get(
    user_input: str = Query(..., description="ç”¨æˆ·è¾“å…¥"),
    stream_mode: str = Query("updates", description="æµæ¨¡å¼ï¼šå¯ç”¨ updates,values,messages,debugï¼›æ”¯æŒç”¨é€—å·/ç«–çº¿ç»„åˆ"),
    thread_id: str | None = Query(None, description="ä¼šè¯çº¿ç¨‹IDï¼Œç”¨äºæŒä¹…åŒ–ä¸æ–­ç‚¹ç»­ä½œ"),
    checkpoint: str | None = Query(None, description="æ–­ç‚¹ID/åç§°ï¼Œç”¨äºä»æŒ‡å®šæ–­ç‚¹æ¢å¤"),
    replay_history: bool = Query(False, description="æ˜¯å¦åœ¨å¼€å§‹æ—¶å›æ”¾å†å² execution_log")
):
    return StreamingResponse(_node_sse_generator(user_input, stream_mode=stream_mode, thread_id=thread_id, checkpoint=checkpoint, replay_history=replay_history), media_type="text/event-stream")

@app.get("/history")
def history(
    thread_id: str = Query(..., description="ä¼šè¯çº¿ç¨‹ID"),
    checkpoint: str | None = Query(None, description="å¯é€‰çš„æ–­ç‚¹ID/åç§°")
):
    def _history_sse_generator():
        # è¯»å–çŠ¶æ€å¿«ç…§
        config = {"configurable": {"thread_id": thread_id}}
        if checkpoint:
            config["configurable"]["checkpoint_id"] = checkpoint
            config["configurable"]["checkpoint"] = checkpoint
        logger.info(f"history config: {config}")
        try:
            snapshot = graph_app.get_state(config=config)
            logger.info(f"history snapshot: {snapshot}")
            # å¼€å§‹äº‹ä»¶
            yield _format_sse({"status": "start", "thread_id": thread_id, "checkpoint": checkpoint}, event="history_start")
            store = None
            if snapshot is not None:
                if hasattr(snapshot, "values"):
                    store = snapshot.values
                elif hasattr(snapshot, "value"):
                    store = snapshot.value
            prev_logs = store.get("execution_log") if isinstance(store, dict) else None
            if isinstance(prev_logs, list) and prev_logs:
                for hist in prev_logs:
                    node = hist.get("node", "unknown")
                    cfg_node = get_node_config(str(node))
                    out = {
                        "step": hist.get("step"),
                        "node": node,
                        "display_name": cfg_node["display_name"],
                        "description": cfg_node["description"],
                        "timestamp": hist.get("timestamp"),
                        "action": hist.get("action"),
                        "result": (hist.get("result")[:500] + "...") if isinstance(hist.get("result"), str) and len(hist.get("result")) > 500 else hist.get("result"),
                        "thread_id": thread_id,
                        "checkpoint": checkpoint
                    }
                    yield _format_sse(out, event="history")
            else:
                yield _format_sse({"thread_id": thread_id, "checkpoint": checkpoint, "history": []}, event="history_empty")
            # ç»“æŸäº‹ä»¶
            yield _format_sse({"status": "completed", "thread_id": thread_id, "checkpoint": checkpoint}, event="end")
        except Exception as e:
            yield _format_sse({"status": "error", "message": str(e)}, event="error")

    return StreamingResponse(_history_sse_generator(), media_type="text/event-stream")

# ===== æ–°å¢ï¼šWebSocket ç«¯ç‚¹ =====
@app.websocket("/ws")
async def websocket_stream(websocket: WebSocket):
    await websocket.accept()
    try:
        while True:
            try:
                message_text = await websocket.receive_text()
            except WebSocketDisconnect:
                break
            except Exception:
                # æœªèƒ½è§£ææ¶ˆæ¯ï¼Œå‘é€é”™è¯¯å¹¶ç»§ç»­
                await websocket.send_text(_format_sse({"status": "error", "message": "invalid message"}, event="error"))
                continue
            try:
                body = json.loads(message_text) if message_text else {}
            except Exception:
                body = {}
            user_input = body.get("user_input") or ""
            stream_mode = body.get("stream_mode") or "updates"
            thread_id = body.get("thread_id")
            checkpoint = body.get("checkpoint")
            replay_history = bool(body.get("replay_history", False))
            if not user_input:
                await websocket.send_text(_format_sse({"status": "error", "message": "missing user_input"}, event="error"))
                continue
            # å°†ç”Ÿæˆçš„äº‹ä»¶é€šè¿‡ WebSocket é€æ¡å‘é€ï¼ˆæ–‡æœ¬å¸§ï¼‰
            for line in _node_sse_generator(
                user_input,
                stream_mode=stream_mode,
                thread_id=thread_id,
                checkpoint=checkpoint,
                replay_history=replay_history,
            ):
                await websocket.send_text(line)
    except WebSocketDisconnect:
        pass
    except Exception as e:
        try:
            await websocket.send_text(_format_sse({"status": "error", "message": str(e)}, event="error"))
        except Exception:
            pass
    finally:
        try:
            await websocket.close()
        except Exception:
            pass

def test_node_name_tracking():
    """æµ‹è¯•èŠ‚ç‚¹åç§°è·Ÿè¸ªåŠŸèƒ½"""
    print("ğŸš€ èŠ‚ç‚¹åç§°è·Ÿè¸ªæµ‹è¯•")
    print("=" * 60)
    
    # åˆ›å»ºå·¥ä½œæµ
    graph = create_node_tracking_workflow()
    
    # æµ‹è¯•è¾“å…¥
    user_input = "æˆ‘æƒ³å­¦ä¹ Pythonç¼–ç¨‹"
    
    print(f"ğŸ“ è¾“å…¥: {user_input}")
    print("-" * 40)
    
    # å‡†å¤‡è¾“å…¥çŠ¶æ€
    inputs = {
        "user_input": user_input,
        "execution_log": [],
        "step_count": 0
    }
    
    print("ğŸ” èŠ‚ç‚¹æ‰§è¡Œè·Ÿè¸ª:")
    print("-" * 40)
    
    try:
        # ä½¿ç”¨ updates æ¨¡å¼è·Ÿè¸ªèŠ‚ç‚¹æ‰§è¡Œ
        for chunk in graph.stream(inputs, stream_mode="updates"):
            # æå–èŠ‚ç‚¹ä¿¡æ¯
            for key, value in chunk.items():
                if isinstance(value, dict) and "current_node" in value:
                    node_name = value["current_node"]
                    display_name = value.get("node_display_name", node_name)
                    description = value.get("node_description", "")
                    step_count = value.get("step_count", 0)
                    
                    print(f"ğŸ“ æ­¥éª¤ {step_count}: {display_name}")
                    print(f"   ğŸ“‹ æè¿°: {description}")
                    print(f"   ğŸ”§ èŠ‚ç‚¹å: {node_name}")
                    
                    # æ˜¾ç¤ºèŠ‚ç‚¹æ‰§è¡Œæ—¥å¿—
                    if "execution_log" in value:
                        for log in value["execution_log"]:
                            if log.get("node") == node_name:
                                print(f"   â° æ—¶é—´: {log.get('timestamp', 'N/A')}")
                                print(f"   ğŸ¯ åŠ¨ä½œ: {log.get('action', 'N/A')}")
                                if "result" in log:
                                    result = log["result"]
                                    if len(result) > 80:
                                        result = result[:80] + "..."
                                    print(f"   ğŸ“Š ç»“æœ: {result}")
                                print()
                    
    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}")

def show_node_configurations():
    """æ˜¾ç¤ºèŠ‚ç‚¹é…ç½®"""
    print("\nğŸ¨ èŠ‚ç‚¹é…ç½®å±•ç¤º")
    print("=" * 60)
    
    print("å¯ç”¨èŠ‚ç‚¹é…ç½®:")
    for node_name, config in NODE_CONFIGS.items():
        print(f"\n{config['emoji']} {config['display_name']}")
        print(f"   èŠ‚ç‚¹å: {node_name}")
        print(f"   æè¿°: {config['description']}")
    
    print("\né…ç½®ç‰¹ç‚¹:")
    print("âœ… æ”¯æŒä¸­æ–‡æ˜¾ç¤ºåç§°")
    print("âœ… æ”¯æŒemojiè¡¨æƒ…ç¬¦å·")
    print("âœ… è¯¦ç»†çš„èŠ‚ç‚¹æè¿°")
    print("âœ… ç»Ÿä¸€çš„é…ç½®ç®¡ç†")
    print("âœ… åŠ¨æ€èŠ‚ç‚¹ä¿¡æ¯è·å–")

def demonstrate_custom_node_names():
    """æ¼”ç¤ºè‡ªå®šä¹‰èŠ‚ç‚¹åç§°åŠŸèƒ½"""
    print("\nğŸ¯ è‡ªå®šä¹‰èŠ‚ç‚¹åç§°æ¼”ç¤º")
    print("=" * 60)
    
    # åˆ›å»ºå·¥ä½œæµ
    graph = create_node_tracking_workflow()
    
    # æµ‹è¯•è¾“å…¥
    test_inputs = [
        "å¦‚ä½•å­¦ä¹ æœºå™¨å­¦ä¹ ",
        "æ¨èä¸€äº›ç¼–ç¨‹ä¹¦ç±",
        "Pythonæœ‰ä»€ä¹ˆä¼˜åŠ¿"
    ]
    
    for i, user_input in enumerate(test_inputs, 1):
        print(f"\nğŸ“ æµ‹è¯• {i}: {user_input}")
        print("-" * 40)
        
        # å‡†å¤‡è¾“å…¥çŠ¶æ€
        inputs = {
            "user_input": user_input,
            "execution_log": [],
            "step_count": 0
        }
        
        try:
            # æ‰§è¡Œå·¥ä½œæµå¹¶è·Ÿè¸ªèŠ‚ç‚¹
            node_execution_order = []
            
            for chunk in graph.stream(inputs, stream_mode="updates"):
                for key, value in chunk.items():
                    if isinstance(value, dict) and "current_node" in value:
                        node_name = value["current_node"]
                        display_name = value.get("node_display_name", node_name)
                        step_count = value.get("step_count", 0)
                        
                        if step_count not in [log["step"] for log in node_execution_order]:
                            node_execution_order.append({
                                "step": step_count,
                                "node": node_name,
                                "display_name": display_name
                            })
            
            # æ˜¾ç¤ºæ‰§è¡Œé¡ºåº
            print("æ‰§è¡Œé¡ºåº:")
            for execution in node_execution_order:
                print(f"  {execution['step']}. {execution['display_name']}")
                
        except Exception as e:
            print(f"âŒ é”™è¯¯: {e}")

if __name__ == "__main__":
    # é»˜è®¤å¯åŠ¨ FastAPI æœåŠ¡ï¼ˆä½¿ç”¨å¯¼å…¥å­—ç¬¦ä¸²ä»¥å¯ç”¨ reload/workersï¼‰
    # todo cd langgraph_demo/study 
    import uvicorn
    uvicorn.run("11_stream_fastapi_ws:app", host="0.0.0.0", port=8000, reload=True)
    
    print("\nâœ… èŠ‚ç‚¹åç§°è‡ªå®šä¹‰ç¤ºä¾‹å®Œæˆï¼")
    print("\nğŸ“š å­¦ä¹ è¦ç‚¹æ€»ç»“:")
    print("1. èŠ‚ç‚¹åç§°é…ç½®: æ”¯æŒä¸­æ–‡ã€emojiå’Œæè¿°")
    print("2. çŠ¶æ€è·Ÿè¸ª: å®æ—¶ç›‘æ§å½“å‰æ‰§è¡Œçš„èŠ‚ç‚¹")
    print("3. æ˜¾ç¤ºåç§°: ç”¨æˆ·å‹å¥½çš„èŠ‚ç‚¹å±•ç¤º")
    print("4. é…ç½®ç®¡ç†: ç»Ÿä¸€çš„èŠ‚ç‚¹é…ç½®å­—å…¸")
    print("5. åŠ¨æ€è·å–: æ ¹æ®èŠ‚ç‚¹åè·å–é…ç½®ä¿¡æ¯")
    print("6. æ‰§è¡Œæ—¥å¿—: è¯¦ç»†çš„èŠ‚ç‚¹æ‰§è¡Œè®°å½•") 