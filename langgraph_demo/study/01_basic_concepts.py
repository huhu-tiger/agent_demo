# -*- coding: utf-8 -*-
"""
LangGraph åŸºç¡€æ¦‚å¿µç¤ºä¾‹
å­¦ä¹ è¦ç‚¹ï¼šçŠ¶æ€ç®¡ç†ã€èŠ‚ç‚¹å®šä¹‰ã€è¾¹è¿æ¥

ä½œè€…: AI Assistant
æ¥æº: LangGraph å®˜æ–¹æ–‡æ¡£å­¦ä¹ 
"""

import os
from typing import TypedDict, List
from typing_extensions import Annotated

# LangGraph æ ¸å¿ƒç»„ä»¶
from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import add_messages

# LangChain ç»„ä»¶
from langchain_core.messages import HumanMessage, AIMessage


import config
# è‡ªå®šä¹‰æ¨¡å‹é…ç½®
os.environ["OPENAI_API_BASE"] = config.base_url  # è‡ªå®šä¹‰æ¨¡å‹åœ°å€
os.environ["OPENAI_API_KEY"] = config.api_key  # è‡ªå®šä¹‰æ¨¡å‹å¯†é’¥
MODEL_NAME = config.model  # è‡ªå®šä¹‰æ¨¡å‹åç§°

# è·å–æ—¥å¿—å™¨
logger = config.logger

# ============================================================================
# åŸºç¡€çŠ¶æ€å®šä¹‰
# ============================================================================

class BasicState(TypedDict):
    """åŸºç¡€çŠ¶æ€å®šä¹‰ - åŒ…å«æ¶ˆæ¯å†å²å’Œç”¨æˆ·è¾“å…¥"""
    messages: Annotated[List[HumanMessage | AIMessage], add_messages]
    user_input: str
    response: str
    step_count: int

# ============================================================================
# åŸºç¡€èŠ‚ç‚¹å®šä¹‰
# ============================================================================

def input_processor(state: BasicState) -> BasicState:
    logger.info(f"input_processor state: {state}")
    """
    è¾“å…¥å¤„ç†èŠ‚ç‚¹ - å¤„ç†ç”¨æˆ·è¾“å…¥
    å­¦ä¹ è¦ç‚¹ï¼šèŠ‚ç‚¹å‡½æ•°çš„åŸºæœ¬ç»“æ„
    """
    logger.info("ğŸ”„ è¾“å…¥å¤„ç†èŠ‚ç‚¹æ­£åœ¨å·¥ä½œ...")
    # logger.info(f"ä½¿ç”¨æ¨¡å‹: {MODEL_NAME}")
    # logger.info(f"state: {state}")
    user_input = state["user_input"]
    step_count = state.get("step_count", 0) + 1
    
    # ç®€å•çš„è¾“å…¥å¤„ç†é€»è¾‘
    processed_input = f"å·²å¤„ç†: {user_input}"
    
    return {
        "user_input": processed_input,
        "step_count": step_count,
        "messages": [HumanMessage(content=processed_input)]
    }

def response_generator(state: BasicState) -> BasicState:
    logger.info(f"response_generator state: {state}")
    """
    å“åº”ç”ŸæˆèŠ‚ç‚¹ - ç”Ÿæˆæ™ºèƒ½ä½“å“åº”
    å­¦ä¹ è¦ç‚¹ï¼šçŠ¶æ€æ›´æ–°å’Œæ¶ˆæ¯å¤„ç†
    """
    logger.info("ğŸ¤– å“åº”ç”ŸæˆèŠ‚ç‚¹æ­£åœ¨å·¥ä½œ...")
    logger.info(f"ä½¿ç”¨æ¨¡å‹: {MODEL_NAME}")
    
    processed_input = state["user_input"]
    step_count = state["step_count"] + 1
    
    # ç”Ÿæˆå“åº”
    response = f"æ­¥éª¤ {step_count}: æˆ‘æ”¶åˆ°äº†æ‚¨çš„æ¶ˆæ¯ '{processed_input}'ã€‚è¿™æ˜¯ä¸€ä¸ªåŸºç¡€å“åº”ç¤ºä¾‹ã€‚"
    logger.info(f"response: {response}")
    return {
        "response": response,
        "messages": [AIMessage(content=response)]
    }

def message_logger(state: BasicState) -> BasicState:
    logger.info(f"message_logger state: {state}")
    """
    æ¶ˆæ¯è®°å½•èŠ‚ç‚¹ - è®°å½•å¤„ç†è¿‡ç¨‹
    å­¦ä¹ è¦ç‚¹ï¼šçŠ¶æ€è¯»å–å’Œæ—¥å¿—è®°å½•
    """
    logger.info("ğŸ“ æ¶ˆæ¯è®°å½•èŠ‚ç‚¹æ­£åœ¨å·¥ä½œ...")
    logger.info(f"ä½¿ç”¨æ¨¡å‹: {MODEL_NAME}")
    
    messages = state["messages"]
    response = state["response"]
    step_count = state["step_count"] + 1
    
    # è®°å½•å¤„ç†ä¿¡æ¯
    log_message = f"å¤„ç†å®Œæˆ - æ­¥éª¤: {step_count}, æ¶ˆæ¯æ•°é‡: {len(messages)}"
    # logger.info(log_message)
    # logger.info(f"state: {state}")
    
    return {
        "response": f"{response}\n\n{log_message}"
    }

# ============================================================================
# å·¥ä½œæµæ„å»º
# ============================================================================

def create_basic_workflow():
    """
    åˆ›å»ºåŸºç¡€å·¥ä½œæµ
    å­¦ä¹ è¦ç‚¹ï¼šStateGraph çš„åˆ›å»ºå’Œé…ç½®
    """
    logger.info("\n" + "="*60)
    logger.info("ğŸš€ åŸºç¡€æ¦‚å¿µå·¥ä½œæµ")
    logger.info(f"ä½¿ç”¨æ¨¡å‹: {MODEL_NAME}")
    logger.info("="*60)
    
    # 1. åˆ›å»ºçŠ¶æ€å›¾
    workflow = StateGraph(BasicState)
    
    # 2. æ·»åŠ èŠ‚ç‚¹
    workflow.add_node("input_processor", input_processor)
    workflow.add_node("response_generator", response_generator)
    workflow.add_node("message_logger", message_logger)
    
    # 3. è®¾ç½®å…¥å£ç‚¹
    workflow.set_entry_point("input_processor")
    
    # 4. æ·»åŠ è¾¹ï¼ˆé¡ºåºæ‰§è¡Œï¼‰
    workflow.add_edge("input_processor", "response_generator")
    workflow.add_edge("response_generator", "message_logger")
    workflow.add_edge("message_logger", END)
    
    # 5. ç¼–è¯‘å·¥ä½œæµ
    graph = workflow.compile()
    
    return graph, workflow

# ============================================================================
# æµ‹è¯•å‡½æ•°
# ============================================================================



def test_basic_concepts():
    """æµ‹è¯•åŸºç¡€æ¦‚å¿µ"""
    logger.info("ğŸ“ æµ‹è¯• LangGraph åŸºç¡€æ¦‚å¿µ")
    logger.info(f"æ¨¡å‹é…ç½®: {MODEL_NAME}")
    logger.info(f"API åœ°å€: {os.environ.get('OPENAI_API_BASE', 'é»˜è®¤åœ°å€')}")
    
    # åˆ›å»ºå·¥ä½œæµ
    graph, workflow = create_basic_workflow()

    # todo å¯è§†åŒ– 
    from show_graph import show_workflow_graph
    show_workflow_graph(graph, "åŸºç¡€æ¦‚å¿µå·¥ä½œæµ",logger)

    # æµ‹è¯•è¾“å…¥
    test_inputs = [
        "ä½ å¥½ï¼Œæˆ‘æƒ³å­¦ä¹  LangGraph",
        "è¯·è§£é‡Šä¸€ä¸‹çŠ¶æ€ç®¡ç†çš„æ¦‚å¿µ",
        "èŠ‚ç‚¹å’Œè¾¹æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ"
    ]
    
    for i, test_input in enumerate(test_inputs, 1):
        logger.info(f"\n--- æµ‹è¯• {i} ---")
        logger.info(f"è¾“å…¥: {test_input}")
        
        try:
            result = graph.invoke({"user_input": test_input})
            logger.info(f"result: {result}")
            # logger.info(f"è¾“å‡º: {result['response']}")
            # logger.info(f"æ­¥éª¤æ•°: {result['step_count']}")
            # logger.info(f"æ¶ˆæ¯æ•°: {len(result['messages'])}")
        except Exception as e:
            logger.error(f"é”™è¯¯: {e}")

if __name__ == "__main__":
    test_basic_concepts() 