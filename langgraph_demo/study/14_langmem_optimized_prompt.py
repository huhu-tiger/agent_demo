#!/usr/bin/env python3
"""
LangMem æç¤ºè¯ä¼˜åŒ–ç¤ºä¾‹
å±•ç¤ºå¦‚ä½•ä½¿ç”¨LangMemè¿›è¡Œæç¤ºè¯ä¼˜åŒ–
æ”¯æŒè‡ªå®šä¹‰æ¨¡å‹é…ç½®
"""

import os
import sys
import json
from datetime import datetime
from typing import List, Dict, Any, Optional

# æ·»åŠ å½“å‰ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from langchain.embeddings import init_embeddings
from langchain.chat_models import init_chat_model
from langgraph.store.memory import InMemoryStore
from langgraph.prebuilt import create_react_agent
from langgraph.checkpoint.memory import InMemorySaver

from langmem import (
    create_memory_store_manager,
    create_manage_memory_tool,
    create_search_memory_tool,
    create_prompt_optimizer
)

# å¯¼å…¥é…ç½®æ¨¡å—
import config
from config import ModelConfig, custom_config
from config import logger


class LangMemDemo:
    """LangMemæ¼”ç¤ºç±» - ä¸“æ³¨äºæç¤ºè¯ä¼˜åŒ–"""
    
    def __init__(self, model_config: Optional[ModelConfig] = None):
        """
        åˆå§‹åŒ–LangMemæ¼”ç¤º
        
        Args:
            model_config: æ¨¡å‹é…ç½®å¯¹è±¡ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤é…ç½®
        """
        try:
            # è®¾ç½®é»˜è®¤æ¨¡å‹é…ç½®
            if model_config is None:
                model_config = custom_config
            
            self.model_config = model_config
            logger.info(f"åˆå§‹åŒ–LangMemæ¼”ç¤ºï¼Œä½¿ç”¨æ¨¡å‹é…ç½®: {model_config.model_provider}:{model_config.model_name}")
            
            # åˆå§‹åŒ–åµŒå…¥æ¨¡å‹
            embedding_config = model_config.get_embedding_config()
            if embedding_config:
                # ä½¿ç”¨è‡ªå®šä¹‰é…ç½®åˆå§‹åŒ–åµŒå…¥æ¨¡å‹
                if model_config.embedding_provider == "openai":
                    self.embeddings = init_embeddings(
                        f"openai:{model_config.embedding_model}",
                        **embedding_config
                    )
                else:
                    # æ”¯æŒå…¶ä»–æä¾›å•†
                    self.embeddings = init_embeddings(
                        f"{model_config.embedding_provider}:{model_config.embedding_model}",
                        **embedding_config
                    )
            else:
                # ä½¿ç”¨é»˜è®¤é…ç½®
                self.embeddings = init_embeddings(f"{model_config.embedding_provider}:{model_config.embedding_model}")
            
            logger.info(f"åµŒå…¥æ¨¡å‹åˆå§‹åŒ–æˆåŠŸ: {model_config.embedding_provider}:{model_config.embedding_model}")
            
            # åˆå§‹åŒ–èŠå¤©æ¨¡å‹
            chat_config = model_config.get_chat_model_config()
            if chat_config:
                # ä½¿ç”¨è‡ªå®šä¹‰é…ç½®åˆå§‹åŒ–èŠå¤©æ¨¡å‹
                if model_config.model_provider == "openai":
                    self.llm = init_chat_model(
                        f"openai:{model_config.model_name}",
                        **chat_config
                    )
                else:
                    # æ”¯æŒå…¶ä»–æä¾›å•†
                    self.llm = init_chat_model(
                        f"{model_config.model_provider}:{model_config.model_name}",
                        **chat_config
                    )
            else:
                # ä½¿ç”¨é»˜è®¤é…ç½®
                self.llm = init_chat_model(f"{model_config.model_provider}:{model_config.model_name}")
            
            logger.info(f"èŠå¤©æ¨¡å‹åˆå§‹åŒ–æˆåŠŸ: {model_config.model_provider}:{model_config.model_name}")
            
            # è·å–åµŒå…¥æ¨¡å‹çš„ç»´åº¦
            embedding_dims = model_config.get_embedding_dimensions()
            logger.info(f"åµŒå…¥ç»´åº¦: {embedding_dims}")
            
            # è®¾ç½®å†…å­˜å­˜å‚¨
            self.store = InMemoryStore(
                index={
                    "dims": embedding_dims,
                    "embed": self.embeddings,
                }
            )

            
            # åˆ›å»ºå†…å­˜å·¥å…·
            self.memory_tools = [
                create_manage_memory_tool(namespace=("memories",), store=self.store),
                create_search_memory_tool(namespace=("memories",), store=self.store),
            ]
            
            # åˆ›å»ºå¸¦å†…å­˜çš„æ™ºèƒ½ä½“
            self.agent = create_react_agent(
                self.llm,  # ä½¿ç”¨å·²åˆå§‹åŒ–çš„èŠå¤©æ¨¡å‹å¯¹è±¡
                tools=self.memory_tools, 
                store=self.store, 
                checkpointer=InMemorySaver()
            )
            
            logger.info("æ™ºèƒ½ä½“åˆ›å»ºæˆåŠŸ")
            
            print(f"âœ… LangMemæ¼”ç¤ºåˆå§‹åŒ–å®Œæˆ")
            print(f"   èŠå¤©æ¨¡å‹: {model_config.model_provider}:{model_config.model_name}")
            print(f"   åµŒå…¥æ¨¡å‹: {model_config.embedding_provider}:{model_config.embedding_model}")
            print(f"   åµŒå…¥ç»´åº¦: {embedding_dims}")
            print(f"   å­˜å‚¨: InMemoryStore")
            print(f"   å‘½åç©ºé—´: ('memories',)")
            
        except Exception as e:
            logger.error(f"LangMemæ¼”ç¤ºåˆå§‹åŒ–å¤±è´¥: {e}")
            raise Exception(f"åˆå§‹åŒ–å¤±è´¥: {e}")
    
    def get_model_info(self) -> Dict[str, str]:
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        return {
            "chat_model": f"{self.model_config.model_provider}:{self.model_config.model_name}",
            "embedding_model": f"{self.model_config.embedding_provider}:{self.model_config.embedding_model}",
            "chat_base_url": self.model_config.base_url or "é»˜è®¤",
            "embedding_base_url": self.model_config.embedding_base_url or "é»˜è®¤",
            "embedding_dimensions": str(self.model_config.get_embedding_dimensions())
        }
    
    def save_optimized_prompt(self, original_prompt: str, optimized_prompt: str, scenario: str, 
                            optimization_type: str, trajectories: List, filename: str = None) -> str:
        """
        ä¿å­˜ä¼˜åŒ–åçš„æç¤ºè¯
        
        Args:
            original_prompt: åŸå§‹æç¤ºè¯
            optimized_prompt: ä¼˜åŒ–åçš„æç¤ºè¯
            scenario: åœºæ™¯ç±»å‹
            optimization_type: ä¼˜åŒ–ç±»å‹
            trajectories: ä½¿ç”¨çš„è½¨è¿¹
            filename: æ–‡ä»¶åï¼Œå¦‚æœä¸ºNoneåˆ™è‡ªåŠ¨ç”Ÿæˆ
            
        Returns:
            ä¿å­˜çš„æ–‡ä»¶è·¯å¾„
        """
        try:
            if filename is None:
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                filename = f"optimized_prompt_{scenario}_{timestamp}.json"
            
            # åˆ›å»ºlogsç›®å½•ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
            logs_dir = os.path.join(os.path.dirname(__file__), "logs")
            os.makedirs(logs_dir, exist_ok=True)
            
            filepath = os.path.join(logs_dir, filename)
            
            # å‡†å¤‡ä¿å­˜çš„æ•°æ®
            data = {
                "original_prompt": original_prompt,
                "optimized_prompt": optimized_prompt,
                "scenario": scenario,
                "optimization_type": optimization_type,
                "trajectories_count": len(trajectories),
                "optimization_timestamp": datetime.now().isoformat(),
                "model_info": self.get_model_info(),
                "analysis": self.analyze_trajectories(trajectories)
            }
            
            # ä¿å­˜åˆ°æ–‡ä»¶
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            
            logger.info(f"ä¼˜åŒ–æç¤ºè¯å·²ä¿å­˜åˆ°: {filepath}")
            return filepath
            
        except Exception as e:
            error_msg = f"ä¿å­˜ä¼˜åŒ–æç¤ºè¯å¤±è´¥: {e}"
            logger.error(error_msg)
            return ""
    
    def load_optimized_prompt(self, filepath: str) -> Dict[str, Any]:
        """
        åŠ è½½ä¼˜åŒ–åçš„æç¤ºè¯
        
        Args:
            filepath: æ–‡ä»¶è·¯å¾„
            
        Returns:
            åŠ è½½çš„æ•°æ®
        """
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            logger.info(f"å·²åŠ è½½ä¼˜åŒ–æç¤ºè¯: {filepath}")
            return data
            
        except Exception as e:
            error_msg = f"åŠ è½½ä¼˜åŒ–æç¤ºè¯å¤±è´¥: {e}"
            logger.error(error_msg)
            return {}

    def optimize_prompt(self, base_prompt: str, trajectories: List, optimization_type: str = "metaprompt") -> str:
        """
        ä¼˜åŒ–æç¤ºè¯
        
        Args:
            base_prompt: åŸºç¡€æç¤ºè¯
            trajectories: å¯¹è¯è½¨è¿¹
            optimization_type: ä¼˜åŒ–ç±»å‹ ("metaprompt", "reflection", "feedback")
            
        Returns:
            ä¼˜åŒ–åçš„æç¤ºè¯
        """
        try:
            logger.info(f"å¼€å§‹ä¼˜åŒ–æç¤ºè¯ï¼Œç±»å‹: {optimization_type}")
            
            optimizer = create_prompt_optimizer(
                self.llm, # ä½¿ç”¨å·²åˆå§‹åŒ–çš„èŠå¤©æ¨¡å‹å¯¹è±¡
                kind=optimization_type,
                config={
                    "max_reflection_steps": 3, 
                    "min_reflection_steps": 1,
                    "reflection_threshold": 0.7
                }
            )
            
            updated_prompt = optimizer.invoke({
                "trajectories": trajectories,
                "prompt": base_prompt
            })
            logger.info(f"ä¼˜åŒ–åçš„æç¤ºè¯: {updated_prompt}")
            logger.info("æç¤ºè¯ä¼˜åŒ–å®Œæˆ")
            return updated_prompt
        except Exception as e:
            error_msg = f"ä¼˜åŒ–æç¤ºè¯å¤±è´¥: {e}"
            logger.error(error_msg)
            return base_prompt
    
    def analyze_trajectories(self, trajectories: List) -> Dict[str, Any]:
        """
        åˆ†æå¯¹è¯è½¨è¿¹
        
        Args:
            trajectories: å¯¹è¯è½¨è¿¹åˆ—è¡¨
            
        Returns:
            åˆ†æç»“æœ
        """
        try:
            logger.info("å¼€å§‹åˆ†æå¯¹è¯è½¨è¿¹")
            
            total_trajectories = len(trajectories)
            avg_score = sum(traj[1].get("score", 0) for traj in trajectories) / total_trajectories
            high_score_count = sum(1 for traj in trajectories if traj[1].get("score", 0) > 0.8)
            
            analysis = {
                "total_trajectories": total_trajectories,
                "average_score": avg_score,
                "high_score_count": high_score_count,
                "high_score_percentage": (high_score_count / total_trajectories) * 100,
                "score_distribution": {
                    "excellent": sum(1 for traj in trajectories if traj[1].get("score", 0) >= 0.9),
                    "good": sum(1 for traj in trajectories if 0.7 <= traj[1].get("score", 0) < 0.9),
                    "fair": sum(1 for traj in trajectories if 0.5 <= traj[1].get("score", 0) < 0.7),
                    "poor": sum(1 for traj in trajectories if traj[1].get("score", 0) < 0.5)
                }
            }
            
            logger.info(f"è½¨è¿¹åˆ†æå®Œæˆ: å¹³å‡åˆ†æ•° {avg_score:.2f}")
            return analysis
        except Exception as e:
            error_msg = f"åˆ†æè½¨è¿¹å¤±è´¥: {e}"
            logger.error(error_msg)
            return {}
    
    def create_sample_trajectories(self, scenario: str = "general") -> List:
        """
        åˆ›å»ºç¤ºä¾‹å¯¹è¯è½¨è¿¹
        
        Args:
            scenario: åœºæ™¯ç±»å‹ ("general", "technical", "creative", "customer_service")
            
        Returns:
            å¯¹è¯è½¨è¿¹åˆ—è¡¨
        """
        if scenario == "technical":
            return [
                (
                    [
                        {"role": "user", "content": "è§£é‡Šä»€ä¹ˆæ˜¯RESTful APIï¼Ÿ"},
                        {"role": "assistant", "content": "RESTful APIæ˜¯ä¸€ç§åŸºäºRESTæ¶æ„é£æ ¼çš„APIè®¾è®¡..."}
                    ],
                    {"score": 0.85, "comment": "æŠ€æœ¯è§£é‡Šå‡†ç¡®ï¼Œä½†å¯ä»¥æ·»åŠ æ›´å¤šå®é™…ä¾‹å­"}
                ),
                (
                    [
                        {"role": "user", "content": "ä»€ä¹ˆæ˜¯å¾®æœåŠ¡æ¶æ„ï¼Ÿ"},
                        {"role": "assistant", "content": "å¾®æœåŠ¡æ¶æ„æ˜¯ä¸€ç§å°†åº”ç”¨ç¨‹åºæ„å»ºä¸ºä¸€ç»„å°å‹è‡ªæ²»æœåŠ¡çš„æ¶æ„æ¨¡å¼..."}
                    ],
                    {"score": 0.92, "comment": "å¾ˆå¥½çš„è§£é‡Šï¼ŒåŒ…å«äº†ä¼˜ç¼ºç‚¹å’Œé€‚ç”¨åœºæ™¯"}
                ),
                (
                    [
                        {"role": "user", "content": "Dockerå’Œè™šæ‹Ÿæœºæœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ"},
                        {"role": "assistant", "content": "Dockerä½¿ç”¨å®¹å™¨åŒ–æŠ€æœ¯ï¼Œè€Œè™šæ‹Ÿæœºä½¿ç”¨è™šæ‹ŸåŒ–æŠ€æœ¯..."}
                    ],
                    {"score": 0.78, "comment": "åŸºæœ¬æ­£ç¡®ï¼Œä½†å¯ä»¥æ›´è¯¦ç»†åœ°è§£é‡ŠæŠ€æœ¯å·®å¼‚"}
                )
            ]
        elif scenario == "creative":
            return [
                (
                    [
                        {"role": "user", "content": "å†™ä¸€ä¸ªå…³äºå¤ªç©ºæ¢ç´¢çš„çŸ­æ•…äº‹"},
                        {"role": "assistant", "content": "åœ¨é¥è¿œçš„æœªæ¥ï¼Œäººç±»ç»ˆäºè¸ä¸Šäº†ç«æ˜Ÿçš„åœŸåœ°..."}
                    ],
                    {"score": 0.88, "comment": "æ•…äº‹æœ‰åˆ›æ„ï¼Œä½†å¯ä»¥å¢åŠ æ›´å¤šæƒ…æ„Ÿå…ƒç´ "}
                ),
                (
                    [
                        {"role": "user", "content": "è®¾è®¡ä¸€ä¸ªæœªæ¥åŸå¸‚çš„æ ‡å¿—"},
                        {"role": "assistant", "content": "è¿™ä¸ªæ ‡å¿—èåˆäº†ç§‘æŠ€æ„Ÿå’Œäººæ–‡å…³æ€€..."}
                    ],
                    {"score": 0.95, "comment": "è®¾è®¡ç†å¿µå¾ˆå¥½ï¼Œæè¿°ç”ŸåŠ¨å…·ä½“"}
                )
            ]
        elif scenario == "customer_service":
            return [
                (
                    [
                        {"role": "user", "content": "æˆ‘çš„è®¢å•è¿˜æ²¡æœ‰æ”¶åˆ°"},
                        {"role": "assistant", "content": "æˆ‘ç†è§£æ‚¨çš„æ‹…å¿§ï¼Œè®©æˆ‘å¸®æ‚¨æŸ¥è¯¢è®¢å•çŠ¶æ€..."}
                    ],
                    {"score": 0.82, "comment": "å›åº”åŠæ—¶ï¼Œä½†å¯ä»¥æä¾›æ›´å¤šè§£å†³æ–¹æ¡ˆ"}
                ),
                (
                    [
                        {"role": "user", "content": "äº§å“æœ‰è´¨é‡é—®é¢˜"},
                        {"role": "assistant", "content": "éå¸¸æŠ±æ­‰ç»™æ‚¨å¸¦æ¥ä¸ä¾¿ï¼Œæˆ‘ä»¬ç«‹å³ä¸ºæ‚¨å¤„ç†..."}
                    ],
                    {"score": 0.89, "comment": "æ€åº¦è¯šæ³ï¼Œè§£å†³æ–¹æ¡ˆæ˜ç¡®"}
                )
            ]
        else:  # general
            return [
                (
                    [
                        {"role": "user", "content": "ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ"},
                        {"role": "assistant", "content": "æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œå®ƒä½¿è®¡ç®—æœºèƒ½å¤Ÿåœ¨æ²¡æœ‰æ˜ç¡®ç¼–ç¨‹çš„æƒ…å†µä¸‹å­¦ä¹ å’Œæ”¹è¿›..."}
                    ],
                    {"score": 0.85, "comment": "å›ç­”å‡†ç¡®ä½†å¯ä»¥æ›´è¯¦ç»†"}
                ),
                (
                    [
                        {"role": "user", "content": "è§£é‡Šä¸€ä¸‹æ·±åº¦å­¦ä¹ "},
                        {"role": "assistant", "content": "æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªå­é¢†åŸŸï¼Œä½¿ç”¨å¤šå±‚ç¥ç»ç½‘ç»œæ¥æ¨¡æ‹Ÿäººè„‘çš„å­¦ä¹ è¿‡ç¨‹..."}
                    ],
                    {"score": 0.92, "comment": "å¾ˆå¥½çš„è§£é‡Šï¼ŒåŒ…å«äº†å…·ä½“ä¾‹å­"}
                ),
                (
                    [
                        {"role": "user", "content": "å¦‚ä½•å­¦ä¹ ç¼–ç¨‹ï¼Ÿ"},
                        {"role": "assistant", "content": "å­¦ä¹ ç¼–ç¨‹å¯ä»¥ä»åŸºç¡€å¼€å§‹ï¼Œé€‰æ‹©ä¸€é—¨å…¥é—¨è¯­è¨€å¦‚Python..."}
                    ],
                    {"score": 0.78, "comment": "å»ºè®®å®ç”¨ï¼Œä½†å¯ä»¥æ›´ç³»ç»ŸåŒ–"}
                )
            ]





def demo_continuous_optimization(model_config: Optional[ModelConfig] = None):
    """æ¼”ç¤ºæŒç»­ä¼˜åŒ–è¿‡ç¨‹"""
    print("\nğŸ”„ æŒç»­ä¼˜åŒ–è¿‡ç¨‹æ¼”ç¤º")
    print("=" * 50)
    
    demo = LangMemDemo(model_config=model_config)
    
    # åˆå§‹æç¤ºè¯
    initial_prompt = "ä½ æ˜¯ä¸€ä¸ªAIåŠ©æ‰‹"
    
    print(f"   åˆå§‹æç¤ºè¯: {initial_prompt}")
    
    # ç¬¬ä¸€è½®ä¼˜åŒ–
    print("\nğŸ“ ç¬¬ä¸€è½®ä¼˜åŒ–ï¼ˆåŸºç¡€å¯¹è¯ï¼‰")
    trajectories_round1 = demo.create_sample_trajectories("general")
    optimized_prompt_1 = demo.optimize_prompt(initial_prompt, trajectories_round1)
    print(f"   ä¼˜åŒ–å: {optimized_prompt_1}")
    
    # ç¬¬äºŒè½®ä¼˜åŒ–ï¼ˆåŸºäºç¬¬ä¸€è½®ç»“æœï¼‰
    print("\nğŸ“ ç¬¬äºŒè½®ä¼˜åŒ–ï¼ˆæŠ€æœ¯å¯¹è¯ï¼‰")
    trajectories_round2 = demo.create_sample_trajectories("technical")
    optimized_prompt_2 = demo.optimize_prompt(optimized_prompt_1, trajectories_round2)
    print(f"   ä¼˜åŒ–å: {optimized_prompt_2}")
    
    # ç¬¬ä¸‰è½®ä¼˜åŒ–ï¼ˆåŸºäºå‰ä¸¤è½®ç»“æœï¼‰
    print("\nğŸ“ ç¬¬ä¸‰è½®ä¼˜åŒ–ï¼ˆåˆ›æ„å¯¹è¯ï¼‰")
    trajectories_round3 = demo.create_sample_trajectories("creative")
    optimized_prompt_3 = demo.optimize_prompt(optimized_prompt_2, trajectories_round3)
    print(f"   ä¼˜åŒ–å: {optimized_prompt_3}")
    
    print("\nğŸ“Š ä¼˜åŒ–æ¼”è¿›è¿‡ç¨‹:")
    print(f"   åˆå§‹ â†’ ç¬¬ä¸€è½® â†’ ç¬¬äºŒè½® â†’ ç¬¬ä¸‰è½®")
    print(f"   åŸºç¡€ â†’ é€šç”¨ä¼˜åŒ– â†’ æŠ€æœ¯ä¼˜åŒ– â†’ åˆ›æ„ä¼˜åŒ–")
    
    print("\nâœ… æŒç»­ä¼˜åŒ–æ¼”ç¤ºå®Œæˆï¼")


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ LangMem æç¤ºè¯ä¼˜åŒ–æ¼”ç¤º")
    print("=" * 60)
    
    try:
        # ä½¿ç”¨è‡ªå®šä¹‰é…ç½®
        print("\nğŸ”§ ä½¿ç”¨è‡ªå®šä¹‰æ¨¡å‹é…ç½®:")
        from config import custom_config
        
    

        # æŒç»­ä¼˜åŒ–è¿‡ç¨‹æ¼”ç¤º
        demo_continuous_optimization(custom_config)
        
        print("\nâœ… æ‰€æœ‰æ¼”ç¤ºå®Œæˆï¼")
        
    except Exception as e:
        print(f"âŒ æ¼”ç¤ºè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main() 