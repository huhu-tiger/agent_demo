#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Gradio LangGraph æ™ºèƒ½å¯¹è¯ç•Œé¢
æ”¯æŒæµå¼è¾“å‡ºã€å¯è§†åŒ–å·¥ä½œæµå›¾ã€èŠ‚ç‚¹æ‰§è¡Œç›‘æ§çš„æ™ºèƒ½å¯¹è¯ç³»ç»Ÿ
"""

import gradio as gr
import asyncio
import logging
import time
import json
import os
import base64
from typing import List, Tuple, Optional, Union, TypedDict, Annotated, Dict, Any
from dataclasses import dataclass, field
from datetime import datetime
import operator

# æ·»åŠ ç³»ç»Ÿè·¯å¾„ä»¥å¯¼å…¥é…ç½®
import sys
sys.path.append(os.path.join(os.path.dirname(__file__), '../..'))
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

try:
    from langgraph.graph import StateGraph, START, END
    from langchain_openai import ChatOpenAI
    from langchain_core.messages import HumanMessage, AIMessage
    from langchain_core.tools import tool
    from langgraph.config import get_stream_writer
    from langchain_core.runnables.graph import MermaidDrawMethod
    import config
    LANGGRAPH_AVAILABLE = True
except ImportError as e:
    LANGGRAPH_AVAILABLE = False
    print(f"LangGraphç›¸å…³åº“æœªå®‰è£…: {e}")

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# å®šä¹‰LangGraphçŠ¶æ€ç»“æ„
class ConversationState(TypedDict):
    """å¯¹è¯çŠ¶æ€å®šä¹‰"""
    user_input: str                    # ç”¨æˆ·è¾“å…¥
    processed_input: str               # å¤„ç†åçš„è¾“å…¥
    analysis_result: str               # åˆ†æç»“æœ  
    ai_response: str                   # AIå“åº”
    final_response: str                # æœ€ç»ˆå“åº”
    execution_log: Annotated[list, operator.add]  # æ‰§è¡Œæ—¥å¿—
    step_count: int                    # æ­¥éª¤è®¡æ•°
    current_node: str                  # å½“å‰èŠ‚ç‚¹åç§°
    node_display_name: str             # èŠ‚ç‚¹æ˜¾ç¤ºåç§°
    conversation_history: Annotated[list, operator.add]  # å¯¹è¯å†å²
    system_prompt: str                 # ç³»ç»Ÿæç¤º

@dataclass
class ModelConfig:
    """æ¨¡å‹é…ç½®ç±»"""
    name: str
    model_name: str
    base_url: str
    api_key: str
    description: str = ""
    temperature: float = 0.7
    max_tokens: int = 4096

@dataclass 
class ChatState:
    """èŠå¤©çŠ¶æ€ç®¡ç†"""
    messages: List[Tuple[str, str]] = field(default_factory=list)
    current_model: Optional[str] = None
    workflow_graph: Optional[object] = None
    system_prompt: str = "ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„AIåŠ©æ‰‹ï¼Œè¯·æ ¹æ®ç”¨æˆ·çš„é—®é¢˜æä¾›å‡†ç¡®ã€æœ‰å¸®åŠ©çš„å›ç­”ã€‚"
    execution_logs: List[Dict[str, Any]] = field(default_factory=list)
    workflow_diagram: str = ""

# å®šä¹‰å·¥å…·å‡½æ•°
@tool
def analyze_user_input(text: str) -> str:
    """åˆ†æç”¨æˆ·è¾“å…¥çš„å·¥å…·"""
    try:
        writer = get_stream_writer()
        writer({"type": "progress", "message": f"å¼€å§‹åˆ†æç”¨æˆ·è¾“å…¥: {text[:50]}..."})
        
        time.sleep(0.5)  # æ¨¡æ‹Ÿåˆ†æè¿‡ç¨‹
        writer({"type": "progress", "message": "æ­£åœ¨è¯†åˆ«ç”¨æˆ·æ„å›¾..."})
        
        time.sleep(0.5)
        analysis = f"åˆ†æç»“æœ: ç”¨æˆ·è¾“å…¥åŒ…å« {len(text)} ä¸ªå­—ç¬¦ï¼Œä¸»è¦æ„å›¾æ˜¯è·å–ä¿¡æ¯æˆ–å¯»æ±‚å¸®åŠ©"
        writer({"type": "analysis_complete", "message": "åˆ†æå®Œæˆ", "result": analysis})
        
        return analysis
    except:
        return f"åˆ†æç»“æœ: ç”¨æˆ·è¾“å…¥åŒ…å« {len(text)} ä¸ªå­—ç¬¦ï¼Œä¸»è¦æ„å›¾æ˜¯è·å–ä¿¡æ¯æˆ–å¯»æ±‚å¸®åŠ©"

@tool
def search_knowledge(query: str) -> str:
    """çŸ¥è¯†æœç´¢å·¥å…·"""
    try:
        writer = get_stream_writer()
        writer({"type": "progress", "message": f"æ­£åœ¨æœç´¢çŸ¥è¯†: {query}..."})
        
        time.sleep(0.8)
        writer({"type": "progress", "message": "æ­£åœ¨æ£€ç´¢ç›¸å…³ä¿¡æ¯..."})
        
        time.sleep(0.5)
        result = f"æœç´¢åˆ°ä¸ '{query}' ç›¸å…³çš„ä¿¡æ¯ï¼Œå»ºè®®æ‚¨æ·±å…¥äº†è§£ç›¸å…³æŠ€æœ¯å¹¶å®è·µåº”ç”¨ã€‚"
        writer({"type": "search_complete", "message": "æœç´¢å®Œæˆ", "result": result})
        
        return result
    except:
        return f"æœç´¢åˆ°ä¸ '{query}' ç›¸å…³çš„ä¿¡æ¯ï¼Œå»ºè®®æ‚¨æ·±å…¥äº†è§£ç›¸å…³æŠ€æœ¯å¹¶å®è·µåº”ç”¨ã€‚"

# å®šä¹‰LangGraphèŠ‚ç‚¹å‡½æ•°
def input_processor(state: ConversationState) -> ConversationState:
    """è¾“å…¥å¤„ç†èŠ‚ç‚¹"""
    user_input = state["user_input"]
    step_count = state.get("step_count", 0) + 1
    
    node_name = "input_processor"
    display_name = "ğŸ“ è¾“å…¥å¤„ç†èŠ‚ç‚¹"
    
    log_entry = {
        "step": step_count,
        "node": node_name,
        "display_name": display_name,
        "action": "å¼€å§‹å¤„ç†ç”¨æˆ·è¾“å…¥",
        "timestamp": time.strftime("%H:%M:%S"),
        "input": user_input,
        "status": "processing"
    }
    
    processed_input = f"å¤„ç†åçš„è¾“å…¥: {user_input}"
    
    log_entry["result"] = processed_input
    log_entry["status"] = "completed"
    
    # è¿”å›å®Œæ•´çš„çŠ¶æ€
    return {
        "user_input": user_input,
        "processed_input": processed_input,
        "analysis_result": state.get("analysis_result", ""),
        "ai_response": state.get("ai_response", ""),
        "final_response": state.get("final_response", ""),
        "execution_log": state.get("execution_log", []) + [log_entry],
        "step_count": step_count,
        "current_node": node_name,
        "node_display_name": display_name,
        "conversation_history": state.get("conversation_history", []),
        "system_prompt": state.get("system_prompt", "")
    }

def analysis_node(state: ConversationState) -> ConversationState:
    """åˆ†æèŠ‚ç‚¹"""
    user_input = state["user_input"]
    step_count = state.get("step_count", 0) + 1
    
    node_name = "analysis_node"
    display_name = "ğŸ” æ™ºèƒ½åˆ†æèŠ‚ç‚¹"
    
    log_entry = {
        "step": step_count,
        "node": node_name,
        "display_name": display_name,
        "action": "å¼€å§‹åˆ†æç”¨æˆ·è¾“å…¥",
        "timestamp": time.strftime("%H:%M:%S"),
        "input": user_input,
        "status": "processing"
    }
    
    # è°ƒç”¨åˆ†æå·¥å…·
    analysis_result = analyze_user_input(user_input)
    
    log_entry["result"] = analysis_result
    log_entry["status"] = "completed"
    log_entry["tool_used"] = "analyze_user_input"
    
    # è¿”å›å®Œæ•´çš„çŠ¶æ€
    return {
        "user_input": user_input,
        "processed_input": state.get("processed_input", ""),
        "analysis_result": analysis_result,
        "ai_response": state.get("ai_response", ""),
        "final_response": state.get("final_response", ""),
        "execution_log": state.get("execution_log", []) + [log_entry],
        "step_count": step_count,
        "current_node": node_name,
        "node_display_name": display_name,
        "conversation_history": state.get("conversation_history", []),
        "system_prompt": state.get("system_prompt", "")
    }

def ai_response_node(state: ConversationState) -> ConversationState:
    """
AIå“åº”ç”ŸæˆèŠ‚ç‚¹
    """
    user_input = state["user_input"]
    analysis_result = state.get("analysis_result", "")
    system_prompt = state.get("system_prompt", "ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„AIåŠ©æ‰‹")
    step_count = state.get("step_count", 0) + 1
    
    node_name = "ai_response_node"
    display_name = "ğŸ¤– AIå“åº”ç”ŸæˆèŠ‚ç‚¹"
    
    log_entry = {
        "step": step_count,
        "node": node_name,
        "display_name": display_name,
        "action": "å¼€å§‹ç”ŸæˆAIå“åº”",
        "timestamp": time.strftime("%H:%M:%S"),
        "input": user_input,
        "status": "processing"
    }
    
    try:
        if LANGGRAPH_AVAILABLE:
            # åˆå§‹åŒ–LLM
            from langchain_openai import ChatOpenAI
            from langchain_core.messages import HumanMessage
            import config as local_config
            
            llm = ChatOpenAI(
                model=local_config.model,
                base_url=local_config.base_url,
                api_key=local_config.api_key,
                temperature=0.7
            )
            
            prompt = f"""
{system_prompt}

ç”¨æˆ·è¾“å…¥: {user_input}
åˆ†æç»“æœ: {analysis_result}

è¯·æä¾›æœ‰å¸®åŠ©çš„å›ç­”:
"""
            
            response = llm.invoke([HumanMessage(content=prompt)])
            ai_response = response.content
            log_entry["tool_used"] = "ChatOpenAI"
        else:
            # æ¨¡æ‹Ÿå“åº”
            ai_response = f"æ ¹æ®æ‚¨çš„é—®é¢˜ '{user_input}'ï¼Œæˆ‘å»ºè®®æ‚¨å¯ä»¥å°è¯•ä»¥ä¸‹æ–¹æ³•æ¥è§£å†³æˆ–äº†è§£ç›¸å…³å†…å®¹ã€‚å¦‚æœæ‚¨éœ€è¦æ›´è¯¦ç»†çš„ä¿¡æ¯ï¼Œè¯·å‘Šè¯‰æˆ‘æ‚¨çš„å…·ä½“éœ€æ±‚ã€‚"
            log_entry["tool_used"] = "simulated_response"
            
    except Exception as e:
        ai_response = f"æŠ±æ­‰ï¼Œå¤„ç†æ‚¨çš„è¯·æ±‚æ—¶å‡ºç°é”™è¯¯: {str(e)}"
        log_entry["error"] = str(e)
    
    log_entry["result"] = ai_response
    log_entry["status"] = "completed"
    
    # è¿”å›å®Œæ•´çš„çŠ¶æ€
    return {
        "user_input": user_input,
        "processed_input": state.get("processed_input", ""),
        "analysis_result": analysis_result,
        "ai_response": ai_response,
        "final_response": state.get("final_response", ""),
        "execution_log": state.get("execution_log", []) + [log_entry],
        "step_count": step_count,
        "current_node": node_name,
        "node_display_name": display_name,
        "conversation_history": state.get("conversation_history", []),
        "system_prompt": system_prompt
    }

def response_finalizer(state: ConversationState) -> ConversationState:
    """å“åº”ç»ˆç»“å™¨èŠ‚ç‚¹"""
    ai_response = state.get("ai_response", "")
    user_input = state["user_input"]
    step_count = state.get("step_count", 0) + 1
    
    node_name = "response_finalizer"
    display_name = "âœ… å“åº”ç»ˆç»“å™¨"
    
    log_entry = {
        "step": step_count,
        "node": node_name,
        "display_name": display_name,
        "action": "ç»ˆç»“å“åº”å¤„ç†",
        "timestamp": time.strftime("%H:%M:%S"),
        "input": ai_response,
        "status": "processing"
    }
    
    final_response = f"\n\nâœ¨ **æœ€ç»ˆå›ç­”**: \n{ai_response}\n\nâœ… å¤„ç†å®Œæˆäº {time.strftime('%H:%M:%S')}"
    
    # æ·»åŠ å¯¹è¯å†å²
    conversation_entry = {
        "user": user_input,
        "assistant": ai_response,
        "timestamp": time.strftime("%Y-%m-%d %H:%M:%S")
    }
    
    log_entry["result"] = final_response
    log_entry["status"] = "completed"
    
    # è¿”å›å®Œæ•´çš„çŠ¶æ€
    return {
        "user_input": user_input,
        "processed_input": state.get("processed_input", ""),
        "analysis_result": state.get("analysis_result", ""),
        "ai_response": ai_response,
        "final_response": final_response,
        "execution_log": state.get("execution_log", []) + [log_entry],
        "step_count": step_count,
        "current_node": node_name,
        "node_display_name": display_name,
        "conversation_history": state.get("conversation_history", []) + [conversation_entry],
        "system_prompt": state.get("system_prompt", "")
    }
class LangGraphChatInterface:
    """LangGraphæ™ºèƒ½å¯¹è¯ç•Œé¢"""
    
    def __init__(self):
        self.chat_state = ChatState()
        self.models = self._load_model_configs()
        self.current_workflow = None
        self.workflow_diagram = ""
        
    def _load_model_configs(self) -> dict:
        """åŠ è½½é¢„è®¾æ¨¡å‹é…ç½®"""
        return {
            "Qwen3-235B": ModelConfig(
                name="Qwen3-235B",
                model_name="Qwen3-235B-A22B-Instruct-2507",
                base_url="http://39.155.179.5:8002/v1",
                api_key="xxx",
                description="é€šä¹‰åƒé—®3ä»£235Bå‚æ•°æ¨¡å‹",
                temperature=0.7
            ),
            "DeepSeek-V3": ModelConfig(
                name="DeepSeek-V3",
                model_name="deepseek-v3",
                base_url="http://61.49.53.5:30002/v1",
                api_key="",
                description="DeepSeek V3æ¨¡å‹",
                temperature=0.7
            ),
            "ModelScope-Qwen": ModelConfig(
                name="ModelScope-Qwen",
                model_name="qwen-plus-latest",
                base_url="https://dashscope.aliyuncs.com/compatible-mode/v1",
                api_key="sk-b5e591f6a4354b34bf34b26afc20969e",
                description="é˜¿é‡Œäº‘ModelScopeå¹³å°çš„é€šä¹‰åƒé—®",
                temperature=0.7
            )
        }
    
    def _create_workflow(self, model_config: ModelConfig, system_prompt: str):
        """åˆ›å»º LangGraph å·¥ä½œæµ"""
        if not LANGGRAPH_AVAILABLE:
            return None
            
        try:
            # è®¾ç½®ç¯å¢ƒå˜é‡
            os.environ["OPENAI_API_BASE"] = model_config.base_url
            os.environ["OPENAI_API_KEY"] = model_config.api_key
            
            # åˆ›å»ºçŠ¶æ€å›¾
            workflow = StateGraph(ConversationState)
            
            # æ·»åŠ èŠ‚ç‚¹
            workflow.add_node("input_processor", input_processor)
            workflow.add_node("analysis_node", analysis_node)
            workflow.add_node("ai_response_node", ai_response_node)
            workflow.add_node("response_finalizer", response_finalizer)
            
            # è®¾ç½®è¾¹
            workflow.set_entry_point("input_processor")
            workflow.add_edge("input_processor", "analysis_node")
            workflow.add_edge("analysis_node", "ai_response_node")
            workflow.add_edge("ai_response_node", "response_finalizer")
            workflow.add_edge("response_finalizer", END)
            
            # ç¼–è¯‘å›¾
            graph = workflow.compile()
            
            logger.info(f"æˆåŠŸåˆ›å»º LangGraph å·¥ä½œæµï¼Œä½¿ç”¨æ¨¡å‹: {model_config.name}")
            return graph
            
        except Exception as e:
            logger.error(f"åˆ›å»º LangGraph å·¥ä½œæµå¤±è´¥: {e}")
            return None
    
    def _generate_workflow_diagram(self, workflow) -> str:
        """ç”Ÿæˆå·¥ä½œæµå›¾è¡¨"""
        if not workflow:
            return ""
            
        try:
            # ç”Ÿæˆ Mermaid å›¾
            mermaid_code = workflow.get_graph().draw_mermaid()
            return mermaid_code
        except Exception as e:
            logger.error(f"ç”Ÿæˆå·¥ä½œæµå›¾è¡¨å¤±è´¥: {e}")
            return ""
    
    def select_model(self, model_name: str, system_prompt: str):
        """é€‰æ‹©å¹¶åˆå§‹åŒ–æ¨¡å‹"""
        if model_name not in self.models:
            return f"âŒ æœªæ‰¾åˆ°æ¨¡å‹é…ç½®: {model_name}", [], gr.update(), "", ""
        
        model_config = self.models[model_name]
        self.chat_state.current_model = model_name
        self.chat_state.system_prompt = system_prompt or self.chat_state.system_prompt
        
        if LANGGRAPH_AVAILABLE:
            self.current_workflow = self._create_workflow(model_config, self.chat_state.system_prompt)
            if self.current_workflow:
                self.workflow_diagram = self._generate_workflow_diagram(self.current_workflow)
                status_msg = f"âœ… å·²æˆåŠŸåŠ è½½æ¨¡å‹: {model_config.name}\nğŸ“ æœåŠ¡åœ°å€: {model_config.base_url}\nğŸ¯ æ¨¡å‹: {model_config.model_name}\nğŸ› ï¸ å·¥ä½œæµ: å·²åˆ›å»º 4 ä¸ªèŠ‚ç‚¹çš„ LangGraph å·¥ä½œæµ"
            else:
                status_msg = f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {model_config.name}"
                self.workflow_diagram = ""
        else:
            status_msg = f"âš ï¸ æ¨¡æ‹Ÿæ¨¡å¼ - å·²é€‰æ‹©æ¨¡å‹: {model_config.name}\nè¯·å®‰è£…LangGraphåº“ä»¥è·å¾—å®Œæ•´åŠŸèƒ½"
            self.workflow_diagram = ""
        
        # æ¸…ç©ºèŠå¤©å†å²
        self.chat_state.messages = []
        self.chat_state.execution_logs = []
        
        return status_msg, [], gr.update(interactive=True), self.workflow_diagram, ""
    
    def add_custom_model(self, name: str, model_name: str, base_url: str, api_key: str, description: str):
        """æ·»åŠ è‡ªå®šä¹‰æ¨¡å‹é…ç½®"""
        if not all([name, model_name, base_url]):
            return "âŒ è¯·å¡«å†™å¿…è¦çš„æ¨¡å‹ä¿¡æ¯ï¼ˆåç§°ã€æ¨¡å‹åã€æœåŠ¡åœ°å€ï¼‰", gr.update()
        
        self.models[name] = ModelConfig(
            name=name,
            model_name=model_name,
            base_url=base_url,
            api_key=api_key or "",
            description=description or "è‡ªå®šä¹‰æ¨¡å‹é…ç½®"
        )
        
        model_choices = list(self.models.keys())
        return f"âœ… å·²æ·»åŠ è‡ªå®šä¹‰æ¨¡å‹: {name}", gr.update(choices=model_choices, value=name)
    def chat_response(self, message: str, history: List[Tuple[str, Optional[str]]]):
        """å¤„ç†èŠå¤©å“åº”"""
        if not message.strip():
            return history, "", ""
        
        if not self.chat_state.current_model:
            history.append((message, "âŒ è¯·å…ˆé€‰æ‹©ä¸€ä¸ªæ¨¡å‹é…ç½®"))
            return history, "", ""
        
        # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯åˆ°å†å²ï¼ˆä¸´æ—¶è®¾ä¸ºNoneï¼Œåç»­ä¼šæ›´æ–°ï¼‰
        history.append((message, None))
        execution_log_text = ""
        
        try:
            if LANGGRAPH_AVAILABLE and self.current_workflow:
                # ä½¿ç”¨LangGraphå·¥ä½œæµå¤„ç†
                initial_state = {
                    "user_input": message,
                    "system_prompt": self.chat_state.system_prompt,
                    "step_count": 0,
                    "execution_log": [],
                    "conversation_history": []
                }
                
                # æ‰§è¡Œå·¥ä½œæµ
                execution_logs = []
                final_response = ""
                
                for chunk in self.current_workflow.stream(initial_state, stream_mode="values"):
                    if "current_node" in chunk:
                        node_info = {
                            "node": chunk.get("current_node", ""),
                            "display_name": chunk.get("node_display_name", ""),
                            "step": chunk.get("step_count", 0)
                        }
                        execution_logs.append(node_info)
                    
                    if "final_response" in chunk:
                        final_response = chunk["final_response"]
                    
                    if "execution_log" in chunk and chunk["execution_log"]:
                        for log in chunk["execution_log"]:
                            if log not in execution_logs:
                                execution_logs.append(log)
                
                # æ ¼å¼åŒ–æ‰§è¡Œæ—¥å¿—
                if execution_logs:
                    log_lines = ["ğŸ”„ **æ‰§è¡Œè¿‡ç¨‹**:"]
                    for i, log in enumerate(execution_logs[-10:]):
                        if isinstance(log, dict):
                            display_name = log.get('display_name', log.get('node', ''))
                            timestamp = log.get('timestamp', '')
                            action = log.get('action', '')
                            status = log.get('status', '')
                            tool_used = log.get('tool_used', '')
                            
                            if display_name and timestamp:
                                status_icon = "âœ…" if status == "completed" else "â³"
                                tool_info = f" [Tool: {tool_used}]" if tool_used else ""
                                log_lines.append(f"{i+1}. {status_icon} {display_name} - {action} ({timestamp}){tool_info}")
                    
                    execution_log_text = "\n".join(log_lines)
                
                # æå–æœ€ç»ˆå“åº”
                if final_response:
                    # æ¸…ç†æ ¼å¼åŒ–æ–‡æœ¬
                    clean_response = final_response.replace("âœ¨ **æœ€ç»ˆå›ç­”**: \n", "").replace("\n\nâœ… å¤„ç†å®Œæˆäº", "").strip()
                    # æŸ¥æ‰¾æœ€åä¸€ä¸ªæ—¶é—´æˆ³
                    import re
                    time_pattern = r'\d{2}:\d{2}:\d{2}'
                    clean_response = re.sub(time_pattern + r'.*$', '', clean_response).strip()
                    response = clean_response
                else:
                    response = "æŠ±æ­‰ï¼Œå¤„ç†æ‚¨çš„è¯·æ±‚æ—¶æœªèƒ½è·å–åˆ°æœ‰æ•ˆå“åº”ã€‚"
                
            else:
                # æ¨¡æ‹Ÿå“åº”
                response = self._simulate_langgraph_response(message)
                execution_log_text = self._simulate_execution_log()
            
            # æ›´æ–°å†å²è®°å½•
            history[-1] = (message, response)
            
        except Exception as e:
            logger.error(f"èŠå¤©å“åº”å‡ºé”™: {e}")
            history[-1] = (message, f"âŒ å¤„ç†æ¶ˆæ¯æ—¶å‡ºé”™: {str(e)}")
            execution_log_text = f"âŒ æ‰§è¡Œé”™è¯¯: {str(e)}"
        
        return history, "", execution_log_text
    
    def _simulate_langgraph_response(self, message: str) -> str:
        """æ¨¡æ‹Ÿ LangGraph å“åº”"""
        responses = [
            f"æ ¹æ®æ‚¨çš„é—®é¢˜ '{message}'ï¼Œæˆ‘çš„åˆ†æç»“æœå¦‚ä¸‹ï¼š",
            "è¿™æ˜¯ä¸€ä¸ªæ¨¡æ‹Ÿçš„ LangGraph å·¥ä½œæµå“åº”ã€‚",
            "åœ¨çœŸå®ç¯å¢ƒä¸­ï¼Œè¿™é‡Œä¼šè¿è¡Œå®Œæ•´çš„ LangGraph å·¥ä½œæµã€‚",
            "è¯·å®‰è£… LangGraph åº“ä»¥è·å¾—å®Œæ•´çš„æ™ºèƒ½å¯¹è¯åŠŸèƒ½ã€‚"
        ]
        return "\n\n".join(responses)
    
    def _simulate_execution_log(self) -> str:
        """æ¨¡æ‹Ÿæ‰§è¡Œæ—¥å¿—"""
        logs = [
            "ğŸ”„ **æ‰§è¡Œè¿‡ç¨‹** (æ¨¡æ‹Ÿæ¨¡å¼):",
            "1. âœ… ğŸ“ è¾“å…¥å¤„ç†èŠ‚ç‚¹ - å¼€å§‹å¤„ç†ç”¨æˆ·è¾“å…¥ (12:34:56)",
            "2. âœ… ğŸ” æ™ºèƒ½åˆ†æèŠ‚ç‚¹ - å¼€å§‹åˆ†æç”¨æˆ·è¾“å…¥ (12:34:57) [Tool: analyze_user_input]",
            "3. âœ… ğŸ¤– AIå“åº”ç”ŸæˆèŠ‚ç‚¹ - å¼€å§‹ç”ŸæˆAIå“åº” (12:34:58) [Tool: ChatOpenAI]",
            "4. âœ… âœ… å“åº”ç»ˆç»“å™¨ - ç»ˆç»“å“åº”å¤„ç† (12:34:59)",
            "",
            "âš ï¸ æ³¨æ„: å½“å‰ä¸ºæ¨¡æ‹Ÿæ¨¡å¼ï¼Œè¯·å®‰è£… LangGraph åº“ä»¥è·å¾—å®é™…æ‰§è¡Œç»“æœã€‚"
        ]
        return "\n".join(logs)
    
    def clear_chat(self):
        """æ¸…ç©ºèŠå¤©è®°å½•"""
        self.chat_state.messages = []
        self.chat_state.execution_logs = []
        return [], ""
    
    def export_chat(self, history: List[Tuple[str, str]]):
        """å¯¼å‡ºèŠå¤©è®°å½•"""
        if not history:
            return None
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"langgraph_chat_export_{timestamp}.json"
        
        export_data = {
            "timestamp": timestamp,
            "model": self.chat_state.current_model,
            "system_prompt": self.chat_state.system_prompt,
            "workflow_type": "LangGraph",
            "conversations": [{"user": user, "assistant": assistant} for user, assistant in history if user and assistant],
            "execution_logs": self.chat_state.execution_logs,
            "workflow_diagram": self.workflow_diagram
        }
        
        # ç¡®ä¿ logs ç›®å½•å­˜åœ¨
        os.makedirs("logs", exist_ok=True)
        filepath = f"logs/{filename}"
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(export_data, f, ensure_ascii=False, indent=2)
        
        return filepath
    
    def create_interface(self):
        """åˆ›å»º Gradio ç•Œé¢"""
        with gr.Blocks(title="LangGraph æ™ºèƒ½å¯¹è¯ç³»ç»Ÿ", theme=gr.themes.Soft()) as interface:
            gr.Markdown("""
            # ğŸ¤– LangGraph æ™ºèƒ½å¯¹è¯ç³»ç»Ÿ
            
            åŸºäº LangGraph æ¡†æ¶çš„å¯¹è¯ç•Œé¢ï¼Œæ”¯æŒå¯è§†åŒ–å·¥ä½œæµã€èŠ‚ç‚¹æ‰§è¡Œç›‘æ§å’Œå®æ—¶æ—¥å¿—
            """)
            
            with gr.Row():
                with gr.Column(scale=1):
                    gr.Markdown("## âš™ï¸ æ¨¡å‹é…ç½®")
                    
                    # æ¨¡å‹é€‰æ‹©
                    model_dropdown = gr.Dropdown(
                        choices=list(self.models.keys()),
                        label="é€‰æ‹©é¢„è®¾æ¨¡å‹",
                        value="",
                        interactive=True
                    )
                    
                    system_prompt = gr.Textbox(
                        label="ç³»ç»Ÿæç¤ºè¯",
                        value=self.chat_state.system_prompt,
                        lines=3,
                        placeholder="å®šä¹‰AIåŠ©æ‰‹çš„è§’è‰²å’Œè¡Œä¸º..."
                    )
                    
                    select_btn = gr.Button("ğŸš€ åŠ è½½æ¨¡å‹", variant="primary")
                    
                    status_display = gr.Textbox(
                        label="çŠ¶æ€ä¿¡æ¯",
                        lines=6,
                        interactive=False
                    )
                    
                    # è‡ªå®šä¹‰æ¨¡å‹é…ç½®
                    with gr.Accordion("â• æ·»åŠ è‡ªå®šä¹‰æ¨¡å‹", open=False):
                        custom_name = gr.Textbox(label="æ¨¡å‹åç§°", placeholder="ä¾‹å¦‚ï¼šMy-GPT-4")
                        custom_model = gr.Textbox(label="æ¨¡å‹ID", placeholder="ä¾‹å¦‚ï¼šgpt-4-turbo")
                        custom_url = gr.Textbox(label="APIåœ°å€", placeholder="ä¾‹å¦‚ï¼šhttps://api.openai.com/v1")
                        custom_key = gr.Textbox(label="APIå¯†é’¥", type="password", placeholder="sk-...")
                        custom_desc = gr.Textbox(label="æè¿°", placeholder="æ¨¡å‹è¯´æ˜...")
                        
                        add_custom_btn = gr.Button("æ·»åŠ æ¨¡å‹")
                    
                    # å·¥ä½œæµå›¾æ˜¾ç¤º
                    with gr.Accordion("ğŸ”„ LangGraph å·¥ä½œæµå›¾", open=False):
                        workflow_diagram = gr.Code(
                            label="Mermaid å·¥ä½œæµå›¾",
                            language="mermaid",
                            lines=15,
                            interactive=False
                        )
                
                with gr.Column(scale=2):
                    gr.Markdown("## ğŸ’¬ å¯¹è¯åŒºåŸŸ")
                    
                    # èŠå¤©ç•Œé¢
                    chatbot = gr.Chatbot(
                        height=400,
                        show_label=False,
                        avatar_images=("ğŸ‘¤", "ğŸ¤–"),
                        bubble_full_width=False
                    )
                    
                    with gr.Row():
                        msg_input = gr.Textbox(
                            placeholder="è¾“å…¥æ‚¨çš„é—®é¢˜...",
                            show_label=False,
                            scale=4,
                            interactive=False  # åˆå§‹çŠ¶æ€ä¸å¯ç”¨
                        )
                        send_btn = gr.Button("å‘é€", variant="primary", interactive=False)
                    
                    # æ‰§è¡Œæ—¥å¿—
                    execution_log = gr.Textbox(
                        label="ğŸ”„ èŠ‚ç‚¹æ‰§è¡Œæ—¥å¿—",
                        lines=8,
                        max_lines=15,
                        interactive=False,
                        placeholder="èŠ‚ç‚¹æ‰§è¡Œä¿¡æ¯å°†åœ¨è¿™é‡Œå®æ—¶æ˜¾ç¤º..."
                    )
                    
                    with gr.Row():
                        clear_btn = gr.Button("ğŸ—‘ï¸ æ¸…ç©ºå¯¹è¯", variant="secondary")
                        export_btn = gr.Button("ğŸ“¤ å¯¼å‡ºå¯¹è¯", variant="secondary")
            
            # äº‹ä»¶ç»‘å®š
            select_btn.click(
                fn=self.select_model,
                inputs=[model_dropdown, system_prompt],
                outputs=[status_display, chatbot, msg_input, workflow_diagram, execution_log]
            )
            
            add_custom_btn.click(
                fn=self.add_custom_model,
                inputs=[custom_name, custom_model, custom_url, custom_key, custom_desc],
                outputs=[status_display, model_dropdown]
            )
            
            # èŠå¤©åŠŸèƒ½
            def submit_message(message, history):
                return self.chat_response(message, history)
            
            msg_input.submit(
                fn=submit_message,
                inputs=[msg_input, chatbot],
                outputs=[chatbot, msg_input, execution_log]
            )
            
            send_btn.click(
                fn=submit_message,
                inputs=[msg_input, chatbot],
                outputs=[chatbot, msg_input, execution_log]
            )
            
            clear_btn.click(
                fn=self.clear_chat,
                outputs=[chatbot, execution_log]
            )
            
            export_btn.click(
                fn=self.export_chat,
                inputs=[chatbot],
                outputs=[gr.File()]
            )
            
            # é¡µè„šä¿¡æ¯
            gr.Markdown("""
            ---
            ### ğŸ“ ä½¿ç”¨è¯´æ˜
            1. **é€‰æ‹©æ¨¡å‹**ï¼šä»é¢„è®¾æ¨¡å‹ä¸­é€‰æ‹©æˆ–æ·»åŠ è‡ªå®šä¹‰æ¨¡å‹é…ç½®
            2. **é…ç½®ç³»ç»Ÿæç¤ºè¯**ï¼šå®šä¹‰AIåŠ©æ‰‹çš„è§’è‰²å’Œè¡Œä¸ºæ–¹å¼
            3. **åŠ è½½æ¨¡å‹**ï¼šç‚¹å‡»"åŠ è½½æ¨¡å‹"æŒ‰é’®åˆå§‹åŒ–é€‰å®šçš„æ¨¡å‹ï¼ŒåŒæ—¶ç”Ÿæˆ LangGraph å·¥ä½œæµ
            4. **æŸ¥çœ‹å·¥ä½œæµå›¾**ï¼šå±•å¼€"LangGraph å·¥ä½œæµå›¾"å¯ä»¥æŸ¥çœ‹ Mermaid æ ¼å¼çš„å·¥ä½œæµå›¾è¡¨
            5. **å¼€å§‹å¯¹è¯**ï¼šåœ¨è¾“å…¥æ¡†ä¸­è¾“å…¥é—®é¢˜ï¼ŒæŒ‰å›è½¦æˆ–ç‚¹å‡»å‘é€
            6. **ç›‘æ§æ‰§è¡Œ**ï¼šåœ¨"èŠ‚ç‚¹æ‰§è¡Œæ—¥å¿—"åŒºåŸŸå®æ—¶æŸ¥çœ‹å„ä¸ªèŠ‚ç‚¹çš„æ‰§è¡Œè¿‡ç¨‹å’Œå·¥å…·è°ƒç”¨æƒ…å†µ
            7. **å¯¼å‡ºå¯¹è¯**ï¼šå¯ä»¥å°†å¯¹è¯è®°å½•å’Œæ‰§è¡Œæ—¥å¿—å¯¼å‡ºä¸ºJSONæ ¼å¼æ–‡ä»¶
            
            ### ğŸ”„ LangGraph å·¥ä½œæµè¯´æ˜
            - **ğŸ“ è¾“å…¥å¤„ç†èŠ‚ç‚¹**ï¼šå¤„ç†å’Œé¢„å¤„ç†ç”¨æˆ·è¾“å…¥
            - **ğŸ” æ™ºèƒ½åˆ†æèŠ‚ç‚¹**ï¼šä½¿ç”¨å·¥å…·åˆ†æç”¨æˆ·æ„å›¾å’Œå†…å®¹
            - **ğŸ¤– AIå“åº”ç”ŸæˆèŠ‚ç‚¹**ï¼šè°ƒç”¨å¤§è¯­è¨€æ¨¡å‹ç”Ÿæˆæ™ºèƒ½å›å¤
            - **âœ… å“åº”ç»ˆç»“å™¨**ï¼šæ•´ç†å’Œè¾“å‡ºæœ€ç»ˆå›å¤
            
            ### âš ï¸ æ³¨æ„äº‹é¡¹
            - è¯·ç¡®ä¿ç½‘ç»œè¿æ¥æ­£å¸¸ï¼Œèƒ½å¤Ÿè®¿é—®é…ç½®çš„APIåœ°å€
            - APIå¯†é’¥è¯·å¦¥å–„ä¿ç®¡ï¼Œä¸è¦æ³„éœ²ç»™ä»–äºº
            - æ¨¡å‹å“åº”é€Ÿåº¦å–å†³äºç½‘ç»œæ¡ä»¶å’ŒæœåŠ¡å™¨æ€§èƒ½
            - æ‰§è¡Œæ—¥å¿—æ˜¾ç¤º LangGraph å·¥ä½œæµä¸­æ¯ä¸ªèŠ‚ç‚¹çš„æ‰§è¡Œæƒ…å†µ
            """)
        
        return interface

def main():
    """ä¸»å‡½æ•°"""
    # åˆ›å»ºç•Œé¢å®ä¾‹
    chat_interface = LangGraphChatInterface()
    
    # åˆ›å»ºå¹¶å¯åŠ¨ç•Œé¢
    interface = chat_interface.create_interface()
    print(1)
    # å¯åŠ¨æœåŠ¡
    interface.launch(
        server_name="0.0.0.0",
        server_port=7860,
        share=False,
        debug=True,
        show_error=True
    )

if __name__ == "__main__":
    main()