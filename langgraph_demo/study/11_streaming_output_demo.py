# -*- coding: utf-8 -*-
"""
LangGraph æµå¼è¾“å‡ºç¤ºä¾‹
å­¦ä¹ è¦ç‚¹ï¼šæµå¼è¾“å‡ºã€çŠ¶æ€è·Ÿè¸ªã€èŠ‚ç‚¹æ‰§è¡Œç›‘æ§
"""

import os
import sys
import time
import json
from typing import TypedDict, Annotated
from typing_extensions import Annotated
import operator

# æ·»åŠ å½“å‰ç›®å½•åˆ°è·¯å¾„
sys.path.append('.')

from langgraph.graph import StateGraph, START, END
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, AIMessage
from langchain_core.tools import tool
from langgraph.config import get_stream_writer
import config

# è®¾ç½®ç¯å¢ƒå˜é‡
os.environ["OPENAI_API_BASE"] = config.base_url
os.environ["OPENAI_API_KEY"] = config.api_key
MODEL_NAME = config.model

# åˆå§‹åŒ–è¯­è¨€æ¨¡å‹
llm = ChatOpenAI(
    model=MODEL_NAME,
    temperature=0.1,
    max_tokens=1000
)

# å®šä¹‰çŠ¶æ€ç»“æ„
class StreamingState(TypedDict):
    """æµå¼è¾“å‡ºçŠ¶æ€å®šä¹‰"""
    user_input: str                    # ç”¨æˆ·è¾“å…¥
    processed_input: str               # å¤„ç†åçš„è¾“å…¥
    analysis_result: str               # åˆ†æç»“æœ
    recommendation: str                # æ¨èç»“æœ
    final_response: str                # æœ€ç»ˆå“åº”
    execution_log: Annotated[list, operator.add]  # æ‰§è¡Œæ—¥å¿—
    step_count: int                    # æ­¥éª¤è®¡æ•°
    current_node: str                  # å½“å‰èŠ‚ç‚¹åç§°
    node_display_name: str             # èŠ‚ç‚¹æ˜¾ç¤ºåç§°

# å®šä¹‰å·¥å…·
@tool
def analyze_user_input(text: str) -> str:
    """åˆ†æç”¨æˆ·è¾“å…¥çš„å·¥å…·"""
    writer = get_stream_writer()
    writer({"type": "progress", "message": f"å¼€å§‹åˆ†æç”¨æˆ·è¾“å…¥: {text[:20]}..."})
    
    # æ¨¡æ‹Ÿåˆ†æè¿‡ç¨‹
    time.sleep(0.1)
    writer({"type": "progress", "message": "æ­£åœ¨è¯†åˆ«ç”¨æˆ·æ„å›¾..."})
    
    time.sleep(0.1)
    writer({"type": "progress", "message": "åˆ†æå®Œæˆ"})
    
    return f"åˆ†æç»“æœ: ç”¨æˆ·è¾“å…¥åŒ…å« {len(text)} ä¸ªå­—ç¬¦ï¼Œä¸»è¦æ„å›¾æ˜¯è·å–ä¿¡æ¯"

@tool
def generate_recommendation(topic: str) -> str:
    """ç”Ÿæˆæ¨èçš„å·¥å…·"""
    writer = get_stream_writer()
    writer({"type": "progress", "message": f"æ­£åœ¨ä¸º '{topic}' ç”Ÿæˆæ¨è..."})
    
    # æ¨¡æ‹Ÿæ¨èç”Ÿæˆè¿‡ç¨‹
    time.sleep(0.1)
    writer({"type": "progress", "message": "æ”¶é›†ç›¸å…³æ•°æ®..."})
    
    time.sleep(0.1)
    writer({"type": "progress", "message": "ç”Ÿæˆä¸ªæ€§åŒ–æ¨è..."})
    
    time.sleep(0.1)
    writer({"type": "progress", "message": "æ¨èç”Ÿæˆå®Œæˆ"})
    
    return f"åŸºäº '{topic}' çš„æ¨è: å»ºè®®æ‚¨æ·±å…¥äº†è§£ç›¸å…³æŠ€æœ¯ï¼Œå¹¶å®è·µåº”ç”¨"

# å®šä¹‰èŠ‚ç‚¹å‡½æ•°
def input_processor(state: StreamingState) -> StreamingState:
    """
    è¾“å…¥å¤„ç†èŠ‚ç‚¹
    å­¦ä¹ è¦ç‚¹ï¼šçŠ¶æ€æ›´æ–°ã€æ—¥å¿—è®°å½•ã€èŠ‚ç‚¹åç§°è·Ÿè¸ª
    """
    user_input = state["user_input"]
    step_count = state.get("step_count", 0) + 1
    
    # èŠ‚ç‚¹ä¿¡æ¯
    node_name = "input_processor"
    display_name = "ğŸ“ è¾“å…¥å¤„ç†èŠ‚ç‚¹"
    
    # è®°å½•èŠ‚ç‚¹å¼€å§‹æ‰§è¡Œ
    log_entry = {
        "step": step_count,
        "node": node_name,
        "display_name": display_name,
        "action": "å¼€å§‹å¤„ç†ç”¨æˆ·è¾“å…¥",
        "timestamp": time.strftime("%H:%M:%S"),
        "input": user_input
    }
    
    # å¤„ç†è¾“å…¥
    processed_input = f"å¤„ç†åçš„è¾“å…¥: {user_input.upper()}"
    
    return {
        "processed_input": processed_input,
        "step_count": step_count,
        "current_node": node_name,
        "node_display_name": display_name,
        "execution_log": [log_entry]
    }

def analysis_node(state: StreamingState) -> StreamingState:
    """
    åˆ†æèŠ‚ç‚¹
    å­¦ä¹ è¦ç‚¹ï¼šå·¥å…·è°ƒç”¨ã€æµå¼è¾“å‡ºã€èŠ‚ç‚¹åç§°è·Ÿè¸ª
    """
    user_input = state["user_input"]
    step_count = state.get("step_count", 0) + 1
    
    # èŠ‚ç‚¹ä¿¡æ¯
    node_name = "analysis_node"
    display_name = "ğŸ” æ™ºèƒ½åˆ†æèŠ‚ç‚¹"
    
    # è®°å½•èŠ‚ç‚¹å¼€å§‹æ‰§è¡Œ
    log_entry = {
        "step": step_count,
        "node": node_name,
        "display_name": display_name,
        "action": "å¼€å§‹åˆ†æç”¨æˆ·è¾“å…¥",
        "timestamp": time.strftime("%H:%M:%S"),
        "input": user_input
    }
    
    # è°ƒç”¨åˆ†æå·¥å…·
    analysis_result = analyze_user_input(user_input)
    
    # è®°å½•åˆ†æå®Œæˆ
    log_entry["result"] = analysis_result
    log_entry["status"] = "completed"
    
    return {
        "analysis_result": analysis_result,
        "step_count": step_count,
        "current_node": node_name,
        "node_display_name": display_name,
        "execution_log": [log_entry]
    }

def recommendation_node(state: StreamingState) -> StreamingState:
    """
    æ¨èç”ŸæˆèŠ‚ç‚¹
    å­¦ä¹ è¦ç‚¹ï¼šLLMè°ƒç”¨ã€çŠ¶æ€ä¼ é€’ã€èŠ‚ç‚¹åç§°è·Ÿè¸ª
    """
    user_input = state["user_input"]
    step_count = state.get("step_count", 0) + 1
    
    # èŠ‚ç‚¹ä¿¡æ¯
    node_name = "recommendation_node"
    display_name = "ğŸ’¡ AIæ¨èèŠ‚ç‚¹"
    
    # è®°å½•èŠ‚ç‚¹å¼€å§‹æ‰§è¡Œ
    log_entry = {
        "step": step_count,
        "node": node_name,
        "display_name": display_name,
        "action": "å¼€å§‹ç”Ÿæˆæ¨è",
        "timestamp": time.strftime("%H:%M:%S"),
        "input": user_input
    }
    
    # ä½¿ç”¨LLMç”Ÿæˆæ¨è
    prompt = f"""
åŸºäºç”¨æˆ·è¾“å…¥: "{user_input}"
è¯·ç”Ÿæˆä¸€ä¸ªæœ‰ç”¨çš„æ¨èæˆ–å»ºè®®ã€‚
"""
    
    response = llm.invoke([HumanMessage(content=prompt)])
    recommendation = response.content
    
    # è®°å½•æ¨èå®Œæˆ
    log_entry["result"] = recommendation
    log_entry["status"] = "completed"
    
    return {
        "recommendation": recommendation,
        "step_count": step_count,
        "current_node": node_name,
        "node_display_name": display_name,
        "execution_log": [log_entry]
    }

def response_synthesizer(state: StreamingState) -> StreamingState:
    """
    å“åº”åˆæˆèŠ‚ç‚¹
    å­¦ä¹ è¦ç‚¹ï¼šçŠ¶æ€æ•´åˆã€æœ€ç»ˆè¾“å‡ºã€èŠ‚ç‚¹åç§°è·Ÿè¸ª
    """
    user_input = state["user_input"]
    analysis_result = state.get("analysis_result", "")
    recommendation = state.get("recommendation", "")
    step_count = state.get("step_count", 0) + 1
    
    # èŠ‚ç‚¹ä¿¡æ¯
    node_name = "response_synthesizer"
    display_name = "ğŸ¯ å“åº”åˆæˆèŠ‚ç‚¹"
    
    # è®°å½•èŠ‚ç‚¹å¼€å§‹æ‰§è¡Œ
    log_entry = {
        "step": step_count,
        "node": node_name,
        "display_name": display_name,
        "action": "å¼€å§‹åˆæˆæœ€ç»ˆå“åº”",
        "timestamp": time.strftime("%H:%M:%S"),
        "input": user_input
    }
    
    # åˆæˆæœ€ç»ˆå“åº”
    final_response = f"""
åŸºäºæ‚¨çš„è¾“å…¥: "{user_input}"

{analysis_result}

{recommendation}

æ„Ÿè°¢æ‚¨çš„ä½¿ç”¨ï¼
"""
    
    # è®°å½•åˆæˆå®Œæˆ
    log_entry["result"] = final_response
    log_entry["status"] = "completed"
    
    return {
        "final_response": final_response,
        "step_count": step_count,
        "current_node": node_name,
        "node_display_name": display_name,
        "execution_log": [log_entry]
    }

def create_streaming_workflow():
    """
    åˆ›å»ºæµå¼è¾“å‡ºå·¥ä½œæµ
    å­¦ä¹ è¦ç‚¹ï¼šå›¾æ„å»ºã€èŠ‚ç‚¹è¿æ¥
    """
    # åˆ›å»ºçŠ¶æ€å›¾
    workflow = StateGraph(StreamingState)
    
    # æ·»åŠ èŠ‚ç‚¹
    workflow.add_node("input_processor", input_processor)
    workflow.add_node("analysis_node", analysis_node)
    workflow.add_node("recommendation_node", recommendation_node)
    workflow.add_node("response_synthesizer", response_synthesizer)
    
    # è®¾ç½®å…¥å£ç‚¹
    workflow.set_entry_point("input_processor")
    
    # æ·»åŠ è¾¹
    workflow.add_edge("input_processor", "analysis_node")
    workflow.add_edge("analysis_node", "recommendation_node")
    workflow.add_edge("recommendation_node", "response_synthesizer")
    workflow.add_edge("response_synthesizer", END)
    
    # ç¼–è¯‘å·¥ä½œæµ
    return workflow.compile()

def test_streaming_modes():
    """
    æµ‹è¯•ä¸åŒçš„æµå¼è¾“å‡ºæ¨¡å¼
    å­¦ä¹ è¦ç‚¹ï¼šæµå¼è¾“å‡ºæ¨¡å¼ã€çŠ¶æ€ç›‘æ§
    """
    print("ğŸš€ LangGraph æµå¼è¾“å‡ºç¤ºä¾‹")
    print("=" * 60)
    
    # åˆ›å»ºå·¥ä½œæµ
    graph = create_streaming_workflow()
    
    # æµ‹è¯•è¾“å…¥
    test_inputs = [
        # "æˆ‘æƒ³å­¦ä¹ Pythonç¼–ç¨‹",
        # "å¦‚ä½•æé«˜ä»£ç è´¨é‡",
        "æ¨èä¸€äº›AIå­¦ä¹ èµ„æº"
    ]
    
    for i, user_input in enumerate(test_inputs, 1):
        print(f"\nğŸ“ æµ‹è¯• {i}: {user_input}")
        print("-" * 40)
        
        # å‡†å¤‡è¾“å…¥çŠ¶æ€
        inputs = {
            "user_input": user_input,
            "execution_log": [],
            "step_count": 0
        }
        
        # æµ‹è¯•ä¸åŒçš„æµå¼æ¨¡å¼
        test_modes = [
            ("values", "å®Œæ•´çŠ¶æ€å€¼"),
            # ("updates", "çŠ¶æ€æ›´æ–°"),
            # ("custom", "è‡ªå®šä¹‰æ•°æ®"),
            # ("debug", "è°ƒè¯•ä¿¡æ¯")
        ]
        
        for mode, description in test_modes:
            print(f"\nğŸ” æ¨¡å¼: {description} ({mode})")
            print("-" * 30)
            
            try:
                # æ‰§è¡Œæµå¼è¾“å‡º
                for chunk in graph.stream(inputs, stream_mode=mode):
                    if mode == "values":
                        # å®Œæ•´çŠ¶æ€å€¼æ¨¡å¼
                        print(f"å®Œæ•´çŠ¶æ€: {json.dumps(chunk, ensure_ascii=False, indent=2)}")
                    elif mode == "updates":
                        # çŠ¶æ€æ›´æ–°æ¨¡å¼
                        print(f"çŠ¶æ€æ›´æ–°: {json.dumps(chunk, ensure_ascii=False, indent=2)}")
                    elif mode == "custom":
                        # è‡ªå®šä¹‰æ•°æ®æ¨¡å¼
                        print(f"è‡ªå®šä¹‰æ•°æ®: {json.dumps(chunk, ensure_ascii=False, indent=2)}")
                    elif mode == "debug":
                        # è°ƒè¯•æ¨¡å¼
                        print(f"è°ƒè¯•ä¿¡æ¯: {json.dumps(chunk, ensure_ascii=False, indent=2)}")
                    
                    print()  # ç©ºè¡Œåˆ†éš”
                    
            except Exception as e:
                print(f"âŒ é”™è¯¯: {e}")

def test_multi_mode_streaming():
    """
    æµ‹è¯•å¤šæ¨¡å¼æµå¼è¾“å‡º
    å­¦ä¹ è¦ç‚¹ï¼šåŒæ—¶ç›‘æ§å¤šç§è¾“å‡ºç±»å‹
    """
    print("\nğŸ¯ å¤šæ¨¡å¼æµå¼è¾“å‡ºæµ‹è¯•")
    print("=" * 60)
    
    # åˆ›å»ºå·¥ä½œæµ
    graph = create_streaming_workflow()
    
    # æµ‹è¯•è¾“å…¥
    user_input = "è¯·å¸®æˆ‘åˆ†æä¸€ä¸‹æœºå™¨å­¦ä¹ çš„å‘å±•è¶‹åŠ¿"
    
    print(f"ğŸ“ è¾“å…¥: {user_input}")
    print("-" * 40)
    
    # å‡†å¤‡è¾“å…¥çŠ¶æ€
    inputs = {
        "user_input": user_input,
        "execution_log": [],
        "step_count": 0
    }
    
    try:
        # åŒæ—¶ä½¿ç”¨å¤šç§æµå¼æ¨¡å¼
        for mode, chunk in graph.stream(
            inputs, 
            stream_mode=["updates", "custom"]
        ):
            print(f"ğŸ“Š æ¨¡å¼: {mode}")
            print(f"ğŸ“¦ æ•°æ®: {json.dumps(chunk, ensure_ascii=False, indent=2)}")
            print("-" * 30)
            
    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}")

def test_node_filtering():
    """
    æµ‹è¯•èŠ‚ç‚¹è¿‡æ»¤
    å­¦ä¹ è¦ç‚¹ï¼šç‰¹å®šèŠ‚ç‚¹çš„è¾“å‡ºç›‘æ§
    """
    print("\nğŸ¯ èŠ‚ç‚¹è¿‡æ»¤æµ‹è¯•")
    print("=" * 60)
    
    # åˆ›å»ºå·¥ä½œæµ
    graph = create_streaming_workflow()
    
    # æµ‹è¯•è¾“å…¥
    user_input = "æˆ‘æƒ³äº†è§£æ·±åº¦å­¦ä¹ "
    
    print(f"ğŸ“ è¾“å…¥: {user_input}")
    print("-" * 40)
    
    # å‡†å¤‡è¾“å…¥çŠ¶æ€
    inputs = {
        "user_input": user_input,
        "execution_log": [],
        "step_count": 0
    }
    
    try:
        # åªç›‘æ§ç‰¹å®šèŠ‚ç‚¹çš„è¾“å‡º
        target_nodes = ["analysis_node", "recommendation_node"]
        
        for chunk in graph.stream(inputs, stream_mode="updates"):
            # æ£€æŸ¥æ˜¯å¦æ˜¯ç›®æ ‡èŠ‚ç‚¹çš„è¾“å‡º
            for node_name in target_nodes:
                if node_name in chunk:
                    print(f"ğŸ¯ ç›®æ ‡èŠ‚ç‚¹ {node_name} è¾“å‡º:")
                    print(f"ğŸ“¦ æ•°æ®: {json.dumps(chunk[node_name], ensure_ascii=False, indent=2)}")
                    print("-" * 30)
                    
    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}")

def show_workflow_structure():
    """
    æ˜¾ç¤ºå·¥ä½œæµç»“æ„
    å­¦ä¹ è¦ç‚¹ï¼šå›¾ç»“æ„å¯è§†åŒ–
    """
    print("\nğŸ—ï¸ å·¥ä½œæµç»“æ„")
    print("=" * 60)
    
    # åˆ›å»ºå·¥ä½œæµ
    graph = create_streaming_workflow()
    
    print("èŠ‚ç‚¹åˆ—è¡¨:")
    print("- input_processor: ğŸ“ è¾“å…¥å¤„ç†èŠ‚ç‚¹")
    print("- analysis_node: ğŸ” æ™ºèƒ½åˆ†æèŠ‚ç‚¹")
    print("- recommendation_node: ğŸ’¡ AIæ¨èèŠ‚ç‚¹")
    print("- response_synthesizer: ğŸ¯ å“åº”åˆæˆèŠ‚ç‚¹")
    
    print("\næ‰§è¡Œæµç¨‹:")
    print("START â†’ input_processor â†’ analysis_node â†’ recommendation_node â†’ response_synthesizer â†’ END")
    
    print("\nçŠ¶æ€å­—æ®µ:")
    state_fields = [
        "user_input: ç”¨æˆ·è¾“å…¥",
        "processed_input: å¤„ç†åçš„è¾“å…¥",
        "analysis_result: åˆ†æç»“æœ",
        "recommendation: æ¨èç»“æœ",
        "final_response: æœ€ç»ˆå“åº”",
        "execution_log: æ‰§è¡Œæ—¥å¿—",
        "step_count: æ­¥éª¤è®¡æ•°",
        "current_node: å½“å‰èŠ‚ç‚¹åç§°",
        "node_display_name: èŠ‚ç‚¹æ˜¾ç¤ºåç§°"
    ]
    
    for field in state_fields:
        print(f"- {field}")

def test_node_name_tracking():
    """
    æµ‹è¯•èŠ‚ç‚¹åç§°è·Ÿè¸ªåŠŸèƒ½
    å­¦ä¹ è¦ç‚¹ï¼šèŠ‚ç‚¹åç§°ç›‘æ§ã€è‡ªå®šä¹‰æ˜¾ç¤ºåç§°
    """
    print("\nğŸ¯ èŠ‚ç‚¹åç§°è·Ÿè¸ªæµ‹è¯•")
    print("=" * 60)
    
    # åˆ›å»ºå·¥ä½œæµ
    graph = create_streaming_workflow()
    
    # æµ‹è¯•è¾“å…¥
    user_input = "æˆ‘æƒ³å­¦ä¹ æœºå™¨å­¦ä¹ "
    
    print(f"ğŸ“ è¾“å…¥: {user_input}")
    print("-" * 40)
    
    # å‡†å¤‡è¾“å…¥çŠ¶æ€
    inputs = {
        "user_input": user_input,
        "execution_log": [],
        "step_count": 0
    }
    
    print("ğŸ” èŠ‚ç‚¹æ‰§è¡Œè·Ÿè¸ª:")
    print("-" * 40)
    
    try:
        # ä½¿ç”¨ updates æ¨¡å¼è·Ÿè¸ªèŠ‚ç‚¹æ‰§è¡Œ
        for chunk in graph.stream(inputs, stream_mode="updates"):
            # æå–èŠ‚ç‚¹ä¿¡æ¯
            for key, value in chunk.items():
                if isinstance(value, dict) and "current_node" in value:
                    node_name = value["current_node"]
                    display_name = value.get("node_display_name", node_name)
                    step_count = value.get("step_count", 0)
                    
                    print(f"ğŸ“ æ­¥éª¤ {step_count}: {display_name} ({node_name})")
                    
                    # æ˜¾ç¤ºèŠ‚ç‚¹æ‰§è¡Œæ—¥å¿—
                    if "execution_log" in value:
                        for log in value["execution_log"]:
                            if log.get("node") == node_name:
                                print(f"   â° æ—¶é—´: {log.get('timestamp', 'N/A')}")
                                print(f"   ğŸ¯ åŠ¨ä½œ: {log.get('action', 'N/A')}")
                                if "result" in log:
                                    result = log["result"]
                                    if len(result) > 100:
                                        result = result[:100] + "..."
                                    print(f"   ğŸ“Š ç»“æœ: {result}")
                                print()
                    
    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}")

def create_custom_node_names():
    """
    åˆ›å»ºè‡ªå®šä¹‰èŠ‚ç‚¹åç§°çš„ç¤ºä¾‹
    å­¦ä¹ è¦ç‚¹ï¼šèŠ‚ç‚¹åç§°è‡ªå®šä¹‰ã€æ˜¾ç¤ºåç§°é…ç½®
    """
    print("\nğŸ¨ è‡ªå®šä¹‰èŠ‚ç‚¹åç§°ç¤ºä¾‹")
    print("=" * 60)
    
    # èŠ‚ç‚¹åç§°é…ç½®
    node_configs = {
        "input_processor": {
            "display_name": "ğŸš€ æ•°æ®é¢„å¤„ç†å¼•æ“",
            "description": "æ™ºèƒ½å¤„ç†ç”¨æˆ·è¾“å…¥æ•°æ®"
        },
        "analysis_node": {
            "display_name": "ğŸ§  æ·±åº¦å­¦ä¹ åˆ†æå™¨",
            "description": "ä½¿ç”¨AIæŠ€æœ¯åˆ†æç”¨æˆ·æ„å›¾"
        },
        "recommendation_node": {
            "display_name": "ğŸ¯ æ™ºèƒ½æ¨èç³»ç»Ÿ",
            "description": "åŸºäºAIç”Ÿæˆä¸ªæ€§åŒ–æ¨è"
        },
        "response_synthesizer": {
            "display_name": "âœ¨ å“åº”ç”Ÿæˆå™¨",
            "description": "æ•´åˆæ‰€æœ‰ç»“æœç”Ÿæˆæœ€ç»ˆå“åº”"
        }
    }
    
    print("è‡ªå®šä¹‰èŠ‚ç‚¹é…ç½®:")
    for node_name, config in node_configs.items():
        print(f"- {node_name}: {config['display_name']}")
        print(f"  æè¿°: {config['description']}")
    
    print("\nä½¿ç”¨æ–¹æ³•:")
    print("1. åœ¨èŠ‚ç‚¹å‡½æ•°ä¸­è®¾ç½® node_name å’Œ display_name")
    print("2. åœ¨çŠ¶æ€ä¸­è·Ÿè¸ª current_node å’Œ node_display_name")
    print("3. åœ¨æµå¼è¾“å‡ºä¸­ç›‘æ§èŠ‚ç‚¹æ‰§è¡Œè¿‡ç¨‹")
    print("4. å¯ä»¥æ ¹æ®éœ€è¦åŠ¨æ€ä¿®æ”¹æ˜¾ç¤ºåç§°")

if __name__ == "__main__":
    # æ˜¾ç¤ºå·¥ä½œæµç»“æ„
    show_workflow_structure()
    
    # æ˜¾ç¤ºè‡ªå®šä¹‰èŠ‚ç‚¹åç§°ç¤ºä¾‹
    # create_custom_node_names()
    
    # æµ‹è¯•èŠ‚ç‚¹åç§°è·Ÿè¸ªåŠŸèƒ½
    # test_node_name_tracking()
    
    # æµ‹è¯•ä¸åŒçš„æµå¼è¾“å‡ºæ¨¡å¼
    # test_streaming_modes()
    
    # æµ‹è¯•å¤šæ¨¡å¼æµå¼è¾“å‡º
    test_multi_mode_streaming()
    
    # æµ‹è¯•èŠ‚ç‚¹è¿‡æ»¤
    # test_node_filtering()
    
    print("\nâœ… æµå¼è¾“å‡ºç¤ºä¾‹å®Œæˆï¼")
    print("\nğŸ“š å­¦ä¹ è¦ç‚¹æ€»ç»“:")
    print("1. æµå¼è¾“å‡ºæ¨¡å¼: values, updates, custom, debug")
    print("2. çŠ¶æ€è·Ÿè¸ª: ç›‘æ§èŠ‚ç‚¹æ‰§è¡Œå’ŒçŠ¶æ€å˜åŒ–")
    print("3. è‡ªå®šä¹‰è¾“å‡º: ä½¿ç”¨ get_stream_writer() å‘é€è‡ªå®šä¹‰æ•°æ®")
    print("4. å¤šæ¨¡å¼ç›‘æ§: åŒæ—¶ä½¿ç”¨å¤šç§æµå¼æ¨¡å¼")
    print("5. èŠ‚ç‚¹è¿‡æ»¤: ç›‘æ§ç‰¹å®šèŠ‚ç‚¹çš„è¾“å‡º")
    print("6. èŠ‚ç‚¹åç§°è·Ÿè¸ª: è¿”å›å½“å‰èŠ‚ç‚¹åç§°å’Œè‡ªå®šä¹‰æ˜¾ç¤ºåç§°")
    print("7. è‡ªå®šä¹‰èŠ‚ç‚¹åç§°: æ”¯æŒä¸­æ–‡ã€emojiå’Œæè¿°æ€§åç§°") 