# -*- coding: utf-8 -*-
"""
LangGraph æ¡ä»¶è·¯ç”±ç¤ºä¾‹
å­¦ä¹ è¦ç‚¹ï¼šæ¡ä»¶è¾¹ã€åŠ¨æ€å†³ç­–ã€è·¯ç”±å‡½æ•°

ä½œè€…: AI Assistant
æ¥æº: LangGraph å®˜æ–¹æ–‡æ¡£å­¦ä¹ 
"""

import os
from typing import TypedDict, List
from typing_extensions import Annotated

# LangGraph æ ¸å¿ƒç»„ä»¶
from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import add_messages

# LangChain ç»„ä»¶
from langchain_core.messages import HumanMessage, AIMessage
from langchain_openai import ChatOpenAI

import config

# è‡ªå®šä¹‰æ¨¡å‹é…ç½®
os.environ["OPENAI_API_BASE"] = config.base_url
os.environ["OPENAI_API_KEY"] = config.api_key
MODEL_NAME = config.model

# è·å–æ—¥å¿—å™¨
logger = config.logger

# åˆå§‹åŒ–æ¨¡å‹ - æ”¯æŒè‡ªå®šä¹‰æ¨¡å‹
def create_llm(model_name=None, temperature=0.1):
    """
    åˆ›å»ºLLMå®ä¾‹ï¼Œæ”¯æŒè‡ªå®šä¹‰æ¨¡å‹
    """
    if model_name is None:
        model_name = MODEL_NAME
    
    return ChatOpenAI(
        model=model_name,
        temperature=temperature,
        streaming=False
    )

# é»˜è®¤æ¨¡å‹å®ä¾‹
llm = create_llm()

# è‡ªå®šä¹‰æ¨¡å‹å®ä¾‹ - context7
context7_llm = create_llm("context7", temperature=0.1)

# ============================================================================
# æ¡ä»¶è·¯ç”±çŠ¶æ€å®šä¹‰
# ============================================================================

class RoutingState(TypedDict):
    """æ¡ä»¶è·¯ç”±çŠ¶æ€ - åŒ…å«å†³ç­–ç»“æœå’Œè·¯ç”±ä¿¡æ¯"""
    messages: Annotated[List[HumanMessage | AIMessage], add_messages]  # æ¶ˆæ¯å†å²ï¼Œè‡ªåŠ¨åˆå¹¶
    user_input: str  # ç”¨æˆ·è¾“å…¥å†…å®¹
    decision: str  # å†³ç­–ç»“æœï¼ˆå†³å®šèµ°å“ªä¸ªåˆ†æ”¯ï¼‰
    route_reason: str  # å†³ç­–åŸå› è¯´æ˜
    response: str  # æœ€ç»ˆå“åº”å†…å®¹
    processing_path: List[str]  # å¤„ç†è·¯å¾„è®°å½•ï¼ˆç”¨äºè¿½è¸ªæ‰§è¡Œæµç¨‹ï¼‰

# ============================================================================
# å†³ç­–èŠ‚ç‚¹å®šä¹‰
# ============================================================================

def decision_maker(state: RoutingState) -> RoutingState:
    """
    å†³ç­–åˆ¶å®šèŠ‚ç‚¹ - åˆ†æç”¨æˆ·è¾“å…¥å¹¶å†³å®šå¤„ç†è·¯å¾„
    ä½œç”¨ï¼šä½œä¸ºå·¥ä½œæµçš„å…¥å£èŠ‚ç‚¹ï¼Œåˆ†æç”¨æˆ·æ„å›¾å¹¶å†³å®šåç»­å¤„ç†è·¯å¾„
    å­¦ä¹ è¦ç‚¹ï¼šæ™ºèƒ½å†³ç­–é€»è¾‘
    """
    logger.info("ğŸ§  å†³ç­–åˆ¶å®šèŠ‚ç‚¹æ­£åœ¨åˆ†æ...")
    logger.info(f"ä½¿ç”¨æ¨¡å‹: {MODEL_NAME}")
    
    user_input = state["user_input"].lower()
    processing_path = state.get("processing_path", [])
    processing_path.append("decision_maker")  # è®°å½•æ‰§è¡Œè·¯å¾„
    
    # æ£€æŸ¥æ˜¯å¦æ˜¯æ¨¡å‹ç›¸å…³é—®é¢˜ - æ‰©å±•å…³é”®è¯æ£€æµ‹
    model_keywords = [
        "æ¨¡å‹", "model", "ä½ æ˜¯è°", "ä½ æ˜¯ä»€ä¹ˆ", "ä½ å«ä»€ä¹ˆ", "ä½ ç”±ä»€ä¹ˆ", "ä½ ç”¨ä»€ä¹ˆ", "ä½ åŸºäºä»€ä¹ˆ",
        "ä½ æ˜¯ä»€ä¹ˆæ¨¡å‹", "ä½ å«ä»€ä¹ˆåå­—", "ä½ çš„èº«ä»½", "ä½ çš„èƒŒæ™¯", "ä½ æ¥è‡ªå“ªé‡Œ", "ä½ æ˜¯ä»€ä¹ˆAI",
        "ä½ æ˜¯ä»€ä¹ˆäººå·¥æ™ºèƒ½", "ä½ æ˜¯ä»€ä¹ˆåŠ©æ‰‹", "ä½ æ˜¯ä»€ä¹ˆå·¥å…·", "ä½ æ˜¯ä»€ä¹ˆç³»ç»Ÿ"
    ]
    if any(keyword in user_input for keyword in model_keywords):
        decision = "model_info"
        route_reason = "æ£€æµ‹åˆ°æ¨¡å‹ç›¸å…³é—®é¢˜"
    else:
        # ä½¿ç”¨æ¨¡å‹è¿›è¡Œæ™ºèƒ½å†³ç­–
        decision_prompt = f"""
        è¯·åˆ†æä»¥ä¸‹ç”¨æˆ·è¾“å…¥ï¼Œå¹¶å†³å®šåº”è¯¥ä½¿ç”¨å“ªä¸ªä¸“ä¸šæ™ºèƒ½ä½“æ¥å¤„ç†ï¼š
        
        ç”¨æˆ·è¾“å…¥: {state["user_input"]}
        
        å¯é€‰çš„æ™ºèƒ½ä½“:
        1. calculator - å¤„ç†æ•°å­¦è®¡ç®—ã€æ•°å­—è¿ç®—ç›¸å…³è¯·æ±‚
        2. weather - å¤„ç†å¤©æ°”æŸ¥è¯¢ã€æ¸©åº¦ã€å¤©æ°”çŠ¶å†µç›¸å…³è¯·æ±‚  
        3. search - å¤„ç†ä¿¡æ¯æœç´¢ã€æŸ¥æ‰¾èµ„æ–™ç›¸å…³è¯·æ±‚
        4. translator - å¤„ç†ç¿»è¯‘ã€è¯­è¨€è½¬æ¢ç›¸å…³è¯·æ±‚
        5. general - å¤„ç†å…¶ä»–é€šç”¨è¯·æ±‚
        
        è¯·åªè¿”å›å¯¹åº”çš„æ™ºèƒ½ä½“åç§°ï¼ˆcalculator/weather/search/translator/generalï¼‰ï¼Œä¸è¦åŒ…å«å…¶ä»–å†…å®¹ã€‚
        """
        
        try:
            # è°ƒç”¨æ¨¡å‹è¿›è¡Œå†³ç­–
            response = llm.invoke([HumanMessage(content=decision_prompt)])
            logger.info(f"æ¨¡å‹è¾“å‡º:{response.content}")
            decision = response.content.strip().lower()
            
            # éªŒè¯å†³ç­–ç»“æœ
            valid_decisions = ["calculator", "weather", "search", "translator", "general"]
            if decision not in valid_decisions:
                decision = "general"
                route_reason = "æ¨¡å‹å†³ç­–ç»“æœæ— æ•ˆï¼Œä½¿ç”¨é€šç”¨å¤„ç†"
            else:
                # æ ¹æ®å†³ç­–ç»“æœè®¾ç½®åŸå› 
                decision_reasons = {
                    "calculator": "æ¨¡å‹åˆ¤æ–­ä¸ºæ•°å­¦è®¡ç®—éœ€æ±‚",
                    "weather": "æ¨¡å‹åˆ¤æ–­ä¸ºå¤©æ°”æŸ¥è¯¢éœ€æ±‚", 
                    "search": "æ¨¡å‹åˆ¤æ–­ä¸ºä¿¡æ¯æœç´¢éœ€æ±‚",
                    "translator": "æ¨¡å‹åˆ¤æ–­ä¸ºç¿»è¯‘éœ€æ±‚",
                    "general": "æ¨¡å‹åˆ¤æ–­ä¸ºé€šç”¨å¤„ç†éœ€æ±‚"
                }
                route_reason = decision_reasons.get(decision, "æ¨¡å‹å†³ç­–")
                
        except Exception as e:
            logger.error(f"æ¨¡å‹è°ƒç”¨å¤±è´¥: {e}")
            # å›é€€åˆ°å…³é”®è¯åŒ¹é…
            if any(word in user_input for word in ["è®¡ç®—", "æ•°å­¦", "æ•°å­—", "+", "-", "*", "/"]):
                decision = "calculator"
                route_reason = "æ£€æµ‹åˆ°æ•°å­¦è®¡ç®—éœ€æ±‚ï¼ˆå›é€€æ¨¡å¼ï¼‰"
            elif any(word in user_input for word in ["å¤©æ°”", "æ¸©åº¦", "ä¸‹é›¨", "æ™´å¤©"]):
                decision = "weather"
                route_reason = "æ£€æµ‹åˆ°å¤©æ°”æŸ¥è¯¢éœ€æ±‚ï¼ˆå›é€€æ¨¡å¼ï¼‰"
            elif any(word in user_input for word in ["æœç´¢", "æŸ¥æ‰¾", "ä¿¡æ¯", "èµ„æ–™"]):
                decision = "search"
                route_reason = "æ£€æµ‹åˆ°ä¿¡æ¯æœç´¢éœ€æ±‚ï¼ˆå›é€€æ¨¡å¼ï¼‰"
            elif any(word in user_input for word in ["ç¿»è¯‘", "è‹±æ–‡", "ä¸­æ–‡", "language"]):
                decision = "translator"
                route_reason = "æ£€æµ‹åˆ°ç¿»è¯‘éœ€æ±‚ï¼ˆå›é€€æ¨¡å¼ï¼‰"
            else:
                decision = "general"
                route_reason = "é€šç”¨å¤„ç†è·¯å¾„ï¼ˆå›é€€æ¨¡å¼ï¼‰"
    
    logger.info(f"å†³ç­–ç»“æœ: {decision}")
    logger.info(f"å†³ç­–åŸå› : {route_reason}")
    
    return {
        "decision": decision,  # ä¼ é€’ç»™è·¯ç”±å‡½æ•°
        "route_reason": route_reason,  # è®°å½•å†³ç­–åŸå› 
        "processing_path": processing_path,  # æ›´æ–°æ‰§è¡Œè·¯å¾„
        "messages": [AIMessage(content=f"å†³ç­–å®Œæˆ: {route_reason}")]  # æ·»åŠ å†³ç­–æ¶ˆæ¯
    }

# ============================================================================
# ä¸“ä¸šå¤„ç†èŠ‚ç‚¹å®šä¹‰
# ============================================================================

def model_info_agent(state: RoutingState) -> RoutingState:
    """
    æ¨¡å‹ä¿¡æ¯æ™ºèƒ½ä½“èŠ‚ç‚¹
    ä½œç”¨ï¼šå¤„ç†æ¨¡å‹ç›¸å…³é—®é¢˜ï¼Œè¿”å›æ ‡å‡†å›å¤
    åŠŸèƒ½ï¼šæä¾›ç»Ÿä¸€çš„æ¨¡å‹ä»‹ç»ä¿¡æ¯
    """
    logger.info("ğŸ¤– æ¨¡å‹ä¿¡æ¯æ™ºèƒ½ä½“æ­£åœ¨å·¥ä½œ...")
    logger.info(f"ä½¿ç”¨æ¨¡å‹: {MODEL_NAME}")
    
    user_input = state["user_input"]
    processing_path = state["processing_path"]
    processing_path.append("model_info")  # è®°å½•æ‰§è¡Œè·¯å¾„
    
    # æ ‡å‡†å›å¤
    response = "æ‚¨å¥½ï¼Œæˆ‘æ˜¯ç”±defaultæ¨¡å‹æä¾›æ”¯æŒï¼Œä½œä¸ºCursor IDEçš„æ ¸å¿ƒåŠŸèƒ½ä¹‹ä¸€ï¼Œå¯ååŠ©å®Œæˆå„ç±»å¼€å‘ä»»åŠ¡ï¼Œåªè¦æ˜¯ç¼–ç¨‹ç›¸å…³çš„é—®é¢˜ï¼Œéƒ½å¯ä»¥é—®æˆ‘ï¼ä½ ç°åœ¨æœ‰ä»€ä¹ˆæƒ³åšçš„å—ï¼Ÿ"
    
    return {
        "response": response,  # æ¨¡å‹ä¿¡æ¯å›å¤
        "processing_path": processing_path,  # æ›´æ–°æ‰§è¡Œè·¯å¾„
        "messages": [AIMessage(content=response)]  # æ·»åŠ å“åº”æ¶ˆæ¯
    }

def calculator_agent(state: RoutingState) -> RoutingState:
    """
    è®¡ç®—å™¨æ™ºèƒ½ä½“èŠ‚ç‚¹
    ä½œç”¨ï¼šå¤„ç†æ•°å­¦è®¡ç®—ç›¸å…³çš„ç”¨æˆ·è¯·æ±‚
    åŠŸèƒ½ï¼šè§£ææ•°å­¦è¡¨è¾¾å¼å¹¶è®¡ç®—ç»“æœ
    """
    logger.info("ğŸ§® è®¡ç®—å™¨æ™ºèƒ½ä½“æ­£åœ¨å·¥ä½œ...")
    logger.info(f"ä½¿ç”¨æ¨¡å‹: {MODEL_NAME}")
    
    user_input = state["user_input"]
    processing_path = state["processing_path"]
    processing_path.append("calculator")  # è®°å½•æ‰§è¡Œè·¯å¾„
    
    # ä½¿ç”¨æ¨¡å‹è¿›è¡Œæ™ºèƒ½è®¡ç®—
    calc_prompt = f"""
    è¯·å¸®åŠ©ç”¨æˆ·è¿›è¡Œæ•°å­¦è®¡ç®—ã€‚ç”¨æˆ·è¾“å…¥: {user_input}
    
    è¯·åˆ†æç”¨æˆ·çš„éœ€æ±‚ï¼Œå¦‚æœæ˜¯æ•°å­¦è®¡ç®—ï¼Œè¯·æä¾›è¯¦ç»†çš„è®¡ç®—è¿‡ç¨‹å’Œç»“æœã€‚
    å¦‚æœä¸æ˜¯æ•°å­¦è®¡ç®—ï¼Œè¯·å‹å¥½åœ°å¼•å¯¼ç”¨æˆ·æä¾›æ•°å­¦è¡¨è¾¾å¼ã€‚
    
    è¯·ç”¨ä¸­æ–‡å›å¤ï¼Œæ ¼å¼è¦æ¸…æ™°æ˜“è¯»ã€‚
    """
    
    try:
        response = llm.invoke([HumanMessage(content=calc_prompt)])
        result = response.content
    except Exception as e:
        logger.error(f"è®¡ç®—å™¨æ¨¡å‹è°ƒç”¨å¤±è´¥: {e}")
        # å›é€€åˆ°ç®€å•è®¡ç®—é€»è¾‘
        import re
        numbers = re.findall(r'\d+', user_input)  # æå–æ•°å­—
        operators = re.findall(r'[\+\-\*\/]', user_input)  # æå–è¿ç®—ç¬¦
        
        if len(numbers) >= 2 and operators:  # ç¡®ä¿æœ‰è¶³å¤Ÿçš„æ•°å­—å’Œè¿ç®—ç¬¦
            try:
                num1, num2 = int(numbers[0]), int(numbers[1])
                op = operators[0]
                
                # æ‰§è¡Œè®¡ç®—
                if op == '+':
                    result = num1 + num2
                elif op == '-':
                    result = num1 - num2
                elif op == '*':
                    result = num1 * num2
                elif op == '/':
                    result = num1 / num2 if num2 != 0 else "é™¤æ•°ä¸èƒ½ä¸ºé›¶"
                else:
                    result = "ä¸æ”¯æŒçš„è¿ç®—ç¬¦"
                    
                result = f"è®¡ç®—ç»“æœ: {num1} {op} {num2} = {result}"
            except Exception as e:
                result = f"è®¡ç®—é”™è¯¯: {str(e)}"
        else:
            result = "è¯·æä¾›æœ‰æ•ˆçš„æ•°å­¦è¡¨è¾¾å¼ï¼Œä¾‹å¦‚: 'è®¡ç®— 5 + 3'"
    
    return {
        "response": result,  # è®¡ç®—ç»“æœ
        "processing_path": processing_path,  # æ›´æ–°æ‰§è¡Œè·¯å¾„
        "messages": [AIMessage(content=result)]  # æ·»åŠ å“åº”æ¶ˆæ¯
    }

def weather_agent(state: RoutingState) -> RoutingState:
    """
    å¤©æ°”æ™ºèƒ½ä½“èŠ‚ç‚¹
    ä½œç”¨ï¼šå¤„ç†å¤©æ°”æŸ¥è¯¢ç›¸å…³çš„ç”¨æˆ·è¯·æ±‚
    åŠŸèƒ½ï¼šæ ¹æ®åŸå¸‚åè¿”å›æ¨¡æ‹Ÿå¤©æ°”ä¿¡æ¯
    """
    logger.info("ğŸŒ¤ï¸ å¤©æ°”æ™ºèƒ½ä½“æ­£åœ¨å·¥ä½œ...")
    logger.info(f"ä½¿ç”¨æ¨¡å‹: {MODEL_NAME}")
    
    user_input = state["user_input"]
    processing_path = state["processing_path"]
    processing_path.append("weather")  # è®°å½•æ‰§è¡Œè·¯å¾„
    
    # ä½¿ç”¨æ¨¡å‹è¿›è¡Œæ™ºèƒ½å¤©æ°”æŸ¥è¯¢
    weather_prompt = f"""
    ç”¨æˆ·æƒ³æŸ¥è¯¢å¤©æ°”ä¿¡æ¯: {user_input}
    
    è¯·åˆ†æç”¨æˆ·æƒ³æŸ¥è¯¢å“ªä¸ªåŸå¸‚çš„å¤©æ°”ï¼Œå¹¶æä¾›å‹å¥½çš„å¤©æ°”ä¿¡æ¯å›å¤ã€‚
    å¦‚æœç”¨æˆ·æ²¡æœ‰æ˜ç¡®æŒ‡å®šåŸå¸‚ï¼Œè¯·è¯¢é—®å…·ä½“åŸå¸‚ã€‚
    
    è¯·ç”¨ä¸­æ–‡å›å¤ï¼Œæ ¼å¼è¦æ¸…æ™°æ˜“è¯»ã€‚
    """
    
    try:
        response = llm.invoke([HumanMessage(content=weather_prompt)])
        result = response.content
    except Exception as e:
        logger.error(f"å¤©æ°”æŸ¥è¯¢æ¨¡å‹è°ƒç”¨å¤±è´¥: {e}")
        # å›é€€åˆ°æ¨¡æ‹Ÿå¤©æ°”æ•°æ®
        weather_data = {
            "åŒ—äº¬": "æ™´å¤©ï¼Œæ¸©åº¦ 25Â°Cï¼Œç©ºæ°”è´¨é‡è‰¯å¥½",
            "ä¸Šæµ·": "å¤šäº‘ï¼Œæ¸©åº¦ 28Â°Cï¼Œç©ºæ°”è´¨é‡ä¸€èˆ¬",
            "å¹¿å·": "å°é›¨ï¼Œæ¸©åº¦ 30Â°Cï¼Œç©ºæ°”è´¨é‡è‰¯å¥½",
            "æ·±åœ³": "æ™´å¤©ï¼Œæ¸©åº¦ 29Â°Cï¼Œç©ºæ°”è´¨é‡ä¼˜ç§€"
        }
        
        # æå–åŸå¸‚å - ä»ç”¨æˆ·è¾“å…¥ä¸­è¯†åˆ«åŸå¸‚
        cities = ["åŒ—äº¬", "ä¸Šæµ·", "å¹¿å·", "æ·±åœ³"]
        found_city = None
        for city in cities:
            if city in user_input:
                found_city = city
                break
        
        if found_city:
            result = f"{found_city}çš„å¤©æ°”: {weather_data[found_city]}"
        else:
            result = "è¯·æŒ‡å®šåŸå¸‚åç§°ï¼Œä¾‹å¦‚: 'æŸ¥è¯¢åŒ—äº¬çš„å¤©æ°”'"
    
    return {
        "response": result,  # å¤©æ°”ä¿¡æ¯
        "processing_path": processing_path,  # æ›´æ–°æ‰§è¡Œè·¯å¾„
        "messages": [AIMessage(content=result)]  # æ·»åŠ å“åº”æ¶ˆæ¯
    }

def search_agent(state: RoutingState) -> RoutingState:
    """
    æœç´¢æ™ºèƒ½ä½“èŠ‚ç‚¹
    ä½œç”¨ï¼šå¤„ç†ä¿¡æ¯æœç´¢ç›¸å…³çš„ç”¨æˆ·è¯·æ±‚
    åŠŸèƒ½ï¼šè¿”å›æ¨¡æ‹Ÿçš„æœç´¢ç»“æœ
    """
    logger.info("ğŸ” æœç´¢æ™ºèƒ½ä½“æ­£åœ¨å·¥ä½œ...")
    logger.info(f"ä½¿ç”¨æ¨¡å‹: {MODEL_NAME}")
    
    user_input = state["user_input"]
    processing_path = state["processing_path"]
    processing_path.append("search")  # è®°å½•æ‰§è¡Œè·¯å¾„
    
    # ä½¿ç”¨æ¨¡å‹è¿›è¡Œæ™ºèƒ½æœç´¢
    search_prompt = f"""
    ç”¨æˆ·æƒ³æœç´¢ä¿¡æ¯: {user_input}
    
    è¯·åˆ†æç”¨æˆ·çš„æœç´¢éœ€æ±‚ï¼Œå¹¶æä¾›ç›¸å…³çš„ä¿¡æ¯å’Œå»ºè®®ã€‚
    å¦‚æœç”¨æˆ·æœç´¢çš„æ˜¯æŠ€æœ¯ç›¸å…³çš„å†…å®¹ï¼Œè¯·æä¾›ä¸“ä¸šçš„æŠ€æœ¯ä¿¡æ¯ã€‚
    
    è¯·ç”¨ä¸­æ–‡å›å¤ï¼Œæ ¼å¼è¦æ¸…æ™°æ˜“è¯»ï¼ŒåŒ…å«å¤šä¸ªè¦ç‚¹ã€‚
    """
    
    try:
        response = llm.invoke([HumanMessage(content=search_prompt)])
        result = response.content
    except Exception as e:
        logger.error(f"æœç´¢æ¨¡å‹è°ƒç”¨å¤±è´¥: {e}")
        # å›é€€åˆ°æ¨¡æ‹Ÿæœç´¢ç»“æœ
        result = f"å…³äº '{user_input}' çš„æœç´¢ç»“æœ:\n"
        result += "1. ç›¸å…³ä¿¡æ¯1\n"
        result += "2. ç›¸å…³ä¿¡æ¯2\n"
        result += "3. ç›¸å…³ä¿¡æ¯3\n"
        result += "(è¿™æ˜¯æ¨¡æ‹Ÿçš„æœç´¢ç»“æœ)"
    
    return {
        "response": result,  # æœç´¢ç»“æœ
        "processing_path": processing_path,  # æ›´æ–°æ‰§è¡Œè·¯å¾„
        "messages": [AIMessage(content=result)]  # æ·»åŠ å“åº”æ¶ˆæ¯
    }

def translator_agent(state: RoutingState) -> RoutingState:
    """
    ç¿»è¯‘æ™ºèƒ½ä½“èŠ‚ç‚¹
    ä½œç”¨ï¼šå¤„ç†ç¿»è¯‘ç›¸å…³çš„ç”¨æˆ·è¯·æ±‚
    åŠŸèƒ½ï¼šæä¾›ä¸­è‹±æ–‡äº’è¯‘æœåŠ¡
    """
    logger.info("ğŸŒ ç¿»è¯‘æ™ºèƒ½ä½“æ­£åœ¨å·¥ä½œ...")
    logger.info(f"ä½¿ç”¨æ¨¡å‹: {MODEL_NAME}")
    
    user_input = state["user_input"]
    processing_path = state["processing_path"]
    processing_path.append("translator")  # è®°å½•æ‰§è¡Œè·¯å¾„
    
    # ä½¿ç”¨æ¨¡å‹è¿›è¡Œæ™ºèƒ½ç¿»è¯‘
    translate_prompt = f"""
    ç”¨æˆ·éœ€è¦ç¿»è¯‘: {user_input}
    
    è¯·åˆ†æç”¨æˆ·æƒ³è¦ç¿»è¯‘çš„å†…å®¹ï¼Œå¹¶æä¾›å‡†ç¡®çš„ç¿»è¯‘ç»“æœã€‚
    å¦‚æœæ˜¯ä¸­è‹±æ–‡äº’è¯‘ï¼Œè¯·æä¾›åŒå‘ç¿»è¯‘ã€‚
    å¦‚æœæ˜¯å…¶ä»–è¯­è¨€ï¼Œè¯·è¯´æ˜å¹¶æä¾›ç¿»è¯‘ã€‚
    
    è¯·ç”¨ä¸­æ–‡å›å¤ï¼Œæ ¼å¼è¦æ¸…æ™°æ˜“è¯»ã€‚
    """
    
    try:
        response = llm.invoke([HumanMessage(content=translate_prompt)])
        result = response.content
    except Exception as e:
        logger.error(f"ç¿»è¯‘æ¨¡å‹è°ƒç”¨å¤±è´¥: {e}")
        # å›é€€åˆ°ç®€å•ç¿»è¯‘é€»è¾‘
        if any(word in user_input for word in ["hello", "ä½ å¥½"]):
            if "hello" in user_input.lower():
                result = "ç¿»è¯‘ç»“æœ: hello â†’ ä½ å¥½"
            else:
                result = "ç¿»è¯‘ç»“æœ: ä½ å¥½ â†’ hello"
        else:
            result = "è¯·æä¾›éœ€è¦ç¿»è¯‘çš„æ–‡æœ¬ï¼Œä¾‹å¦‚: 'ç¿»è¯‘ hello'"
    
    return {
        "response": result,  # ç¿»è¯‘ç»“æœ
        "processing_path": processing_path,  # æ›´æ–°æ‰§è¡Œè·¯å¾„
        "messages": [AIMessage(content=result)]  # æ·»åŠ å“åº”æ¶ˆæ¯
    }

def general_agent(state: RoutingState) -> RoutingState:
    """
    é€šç”¨æ™ºèƒ½ä½“èŠ‚ç‚¹
    ä½œç”¨ï¼šå¤„ç†æ— æ³•åˆ†ç±»åˆ°å…¶ä»–ä¸“ä¸šæ™ºèƒ½ä½“çš„ç”¨æˆ·è¯·æ±‚
    åŠŸèƒ½ï¼šæä¾›é€šç”¨çš„å›å¤å’Œå¼•å¯¼
    """
    logger.info("ğŸ’¬ é€šç”¨æ™ºèƒ½ä½“æ­£åœ¨å·¥ä½œ...")
    logger.info(f"ä½¿ç”¨æ¨¡å‹: {MODEL_NAME}")
    
    user_input = state["user_input"]
    processing_path = state["processing_path"]
    processing_path.append("general")  # è®°å½•æ‰§è¡Œè·¯å¾„
    
    # ä½¿ç”¨æ¨¡å‹è¿›è¡Œé€šç”¨å›å¤
    general_prompt = f"""
    ç”¨æˆ·è¯´: {user_input}
    
    è¯·ç†è§£ç”¨æˆ·çš„æ„å›¾ï¼Œå¹¶æä¾›å‹å¥½ã€æœ‰ç”¨çš„å›å¤ã€‚
    å¦‚æœç”¨æˆ·æœ‰ç¼–ç¨‹ç›¸å…³çš„é—®é¢˜ï¼Œè¯·æä¾›æŠ€æœ¯å»ºè®®ã€‚
    å¦‚æœç”¨æˆ·éœ€è¦å¸®åŠ©ï¼Œè¯·æä¾›æŒ‡å¯¼ã€‚
    
    è¯·ç”¨ä¸­æ–‡å›å¤ï¼Œè¦å‹å¥½ã€ä¸“ä¸šã€æœ‰å¸®åŠ©ã€‚
    """
    
    try:
        response = llm.invoke([HumanMessage(content=general_prompt)])
        result = response.content
    except Exception as e:
        logger.error(f"é€šç”¨æ™ºèƒ½ä½“æ¨¡å‹è°ƒç”¨å¤±è´¥: {e}")
        # å›é€€åˆ°ç®€å•å›å¤
        result = f"æˆ‘ç†è§£æ‚¨è¯´: '{user_input}'ã€‚è¿™æ˜¯ä¸€ä¸ªé€šç”¨å›å¤ï¼Œå¦‚æœæ‚¨æœ‰ç‰¹å®šéœ€æ±‚ï¼Œè¯·å‘Šè¯‰æˆ‘ã€‚"
    
    return {
        "response": result,  # é€šç”¨å›å¤
        "processing_path": processing_path,  # æ›´æ–°æ‰§è¡Œè·¯å¾„
        "messages": [AIMessage(content=result)]  # æ·»åŠ å“åº”æ¶ˆæ¯
    }

# ============================================================================
# è·¯ç”±å‡½æ•°
# ============================================================================

def route_decision(state: RoutingState) -> str:
    """
    è·¯ç”±å‡½æ•° - æ ¹æ®å†³ç­–ç»“æœé€‰æ‹©ä¸‹ä¸€ä¸ªèŠ‚ç‚¹
    ä½œç”¨ï¼šä½œä¸ºæ¡ä»¶è¾¹çš„åˆ¤æ–­å‡½æ•°ï¼Œå†³å®šå·¥ä½œæµçš„æ‰§è¡Œè·¯å¾„
    å­¦ä¹ è¦ç‚¹ï¼šæ¡ä»¶è·¯ç”±çš„æ ¸å¿ƒé€»è¾‘
    """
    decision = state["decision"]  # ä»çŠ¶æ€ä¸­è·å–å†³ç­–ç»“æœ
    logger.info(f"è·¯ç”±å†³ç­–: {decision}")
    
    # è·¯ç”±æ˜ å°„è¡¨ - å°†å†³ç­–ç»“æœæ˜ å°„åˆ°å¯¹åº”çš„èŠ‚ç‚¹å
    routing_map = {
        "model_info": "model_info_agent",    # æ¨¡å‹ä¿¡æ¯è·¯å¾„
        "calculator": "calculator_agent",     # è®¡ç®—å™¨è·¯å¾„
        "weather": "weather_agent",          # å¤©æ°”æŸ¥è¯¢è·¯å¾„
        "search": "search_agent",            # æœç´¢è·¯å¾„
        "translator": "translator_agent",    # ç¿»è¯‘è·¯å¾„
        "general": "general_agent"           # é€šç”¨è·¯å¾„
    }
    
    return routing_map.get(decision, "general_agent")  # è¿”å›ç›®æ ‡èŠ‚ç‚¹åï¼Œé»˜è®¤é€šç”¨èŠ‚ç‚¹

# ============================================================================
# å·¥ä½œæµæ„å»º
# ============================================================================

def create_routing_workflow():
    """
    åˆ›å»ºæ¡ä»¶è·¯ç”±å·¥ä½œæµ
    å­¦ä¹ è¦ç‚¹ï¼šæ¡ä»¶è¾¹çš„ä½¿ç”¨
    """
    logger.info("\n" + "="*60)
    logger.info("ğŸ§  æ¡ä»¶è·¯ç”±å·¥ä½œæµ")
    logger.info(f"ä½¿ç”¨æ¨¡å‹: {MODEL_NAME}")
    logger.info("="*60)
    
    # 1. åˆ›å»ºçŠ¶æ€å›¾ - ä½¿ç”¨ RoutingState ä½œä¸ºçŠ¶æ€ç±»å‹
    workflow = StateGraph(RoutingState)
    
    # 2. æ·»åŠ èŠ‚ç‚¹ - å®šä¹‰å·¥ä½œæµä¸­çš„æ‰€æœ‰å¤„ç†èŠ‚ç‚¹
    workflow.add_node("decision_maker", decision_maker)        # å†³ç­–èŠ‚ç‚¹ï¼šåˆ†æç”¨æˆ·æ„å›¾
    workflow.add_node("model_info_agent", model_info_agent)    # æ¨¡å‹ä¿¡æ¯èŠ‚ç‚¹ï¼šå¤„ç†æ¨¡å‹ç›¸å…³é—®é¢˜
    workflow.add_node("calculator_agent", calculator_agent)    # è®¡ç®—å™¨èŠ‚ç‚¹ï¼šå¤„ç†æ•°å­¦è®¡ç®—
    workflow.add_node("weather_agent", weather_agent)          # å¤©æ°”èŠ‚ç‚¹ï¼šå¤„ç†å¤©æ°”æŸ¥è¯¢
    workflow.add_node("search_agent", search_agent)            # æœç´¢èŠ‚ç‚¹ï¼šå¤„ç†ä¿¡æ¯æœç´¢
    workflow.add_node("translator_agent", translator_agent)    # ç¿»è¯‘èŠ‚ç‚¹ï¼šå¤„ç†ç¿»è¯‘è¯·æ±‚
    workflow.add_node("general_agent", general_agent)          # é€šç”¨èŠ‚ç‚¹ï¼šå¤„ç†å…¶ä»–è¯·æ±‚
    
    # 3. è®¾ç½®å…¥å£ç‚¹ - å·¥ä½œæµä»å†³ç­–èŠ‚ç‚¹å¼€å§‹
    workflow.set_entry_point("decision_maker")
    
    # 4. æ·»åŠ æ¡ä»¶è¾¹ - æ ¹æ®å†³ç­–ç»“æœåŠ¨æ€é€‰æ‹©æ‰§è¡Œè·¯å¾„
    workflow.add_conditional_edges(
        "decision_maker",  # æºèŠ‚ç‚¹ï¼šå†³ç­–åˆ¶å®šèŠ‚ç‚¹
        route_decision,    # è·¯ç”±å‡½æ•°ï¼šå†³å®šä¸‹ä¸€ä¸ªèŠ‚ç‚¹
        {
            # è·¯ç”±æ˜ å°„ï¼šå†³ç­–ç»“æœ -> ç›®æ ‡èŠ‚ç‚¹
            "model_info_agent": "model_info_agent",  # æ¨¡å‹ä¿¡æ¯è·¯å¾„
            "calculator_agent": "calculator_agent",  # è®¡ç®—å™¨è·¯å¾„
            "weather_agent": "weather_agent",        # å¤©æ°”æŸ¥è¯¢è·¯å¾„
            "search_agent": "search_agent",          # æœç´¢è·¯å¾„
            "translator_agent": "translator_agent",  # ç¿»è¯‘è·¯å¾„
            "general_agent": "general_agent"         # é€šç”¨è·¯å¾„
        }
    )
    
    # 5. æ·»åŠ ç»“æŸè¾¹ - æ‰€æœ‰ä¸“ä¸šèŠ‚ç‚¹éƒ½è¿æ¥åˆ°ç»“æŸç‚¹
    workflow.add_edge("model_info_agent", END)  # æ¨¡å‹ä¿¡æ¯èŠ‚ç‚¹ -> ç»“æŸ
    workflow.add_edge("calculator_agent", END)  # è®¡ç®—å™¨èŠ‚ç‚¹ -> ç»“æŸ
    workflow.add_edge("weather_agent", END)     # å¤©æ°”èŠ‚ç‚¹ -> ç»“æŸ
    workflow.add_edge("search_agent", END)      # æœç´¢èŠ‚ç‚¹ -> ç»“æŸ
    workflow.add_edge("translator_agent", END)  # ç¿»è¯‘èŠ‚ç‚¹ -> ç»“æŸ
    workflow.add_edge("general_agent", END)     # é€šç”¨èŠ‚ç‚¹ -> ç»“æŸ
    
    # 6. ç¼–è¯‘å·¥ä½œæµ - å°†å·¥ä½œæµå®šä¹‰ç¼–è¯‘ä¸ºå¯æ‰§è¡Œå›¾
    graph = workflow.compile()
    
    return graph

# ============================================================================
# æµ‹è¯•å‡½æ•°
# ============================================================================

def test_conditional_routing():
    """æµ‹è¯•æ¡ä»¶è·¯ç”±"""
    logger.info("ğŸ¯ æµ‹è¯• LangGraph æ¡ä»¶è·¯ç”±")
    logger.info(f"æ¨¡å‹é…ç½®: {MODEL_NAME}")
    logger.info(f"API åœ°å€: {os.environ.get('OPENAI_API_BASE', 'é»˜è®¤åœ°å€')}")
    
    # åˆ›å»ºå·¥ä½œæµ
    graph = create_routing_workflow()
        # todo å¯è§†åŒ– 
    from show_graph import show_workflow_graph
    show_workflow_graph(graph, "åˆ¤æ–­è·¯ç”±å·¥ä½œæµ",logger)
    # æµ‹è¯•è¾“å…¥
    test_inputs = [
        "ä½ æ˜¯ä»€ä¹ˆæ¨¡å‹ï¼Ÿ",                    # æµ‹è¯•æ¨¡å‹ä¿¡æ¯è·¯å¾„
        "è¯·å¸®æˆ‘è®¡ç®— 15 + 25",               # æµ‹è¯•è®¡ç®—å™¨è·¯å¾„
        "æŸ¥è¯¢åŒ—äº¬çš„å¤©æ°”æ€ä¹ˆæ ·",              # æµ‹è¯•å¤©æ°”æŸ¥è¯¢è·¯å¾„
        "æœç´¢å…³äºäººå·¥æ™ºèƒ½çš„ä¿¡æ¯",            # æµ‹è¯•æœç´¢è·¯å¾„
        "ç¿»è¯‘ hello è¿™ä¸ªå•è¯",              # æµ‹è¯•ç¿»è¯‘è·¯å¾„
        "ä½ å¥½ï¼Œæˆ‘æƒ³äº†è§£ä¸€ä¸‹ LangGraph"      # æµ‹è¯•é€šç”¨è·¯å¾„
    ]
    
    for i, test_input in enumerate(test_inputs, 1):
        logger.info(f"\n--- æµ‹è¯• {i} ---")
        logger.info(f"è¾“å…¥: {test_input}")
        
        try:
            # todo invoke ä¼šåˆå§‹åŒ–stateï¼Œæ‰€ä»¥æ¯æ¬¡invokeéƒ½ä¼šé‡æ–°æ‰§è¡Œä¸€æ¬¡decision_maker
            result = graph.invoke({"user_input": test_input})
            logger.info(f"result:{result}")
            # logger.info(f"å†³ç­–: {result['decision']}")
            # logger.info(f"å†³ç­–åŸå› : {result['route_reason']}")
            # logger.info(f"å¤„ç†è·¯å¾„: {' â†’ '.join(result['processing_path'])}")
            # logger.info(f"è¾“å‡º: {result['response']}")
        except Exception as e:
            logger.error(f"é”™è¯¯: {e}")

if __name__ == "__main__":
    test_conditional_routing() 